分类号： TP391.41 学校代号：10150  
U D C ： 密级：   公开       学       号：20213048

# 大建交通大學专业硕士学位论文

# 人脸交换图像鉴伪技术研究Research on Detection Technologies ofFace-swapping Forgery ImageIdentification

学 生 姓 名 ： 费思宇校内导师及职称： 贾世杰 教授行业产业导师及职称： 戴家文 高级工程师专 业 名 称 ： 电子信息研 究 方 向 信号处理与模式识别论 文 类 型 ： 应用研究培 养 类 型 ： 全日制专业学位硕士论 文 答 辩 日 期 ： 2023 年 6 月 10 日学 位 授 予 单 位 ： 大连交通大学

# 摘   要

基于深度学习的人脸交换技术是一种使用深度学习的方法用目标人脸替换源人脸的人脸伪造技术，其发展已给社会生活带来较大负面影响。目前已存在一些针对人脸交换的鉴伪技术，但多数方法不具有可解释性，且检测精度也有待提高。本文研究人脸交换图像鉴伪技术，主要研究内容如下：

对深度伪造人脸生成技术和鉴别技术的代表性方法进行研究，简要分析了人脸生成、人脸重现、属性编辑、人脸交换等深度人脸伪造技术；XceptionNet、MesoNet、X-Ray 等伪造检测技术；重点分析讨论了人脸交换图像伪造算法及检测算法的研究现状。

依据伪造人脸图像两眼间角膜高光不一致性，提出基于人眼瞳孔特性的 FaceSwap-GAN 人脸伪造图像检测方法。该方法利用 Dlib 提取面部关键点截取眼部区域，再利用添加 SK-ResNext 模块的 U-Net 模型提取虹膜区域，使用最大类间方差法自适应地确定合适的分割阈值分离角膜高光，将左右眼角膜高光对齐计算二者IOU分数，以分数值作为判断指标。在 FaceSwap-GAN 数据集上的测试结果表明，本文方法得到的 AUC 值为0.96、精确度为 $9 5 . 5 8 \%$ ，比目前主流的两种基于规则的检测方法，得到的 AUC 值分别提高了0.02 和0.07；精确度分别提升了 $4 . 4 0 \%$ 和 $2 . 3 5 \%$ 。

依据人脸交换伪造视频存在的帧内伪影与帧间变化特点，提出基于特征融合的人脸伪造视频检测方法。首先抽取视频帧并利用 Dlib 提取面部关键点截取眼部区域，将眼部图像送入添加注意力机制的 EfficientNetB4 网络提取帧内特征，将眼部图像序列送入孪生网络提取帧间差异信息，再将二者加和拼接，送入全连接层利用 Softmax 分类器进行检测。实验表明，在本文构建的 FSV 数据集上进行测试，该方法在高清画质视频集上进行检测时得到的 AUC 值为 0.92、精确度为 $91 . 5 \%$ ，与主流方法相比 AUC 值提升了 0.03\~0.13、精确度提升了 $0 . 0 5 1 \% { \sim } 0 . 1 8 3 \%$ ；在低清画质视频集上得到的 AUC值为 0.89、精确度为 $87 . 5 \%$ ，比主流算法的 AUC 值提升了 $0 . 0 4 { \sim } 0 . 1 2$ 、精确度提升了$0 . 0 2 8 \% { \sim } 0 . 1 5 7 \%$ 。

关键字：人脸交换；伪造检测；人眼瞳孔特性；特征融合；FaceSwap-GAN

# Abstract

The development of deep learning-based face-swapping, a face forgery technique that uses deep learning methods to replace the source face with the target face, has had a major negative impact on social life. A number of authentication techniques for face-swapping already exist, but most of the methods do not have interpretable results and the detection accuracy needs to be improved. This thesis investigates techniques of face-swapping forgery image identification, with the following main research components:

A representative approach to deep forgery face generation techniques and identification techniques is investigated, with a brief analysis of deep face forgery techniques such as face generation, face reproduction, attribute editing and face swapping; forgery detection techniques such as XceptionNet, MesoNet & X-Ray. The analyses and discussions focus on the current state of research in face-swapping image forgery algorithms and detection algorithms.

Based on the inconsistency of corneal highlights between the two eyes of the forged face image, the FaceSwap-GAN face forgery detection method based on the pupil characteristics of human eyes is proposed. we uses Dlib to extract the key points of the face to intercept the eye region, then uses the U-Net model with the SK-ResNext module added to extract the iris region, uses the maximum inter-class variance method to adaptively determine the appropriate segmentation threshold to separate the corneal highlights, aligns the left and right corneal highlights to calculate the IOU score of the two, and uses the score value as the judgment index. The test results on the FaceSwap-GAN dataset show that the AUC value of 0.96 and the accuracy of $9 5 . 5 8 \%$ obtained by this method are 0.02 and 0.07 respectively, which are $4 . 4 0 \%$ and $2 . 3 5 \%$ higher than the AUC values obtained by the two mainstream rule-based detection methods.

Based on the intra-frame artifacts and inter-frame variations of face-swapping forgery videos, a feature fusion-based face forgery video detection method is proposed. The eye images are then fed into an EfficientNetB4 network with an attention mechanism to extract the intra-frame features, and the eye sequences are fed into a Siamese-network to extract the inter-frame difference information, then the two are summed and stitched together and fed into a fully connected layer for detection using a Softmax classifier. Experiments show that, when test on the FSV dataset constructed in this thesis, the method achieves an AUC value of 0.92 and an accuracy of $91 . 5 \%$ on the HQ video dataset, which is an improvement of $0 . 0 3 \mathrm { \sim } 0 . 1 3$ in AUC value and $0 . 0 5 1 \% { \sim } 0 . 1 8 3 \%$ in accuracy compared to the mainstream methods; The AUC value of 0.89 and accuracy of $8 7 . 5 \%$ are obtained on the LQ video dataset, an improvement of $0 . 0 4 { \sim } 0 . 1 2$ and $0 . 0 2 8 { \sim } 0 . 1 5 7 \%$ over the AUC values of mainstream algorithms.

Key Words：Face-swapping；Forgery Detection；Human Eye Pupil Characteristics； Feature Fusion

# 目  录

第一章 绪论.  
1.1 课题研究背景及意义 .  
1.2 国内外研究现状 . 2  
1.2.1 深度伪造人脸生成技术研究现状 .. 2  
1.2.2 深度伪造人脸检测技术研究现状 . 3  
1.3 数据集. ..4  
1.3.1 公共数据集介绍 .. ..4  
1.3.2 本文构建数据集 ..6  
1.4 本文使用平台 .. ....8  
1.5 本文组织结构 ... ..9  
本章小结. .10  
第二章 人脸交换伪造与检测相关技术 . 11  
2.1 引言 . 11  
2.2 人脸交换伪造技术 . .11  
2.2.1 Deep Fake . . 11  
2.2.2 Face Swap-GAN ... .14  
2.2.3 Face Shifter ..15  
2.3 人脸交换伪造检测技术. ...16  
2.3.1 检测技术基础 ..16  
2.3.2 主流伪造检测方法 . ..18  
本章小结 .. .20  
第三章 基于人眼瞳孔特性的 Face Swap-GAN 人脸伪造检测 . .21  
3.1 引言 .21  
3.2 人眼的生物物理学规律.. .22  
3.3 本文方法 .23  
3.3.1 整体流程 . .23  
3.3.2 虹膜分割 .24  
3.3.3 角膜高光提取 . ..30  
3.4 实验结果与分析 . .31  
3.4.1 实验参数设置 ..31  
3.4.2 评价指标 .. ..32  
3.4.3 实验结果分析 . ..34  
本章小结 .39  
第四章 基于特征融合的视频人脸伪造检测 .40

4.1 引言 40  
4.2 本文方法 .40  
4.2.1 总体结构 .40  
4.2.2 特征提取网络 .41  
4.2.3 帧间差异特征表示 . .46  
4.2.4 Softmax 分类器 .47  
4.3 实验结果与分析 . .49  
4.3.1 实验参数设置. .49  
4.3.2 评价指标 .51  
4.3.3 实验结果分析. 51  
本章小结 . 57  
第五章 总结与展望 .58  
5.1 总结. .58  
5.2 展望. .59  
参考文献 .. .60

# 第一章 绪论

# 1.1 课题研究背景及意义

近年来随着科学技术不断蓬勃发展，以深度学习为代表的人工智能技术逐渐走进大众的视野，也使得深度人脸伪造技术得到了迅猛发展，现如今已经能非常容易地对人的面部特征进行模仿、伪造，而且形式变得多样化，除了图片之外，还出现了伪造视频以及伪造音频。

人脸伪造技术以人脸图像生成技术为核心。包含直接利用网络生成虚假人脸的技术以及人脸交换、人脸重现、人脸老化等技术[1]，而人脸交换技术因其操作简单被广泛用于伪造。

人脸交换技术，顾名思义就是把图片或者视频中的人脸替换成另外一张人脸从而合成一张全新的人脸图片或者视频的技术。这样的技术如果不正确使用会给社会带来危害。例如有人利用人脸交换技术制造假新闻，再配上夸张的标题吸引眼球，使得新闻媒体行业陷入到信任危机之中；又如此前在国内红极一时的换脸软件 ZAO[2]，该软件用户协议不规范，存在泄露用户数据等网络安全隐患问题，推出后受到群众抵制，上线不久后就被迫下架。其实除了 ZAO 之外，还有很多换脸软件例如 FaceApp[3]、Faceswap[4]等，这也就意味着，换脸的准入门槛很低，此项技术的乱用会对个人信息安全、肖像权、隐私权等造成极大的威胁，甚至还会扰乱公众秩序、影响政府权威、威胁国家安全。因此对伪造检测技术的研究至关重要。

随着人脸交换技术的不断发展，检测技术也在不断迭代更新，很多研究者已经提出了一些检测方法，主要是针对换脸图像和换脸视频进行检测。对换脸图像的检测方法主要是对人脸交换过程中产生的不自然伪影进行检测，以此来判断图像中的人脸是否进行了交换。而换脸视频检测主要分为两类，第一类是利用视频相邻两帧之间的帧间差异信息作为判断依据，利用视频中人物的眨眼频率[5]、嘴部动作[6]、头部姿态[7]等行为的时间相关性，用 LSTM[8]进行鉴别。像 Sabir 等人利用时间上的相关性，设计了一种基于递归皮质网络(Recursive Cortical Network, RCN) [9]的人脸伪造视频检测方法。另一类是通过提取帧内图像特征信息，与图像检测类似以人脸阴影和图像边缘存在的一些不自然伪影为依据进行鉴别，通常采用提取特征后使用分类器的方式进行检测。如 Nguyen 等人[10]提出的名为胶囊网络的模型，后来又有研究团队提出一种卷积神经网络[11]，用其进行特征提取，使得提取到的特征更加精准有效，提升鉴伪网络的检测能力。但传统的检测方法不能兼顾高效、简单与检测准确率，因此对如何在兼顾简单高效的前提下提升伪造检测算法的检测能力进行研究很有必要。

# 1.2 国内外研究现状

# 1.2.1 深度伪造人脸生成技术研究现状

深度伪造人脸生成技术依据对人脸篡改区域和篡改目的的不同，可分为人脸生成、人脸重现、属性编辑、人脸交换等。人脸生成是利用网络生成一张之前不存在的人脸图像。人脸重现是指在保持图像中人脸身份不变的情况下改变人物的面部表情。属性编辑则是对人脸一些属性进行修改，例如发色、年龄、表情、光照等。人脸交换是指在不改变背景的情况下将源图像的人脸替换到目标图像上实现换脸的一种技术。

人脸生成是一种不依赖于现实中真实人脸的伪造技术，通过从噪声中提取信息从而生成现实中不存在的人脸。相关技术模型有自编码器、GAN(Generative AdversarialNets，GAN)[12]以及扩散模型等。自编码器利用编解码结构随机生成新图像，GAN 利用生成器与判别器的博弈生成新的图像，扩展模型利用正向扩散和反向扩散两个过程来生成图像。现如今开源的生成模型有很多，像 Open AI[13]、GhatGPT[14]、StableDiffusion[15]等。但人脸生成技术最具代表性的就是 GAN，其通过生成器与判别器之间的博弈，生成一张无限接近真实样本的虚假人脸。Radford 等人将卷积神经网络引入到生成器和判别器中，提出了一种拥有更强的拟合与表达能力的网络，并将其命名为深度卷积对抗神经网络(Deep Convolutinal GAN，DCGAN)[16]，Karras 等人提出可以生成高分辨率图像的生成网络 ProGAN[17]，最高可生成分辨率为 $1 0 2 4 \times 1 0 2 4$ 的人脸图像。Karras 团队提出 StyleGAN[18]，通过使用 AdaIn[19]模块使得生成器能够生成指定特征（肤色、发色、瞳孔颜色等）的人脸图像。

人脸重现技术是一种表情姿态迁移技术，它能够对目标人物的人脸图像进行篡改，使得篡改后的图像拥有目标人物的长相，同时保留源人物的表情和姿态。Thies 等人提出的 Face2Face[20]就是最为经典的人脸表情伪造方法，可通过先对源图像和目标图像进行图像渲染工作，然后将源图像中的表情通过 3D 重建的方式实现源人脸图像和目标人脸表情的迁移。

属性编辑是一种通过修改人脸面部属性进行伪造的技术。He 等人[21]通过将人脸的属性进行分类，并在语义空间对各种属性进行解耦，保证人脸属性能够进行正确的编辑。Yao 等人[22]在 StyleGAN 的基础上对损失函数进行改进，使得属性编辑更加可控，$\mathrm { X u } ^ { [ 2 3 ] }$ 等人利用Transformer 框架，使得生成图片的质量和属性的编辑更加的灵活。

人脸交换技术是比较经典的人脸伪造生成技术，早期的人脸交换基于计算机图像学，最为经典的就是 Faceswap，但这种方法依赖源图像人脸和目标图像人脸模型的相似程度，若二者差距较大则伪造痕迹较为明显。后来深度学习的出现给人脸交换带来了新思路，2017 年 Reddit 论坛上开源的项目 DeepFakes[23]就是最典型的基于深度学习的人脸交换技术，除此之外还有 FaceShifter，再后来研究人员将风格迁移思想以及GAN 引入人脸交换中，Korshunova 等人巧妙的将风格迁移[25]技术引入到人脸交换领域。随着伪造检测领域的不断发展，又有研究者将 GAN 与变分自编码器(VariationalAutoencoder，VAE)结合[26]提出一种新的人脸交换方法，该方法可用于对人物头发的编辑，并且仅使用单张图片就能够进行源人脸和目标人脸的交换，FaceSwap-GAN[27]在RSGAN[28]的基础上进行改进，进一步提升了伪造人脸的质量。

# 1.2.2 深度伪造人脸检测技术研究现状

依据不同的伪造形式，深度伪造人脸检测技术可以分为针对伪造人脸图像的检测技术和针对伪造人脸视频的检测技术。针对伪造人脸图像的检测技术主要基于伪造图像和真实图像之间的统计分布差异以及伪造过程中产生的不自然伪影进行检测。而针对伪造人脸视频的检测技术主要利用视频的连续性、时序性等特点，根据视频相邻帧的关联性进行检测。

# (1) 基于图像级的检测

针对图像的检测方法主要分为两类，一类是利用统计特征进行检测。例如McCloskey[29]等人利用伪造图像的颜色分布与真实图像的颜色分布之间的差异，使用支持向量机进行分类。又如有研究者基于伪造图像与真实图像之间存在的纹理差异，利用 Gram 矩阵表示纹理进行检测[30]。另一类则是利用伪造图像中存在的边缘模糊、人脸区域与非人脸区域分辨率不一致、少样本情况下五官变形等问题进行检测。例如孙鹏[31]等人针对人脸交换过程中图像拼接导致的色彩偏移量不一致问题，提出一种能自动检测拼接篡改图像中的色彩偏移量不一致并定位拼接篡改区域的方法。Stehouwer[32]等将注意力机制[33]引入鉴伪，让模型学会关注更重要的信息，将检测结果提升到一个新高度。Tan M 等人将适用于人脸伪造检测的注意力机制引入 Efficient-Net $\mathbf { B } 3 ^ { [ 3 4 ] }$ 网络架构之中，使网络能不受其它因素干扰只聚焦于篡改痕迹。商汤团队创造性的将频域信息引入到鉴伪技术中，提出一种局部频域特征网络[35]用来进行频域伪造特征的提取，并利用注意力机制将频域信息与时间域信息相融合，实验表明该方法在对压缩图像进行伪造检测时拥有较强的鲁棒性。

(2) 基于视频级的检测

当前伪造人脸视频绝大多数采取的是对局部人脸区域进行伪造的方法，因此视频级检测主要是利用视频帧与帧之间人脸特征的不一致性。如 Guera 等人[36]发现伪造人脸视频包含帧内不一致以及帧与帧之间不一致，基于此提出了一种利用 CNN 和 LSTM进行伪造视频检测的方法；Sabir 等人[37]将卷积网络 DenseNet58 和门控循环单元[38]相结合，提出了循环卷积模型，利用帧与帧之间的时间差异来鉴别伪造人脸视频；Afchar 等人提出两个卷积神经网络 Meso-4 和 MesoInception- $4 ^ { [ 3 9 ] }$ ，利用卷积神经网络捕捉帧内信息检测伪造视频。Amerini 等人提出使用光流估计的方法[40]检测伪造视频。同时提出一种基于多实例学习的伪造检测方法，该方法可以针对同一帧中包含多张不同人脸，但其中只有一个特定人物的人脸被伪造的视频进行检测，其检测结果准确率比基于投票策略的检测方法的准确率更高。Masi 等人[41]将频域信息与 RGB 域信息相结合提出一种双分支递归网络进行伪造视频检测。Li 等人设计并提出了一种基于EfficientNet 的双流网络[42]，在对 FaceForensics＋＋数据集[43]中采用 DeepFake 伪造方法生成的数据进行测试时，得到了很高的检测精确度，而且该模型具有较强的对抗压缩的能力。后来随着 transformer 在很多视觉任务上出色的表现，Wodajo 等人将 CNN与 transformer 进 行 融 合 ， 提 出 了 一 种 基 于 自 注 意 力 层 的 伪 造 检 测 模 型CVIT(Convolutions to Vision Transformers)[44]，提升了检测模型的通用性，同时因为学习了局部特征和全局特征使得网络的检测精确度也得到了提升。

除此之外，还有研究者针对伪造人脸视频中缺少正常生物信号这一问题，提出相应的检测方法。Li[5]等人发现伪造人脸视频中的人物眨眼频率存在异常，因此提出利用生物信号——眨眼频率来检测伪造视频的方法。后来又有研究者提出了基于心率[45]、基于脉搏波[46]、基于头部姿态估计[7]等检测方法。

# 1.3 数据集

# 1.3.1 公共数据集介绍

本文数据集主要用于对分割网络进行训练，以及对检测网络进行训练并测试。分割数据集以 CASIA-Iris[47]为基础，检测网络用到的数据集主要以人脸鉴伪领域常见的FaceForensics＋＋、DF-TIMIT 数据集为基础，下文主要对这三种常见数据集以及本文所用数据集进行介绍。

# (1) CASIA-Iris 数据集

由中国科学院自动化所的研究小组所开发，已从 CASIA-IrisV1 更新为 CASIA-IrisV4。CASIA-IrisV4 包含六个子集。共包含来自 1800 多个真实主体和 1000 个虚拟主体的54607 张虹膜图像。如图1.1 为数据集中部分样本展示。

![](images/cfcc7626449a2e0bf2c620061942b7bddb9fd66becb7941dd097965f2ba860b1.jpg)  
图 1.1 CASIA-Iris 数据集样本展示Fig. 1.1 CASIA-Iris dataset sample display

(2) FaceForensics＋＋数据集

FaceForensics＋＋数据集[48]是目前人脸鉴伪领域被普遍使用的数据集，作为最初的大规模深度伪造人脸数据集之一，它发布于 2019 年初，其中包含 1000 个在YouTube 上收集的新闻和采访人物真实视频，以及使用 DeepFakes、Faceswap、Face2Face、Neural Textures 生成的伪造视频。后来还将 CVPR2020 中 FaceShifer 伪造方法加入其中，这 5 种伪造方法每种各生成 1000 个虚假视频，最终共得到 5000 个伪造视频。每种生成方式都采用 H246 编码方式的不同压缩参数进行压缩处理，得到了 3 种不同压缩率的视频，分别为原始视频(Raw)、高清画质视频(c23 或 HQ)、低清画质视频(c40 或 LQ)。如图 1.2 所示，其中从左到右分别为原视频截图、c23 高清画质视频截图、c40 低清画质视频截图。

![](images/519ffc7515f340cff380f52a7f913a2de51d9d8d0bea0abbdc7f2a6bacbe07ad.jpg)  
图 1.2 FaceForensics $^ { 1 + + }$ 数据集三种清晰度Fig. 1.2 Three definitions of FaceForensics $^ { + + }$ dataset

(3) DF-TIMIT 数据集

DF-TIMIT 数据集采用 FaceSwap-GAN 伪造方法，使用来自 VidTIMIT 数据集的真实人脸视频进行人脸交换，生成高清画质视频和低清画质视频各 320 个，同时通过使用直方图归一化以及改变各种外在条件的方式来调整伪造视频中的噪声。如图 1.3 为DF-TIMIT 数据集示例，其中从左到右依次为从 VidTIMIT 数据集中选取的真实视频截图、经过换脸后的低清画质视频截图、经过换脸后的高清画质视频截图。

![](images/39de091159999ed43ab96b0342a00acb4e6f1a26913c592aa52b64f543b17a0b.jpg)  
图 1.3 DF-TIMIT 数据集三种清晰度Fig. 1.3 Three definitions of DF-TIMIT dataset

# 1.3.2 本文构建数据集

本文使用了两个数据集分别是 Iris 图像数据集以及 FSV 数据集。Iris 图像数据集用来训练瞳孔分割网络，FSV 数据集用来对提出的基于特征融合的伪造检测网络进行训练及测试。

# (1) Iris 图像数据集

Iris 图像数据集来自中国科学院自动化所虹膜数据库(CASIA iris image database，CASIA-Iris)[47]。分别从 CASIA-Iris 的自数据库 CASIA-IrisV2，CASIA-IrisV3 以及CASIA-IrisV4 中各取 900 张虹膜图像一共得到 2700 张真实人眼样本。数据总体构成如表 1.1 所示。

表 1.1 数据集构成  
Table 1.1 The composition of the dataset   

<html><body><table><tr><td>来源</td><td>CASIA-IrisV2</td><td>CASIA-IrisV3</td><td>CASIA-IrisV4</td></tr><tr><td>数量</td><td>900</td><td>900</td><td>900</td></tr><tr><td>总张数</td><td></td><td>2700</td><td></td></tr></table></body></html>

使用 labelme 标签标定软件对数据集中的样本进行标签标定，同时通过对图像进行旋转缩放、亮度增强、噪声扰动，对数据集进行扩充。在对图像进行缩放时，使用OpenCV 提供的 getRotationMatrix2D 方法求得仿射矩阵，以图片中心为旋转中心，顺时针旋转 $3 0 ^ { \circ }$ °，缩放 0.5 倍。使用 OpenCV 提供的 cv2.LUT 函数对图像进行亮度增强。向图像中添加均值为 0，方差为 0.001 的高斯噪声。在保证图像不产生较为明显的形变的前提下利用上述方式将其扩充为原来的 4 倍。

标定示例如图 1.4 所示。绿色区域标注为瞳孔区域，红色区域标注为除瞳孔外虹膜其它区域。

![](images/ec8df0743cdb032c6d9fa3192c923fe7c2b2136a80c39cd2f11fccec3eb188b3.jpg)  
图 1.4 部分数据集及对应标注  
Fig. 1.4 Part of the dataset and corresponding annotations

(2) FSV 数据集

在 FaceForensics $+$ ＋数据集中只有 DeepFakes、Faceswap 和 FaceShifer 属于人脸交换伪造方法，因此本文以 FaceForensics $+$ ＋数据集中的 DeepFakes、Faceswap 和FaceShifer 数据集为基础，构建实验数据集。

通常情况下数据集中的伪造视频质量高低不一，有些生成视频效果不好，肉眼即可分辨真伪，只取量不求质会影响网络训练效果。因此本文对 FaceForensics＋＋数据集进行人工筛选，剔除有明显伪影的虚假视频，最终构建出包含 2950 段人脸视频，共计 66000 张人脸图像的人脸交换视频数据集，并命名为 FSV(Face Swapping Video)数据集。数据集总体构成如表 1.2 所示。

具体来说，从来自 You Tube 的 1000 个真实视频中，每段视频随机抽取 15 帧，并在 VidTIMIT 数据集 320 个真实视频中随机选取 300 个视频，从每个视频中抽取 10帧，一共得到 36000 张真实人脸数据集，训练集与测试集的比例为 4:1。并选取

FaceForensics＋＋数据集中两种画质(HQ、LQ)的 DeepFake、Faceswap、FaceShifter 数据集各 550 个视频，每段视频随机抽取 10 帧，一共得到 30000 张虚假人脸图像，同样按照4:1 的比例划分训练集与测试集。表1.3 展示FSV 数据集信息。

表 1.2 数据集构成  
Table 1.2 The composition of the dataset   
表 1.3 FSV 数据集信息  

<html><body><table><tr><td>类别</td><td>来源</td><td>总数量（张）</td><td>训练集数量（张)</td><td>测试集数量（张)</td></tr><tr><td>真脸图像</td><td>You Tube Vid-TIMIT</td><td>36000</td><td>28800</td><td>7200</td></tr><tr><td>假脸图像</td><td>FaceForensics++</td><td>30000</td><td>24000</td><td>6000</td></tr></table></body></html>

Table 1.3 Information of FSV dataset   

<html><body><table><tr><td rowspan="2">数据集</td><td colspan="2">HQ</td><td colspan="2">LQ</td></tr><tr><td>训练集</td><td>测试集</td><td>训练集</td><td>测试集</td></tr><tr><td>DeepFakes</td><td>4000</td><td>1000</td><td>4000</td><td>1000</td></tr><tr><td>Faceswap</td><td>4000</td><td>1000</td><td>4000</td><td>1000</td></tr><tr><td>Faceshifter</td><td>4000</td><td>1000</td><td>4000</td><td>1000</td></tr><tr><td>总计</td><td colspan="2">15000</td><td colspan="2">15000</td></tr></table></body></html>

# 1.4 本文使用平台

本文面向人脸交换的鉴别技术研究是在深度学习框架 Pytorch 下完成的，使用Windows10 操作系统，编程语言为 Python3.7。

关于深度学习网络架构的选择，Pytorch 是一个基于 Torch 的 Python 开源深度学习框架，它支持 Python 语言编写，而且程序运行不需要很多繁琐的步骤，在运行时使用者可以清晰的看到出现错误的代码具体在哪一行，易于操作者调试错误，且灵活性强、效率高。

关于编程语言的选择，与其它语言相比 Python 是所有编程语言当中，代码量最低的，非常易于读写。而且 Python 是免费开源的，可以兼容的平台众多，使用起来比其它语言更加的方便简单。

OpenCV 是一个计算机视觉和机器学习软件库，可以在 Windows、Linux、Android和 Mac 操作系统上运行。它由一系列 C 函数和几个 $\cot +$ 类组成，同时还提供Python、Ruby、Matlab 等语言的接口，在图像处理和计算机视觉中实现了许多通用算法。

# 1.5 本文组织结构

本文主要针对人脸交换伪造检测技术进行研究，设计检测算法对换脸图像以及换脸视频进行检测，本文各章的主要安排如下：

第一章 绪论。对人脸交换伪造检测这一课题的研究背景及意义进行阐述，总结分析并简要评述人脸伪造技术以及人脸伪造检测技术的国内外研究现状，介绍本文实验中所使用的数据集以及实验所使用的相关平台，最后对本文的组织架构进行简单说明。

第二章 人脸交换伪造与检测相关技术。本章从人脸交换伪造技术和人脸交换伪造检测技术两方面进行阐述，分别简述了三种常见的人脸交换伪造技术 DeepFake、FaceSwap-GAN、Faceshifter 的原理，并在此基础上分析研究了深度伪造人脸主流的检测方法，包括 XceptionNet、MesoNet、X-Ray 三种方法，并对伪造检测技术的相关基础性知识进行了简述，为后续的研究工作打下坚实的基础。

第三章 基于人眼瞳孔特性的 FaceSwap-GAN 人脸伪造检测。本章对 U-Net 模型进行改进，利用人眼角膜反光特性对 FaceSwap-GAN 生成的虚假人脸图像进行伪造检测。首先对本文提出的鉴伪方法流程进行介绍，对其中涉及到的各个模块进行功能讲述，最后通过对实验结果的对比分析，以及个例研究，对本文提出的方法的检测性能进行分析并得出结论。

第四章 基于特征融合的视频人脸伪造检测。本章对前文所述内容继续进行研究，提出一种基于特征融合的视频人脸伪造检测方法，设计一种双流网络，结合视频帧内特征信息与视频帧间差异信息，先依次对提取帧内信息的网络、提取帧间差异信息的网络以及Softmax 分类器进行介绍，最后通过实验结果分析得出相关结论。

第五章 总结与展望。本章主要对全文主要内容进行总结与归纳，并根据实验过程中的心得体会对本文设计的两种鉴伪算法中的不足进行了描述，同时据此对今后的工作进行展望。

# 本章小结

本章首先对本文所选课题的研究背景及意义进行阐述，其次总结分析并简要评述了人脸伪造技术以及人脸伪造检测技术的国内外研究现状，然后介绍本文实验中所使用的数据集以及实验所使用的相关平台，最后对本文的组织架构进行简单说明。

# 第二章 人脸交换伪造与检测相关技术

# 2.1 引言

近年来，基于人脸局部信息的替换变得越来越流行，各种伪造的国家领导人在公开场合发表言论的虚假视频、上市公司高管发布的不实视频新闻、各种伪造的网红明星娱乐搞笑视频层出不穷。随着基于深度学习的人脸伪造技术的不断成熟，让目标对象说出未曾说过的话以及做出未曾做过的表情变得可能，以此技术伪造的作品逼真度极高，对网络环境安全性造成极大的威胁，因此，对伪造检测技术的研究迫在眉睫。深度伪造人脸技术包含人脸生成和人脸编辑两大类技术，基于人脸局部信息的替换主要使用的是人脸编辑技术，人脸编辑技术中人脸交换技术是最常见的一种人脸编辑方式，本文主要针对人脸交换技术进行研究。

人脸交换技术是一种将目标人脸与源人脸进行替换以合成新的人脸的方法，常见的人脸交换技术包括：原始 DeepFakes、FaceSwap-GAN、FaceShifter 等，本章对这三种人脸交换伪造方法进行介绍，并介绍主流的人脸交换伪造检测技术，以便于在后文中与本文设计的鉴伪算法的检测效果进行对比与分析。

# 2.2 人脸交换伪造技术

# 2.2.1 DeepFake

DeepFake 最早来源于一个名为“DeepFakes”的 Reddit 社交网站用户名。通常人们将DeepFake 理解为深度伪造人脸技术，但有时也将 DeepFake 特指为以自动编码器为核心，可用于生成换脸图像和视频的人脸交换技术。本部分主要对特指的 DeepFake 技术原理进行介绍。

自动编码器是神经网络的一种，由编码器和解码器两部分构成，最基本的自动编码器由一个输入层、一个隐藏层和一个输出层组成，是一个简单的三层神经网络，其中输出层和输入层的维数相同。图2.1 展示的是自动编码器基本结构。

通常情况下，编码器将输入层数据 $x \in \mathrm { X }$ 映射到 $\boldsymbol { \mathrm { h } } \in \mathbb { F }$ ，如公式（2.1）所示。

$$
\boldsymbol { \mathrm { h } } = \sigma ( W \boldsymbol { x } + b ) 
$$

其中，h表示输入层得到的潜在特征向量(Latent)， $\sigma$ 代表激活函数，ܹ是权重矩阵，b表示偏置向量。自动编码器对权重和偏置进行随机初始化，并在训练过程中迭代更新。在解码过程中，解码器通过h在输出层输出 $x ^ { \prime }$ ，完成对输入数据 $x$ 的重构过程，

如公式（2.2）所示。

$$
x ^ { \prime } = \sigma ^ { \prime } ( W ^ { \prime } h + b ^ { \prime } )
$$

自动编码器通过误差的反向传播进行训练，在训练过程中会不断对两组 $( W , ~ b )$ 进行训练，直至二者误差达到全局最小值时收敛，如公式（2.3）所示。

$$
L ( x , x ^ { \prime } ) = \| x , x ^ { \prime } \| ^ { 2 } = \left\| x - \sigma ^ { \prime } ( W ^ { \prime } { \bigl ( } \sigma ( W x + b ) { \bigr ) } + b ^ { \prime } \right\| ^ { 2 }
$$

![](images/49c491d62988179cc74446e675e67fcff074f412374d78ea6c14ddbee5baf230.jpg)  
图 2.1 自动编码器结构图  
Fig. 2.1 AutoEncoder structure diagram

2017 年出现 DeepFake 色情视频，名人的面孔被换成色情演员的面孔，也就是FakeApp 的前身。一年后使用 FakeApp 制作的美国前总统奥巴马的 DeepFake 演讲视频掀起波澜[49]。随后一种能随意编辑虚假图像的方法被提出，它可以随意编辑人脸的肤色、头发颜色和背景内容[50]，与此前的虚假人脸生成方式不同，这是一种重大突破。同年，还提出了一种真实头部说话神经模型的少样本对抗学习方法，此方法基于GAN，利用小样本学习训练，只要输入一张人物图像就可以生成人物图像开口说话的动图[51]。几年后一款名叫 WomBo AI 的应用在网络上引起关注，该应用可以将任意人物的几张照片制成一个简短的视频片段，其基于学习真实表演者录制的视频中的知识，再使用 DeepFake 技术将学习到的人物知识与目标人物所处场景结合起来完成视频制作。2022 年泽连斯基的 DeepFake 视频事件引起各界担忧，视频中伪造的乌克兰总统泽连斯基呼吁民众放下武器[52]。DeepFake 技术的应用在未来是否会对国际形势造成重大影响引起了各界重视。

图 2.2 展示了以自动编码器为核心的 DeepFake 网络生成伪造人脸图像的过程。DeepFake 网络由一个编码器和两个解码器构成。编码器的作用是将输入人脸的属性进行编码，得到面部特征潜在表示，解码器的作用则是将潜在表示解码还原成人脸图像。DeepFake 网络中的两个解码器 Decoder A 和 Decoder B 分别对两张人脸编码进行解码，Decoder A 只能将属性编码还原成 Face A 的长相，Decoder B 同理。在训练阶段使用两组人脸图像(Face A 和 Face B)，Face A 为源人脸图像，是原始视频中即将被替换的人脸图像，Face B 为目标人脸图像，是准备被替换到视频中的人脸图像。为了能使得模型在训练时学到源人脸图像和目标人脸图像共同的潜在特征，因此在训练阶段两个编码器的权重共享，再通过使用权重不同的解码器分别对两组人脸图像进行数据重构。在生成阶段，Face A 通过编码器得到潜在特征表示 Latent Face A，Latent Face A 经过 Decoder A 得到 Face A，Latent Face A 经过 Decoder B 得到具有 Face A 属性和 FaceB 长相的图像，实现深度伪造。

![](images/6ebd166523b7c4ad2c06907d924cb95da3536dba27be0ae720a19f6e97ced8ad.jpg)  
图 2.2 DeepFake 网络示意图  
Fig. 2.2 Schematic diagram of DeepFake

# 2.2.2 FaceSwap-GAN

FaceSwap-GAN[27]将 GAN 引入生成技术，使得生成的换脸图像或视频真实性更高，与之前的伪造网络不同，FaceSwap-GAN 可以实现任意两张人脸图片之间较好的换脸效果，而不需要提前对面孔进行训练，此外由于引入了分割模型，所以 FaceSwap-GAN 在应对侧脸的时候也展现了较好的伪造效果，而且 FaceSwap-GAN 不仅可以完成人脸交换还能出色地完成表情迁移任务。

生成式对抗网络 GAN 一般由生成模型 G 和判别模型 D 构成，其中生成模型 G 从潜在空间(Latent space)中随机取样作为输入，其输出结果需要尽量模仿训练集中的真实样本，而判别模型 D 的输入则是真实样本或者生成模型的输出，其任务是需要将生成模型的输出与真实样本尽可能的区分出来。训练过程中生成模型和判别模型的权重交替更新，两个模型在相互对抗中共同进步。GAN 结构图如图 2.3 所示。

![](images/b538f8b07467cbe80210cdb9380dd1f2e5411b77f165d55510ffb8bd3719f801.jpg)  
图 2.3 生成对抗网络结构  
Fig. 2.3 Generating a confrontation network structure

FaceSwap-GAN[53]总体结构由三个生成模型和一个分割模型组成，如图 2.4 所示为FaceSwap-GAN 结构概述。其中 $I _ { s }$ 是源人脸， $I _ { t }$ 是目标人脸， $\mathrm { G } _ { \mathrm { r } } , \ \mathrm { G } _ { \mathrm { c } } , \ \mathrm { G }$ $\mathbf { G } _ { \mathrm { b } }$ 是三个生成模型， $\mathbf { G } _ { \mathrm { s } }$ 是分割模型。首先根据源图像 $I _ { s }$ 和目标图像 $I _ { t }$ ，基于 3D 人脸面部地标检测器分别提取 $I _ { s }$ 、 $I _ { t }$ 的人脸框、面部关键点、欧拉角这三个指标，然后利用重演模块 $\mathrm { G } _ { \mathrm { r } }$ 根据提取的欧拉角和 $I _ { s }$ ，生成姿态角度与 $I _ { t }$ 一致但保存 $I _ { s }$ 特征的人脸 $I _ { r }$ 。然后通过分割模块 $\mathbf { G } _ { \mathrm { s } }$ 分别对生成的图片 $I _ { r }$ 和目标图像 $I _ { t }$ 进行分割操作，得到人脸和头发的 mask 区域，然后通过修复模块 $\mathbf { G } _ { \mathrm { c } }$ 对人脸区域进行修复，得到 $\dot { I } _ { c }$ ，最后基于融合模块 $\mathbf { G } _ { \mathrm { b } }$ 、目标图 $I _ { t }$ 、修复后的人脸区域图像 $I _ { c }$ 、mask 图进行融合，得到换脸后的图 $I _ { b }$ 。

![](images/60a7def65a42cb92d0e1f959684ef76d2132d7e77bf1e86e6eafc7ef495fcb4a.jpg)  
图 2.4 FaceSwap-GAN 示意图[53]Fig. 2.4 Diagram of FaceSwap-GAN[53]

# 2.2.3 FaceShifter

上述两种换脸方法都存在当原始图像出现遮挡，例如图像中存在帽子、头饰、眼镜等饰物，但最终生成图像中却不包含遮挡物这种情况。为了解决这种问题 Li 等人提出了 FaceShifter[53]，一种包含两个阶段的人脸交换框架。第一阶段网络用于生成初步的换脸图像，第二阶段的网络则负责在第一阶段生成的换脸图像上进行细化，为换脸后的图像增加遮挡物如眼镜、发饰、帽子等。其示意图如图 2.5 所示。

![](images/fed2bdc646387db5a72a386338ff358fef9386e8072f88efb8160d123c88d920.jpg)  
图 2.5 FaceShifter 示意图Fig. 2.5 Schematic of FaceShifter

FaceShifter 网络第一阶段使用自由适应嵌入融合网络 AEI-Net，AEI-Net 网络用来获取源人脸图像I 的身份特征和目标人脸图像I 的属性特征，然后将两种信息融合，通过自适应的方式生成初步的人脸交换图像X′，第二阶段使用启发式误差识别优化网络

HEAR-Net，HEAR-Net 网络的作用是处理此前换脸过程中产生的人脸遮挡问题，例如源人脸图像I 没有带眼镜，而目标人脸图像I 带了眼镜并且遮挡一部分人脸，但是生成图像 $X ^ { \prime }$ 中眼镜却出现部分消失，这种情况就需要经过 HEAR-Net 网络优化处理，最终得到高质量且有遮挡的人脸交换图。

# 2.3 人脸交换伪造检测技术

# 2.3.1 检测技术基础

(1) 深度神经网络

深度神经网络(Deep Neural Network，DNN)是深度学习领域中最为基础也是最为常见的一种网络。按照不同层所处的位置进行划分，深度神经网络的内部结构可以分为：输入层，隐藏层和输出层，输入层在前，输出层在后，而隐藏层在中间。层与层之间采用全连接的方式，即所处在第 n 层的任意一个神经元会与所处在第 $\mathfrak { n } { + } 1$ 层的任意一个神经元相连。如图 2.6 是深度神经网络的结构示意图，其中 X 是输入，Y 是输出，在输入层和输出层之间的隐藏层以及中间的连接，负责将输入数据推算成合理的值并输出。

![](images/20b56ec5bacd507d8a9f19142e2cfab90fb48973cf677293afc4af5d54b5efea.jpg)  
图 2.6 深度神经网络示意图  
Fig. 2.6 Diagram of DNN

数据从输入层流动到输出层，与图中网状箭头的方向一致，每个箭头前一层到后一层的计算可以用公式（2.4）表示。

$$
y = w x + b
$$

其中ݓ称为权重(weight)， $b$ 为偏置(bias)，每个箭头都有单独的ݓ、 $b$ 。后一层节点的值等于前一层节点经过（2.4）公式计算后的值。

为了使得上述的线性函数变为非线性函数，让网络内部更加复杂，在每个节点输出数据之前对其使用一个非线性函数运算，例如sigmod或者 ReLU 函数。最后节点输出如公式（2.5）所示。

$$
y = \operatorname { s i g m o d } ( \sum ( W _ { n } * x + b _ { n } ) )
$$

最后经过 Softmax，输出的是一个类别的概率，取值范围为[0,1]，且所有输出的值和为 1。如公式（2.6）所示，公式中 $a _ { i }$ 表示输出层的输入信号， $n$ 表示输出层一共包含$n$ 个神经元，计算结果 $S _ { i }$ 表示第݅个神经元的输出。

$$
S _ { i } = \frac { e ^ { a _ { i } } } { \displaystyle \sum _ { j = 1 } ^ { n } e ^ { a _ { j } } }
$$

(2) 卷积神经网络

卷积神经网络(Convolution Neural Network，CNN)是一类功能很强大的神经网络，可以进行图像数据处理。卷积神经网络在神经网络之中添加卷积操作，实现对输入图像的特征提取。它需要的参数少，并且卷积计算也很容易用 GPU 并行计算，因此被研究者所青睐。网络示意图如图 2.7 所示。

![](images/b82ce6490da2a528c8bd8a5ed0a967c2fec5bc17404231ec7e41d23e9cf4ced1.jpg)  
图 2.7 卷积神经网络示意图Fig. 2.7 Diagram of CNN

卷积神经网络的基本结构包含：卷积层、池化层、全连接层等。通过将这些层堆叠起来，就可以构建一个具有图像数据处理功能的卷积神经网络。

卷积层是卷积神经网络中最为重要的一部分，卷积神经网络中绝大部分的计算量都在卷积层产生，它主要是用一个采样器从输入中采集关键数据内容，在实际当中往往将卷积层与激活函数ReLU 层共同称之为卷积层。

池化层的使用可以逐渐的降低空间的尺寸，这样可以减少网络中的参数数量，减少计算资源的耗费，避免产生过拟合现象。在实际应用中通常会在几个连续的卷积层之间插入池化层。

全连接层一般在输出层之前使用，与常规神经网络中的全连接层一样，其作用是将经过前面卷积以及池化操作后的特征进行整合，并进行归一化处理，输出对各种分类情况的概率值，分类器则会依据全连接层输出的概率进行分类。

卷积神经网络最初用于手写字体识别任务，错误率为 $1 \% ^ { [ 5 5 ] }$ ，后来 AlexNet[56]网络横空出世，其在 ImageNet 图像分类大赛中拔得头筹，分类精度远超传统模型，至此以后卷积神经网络朝着更深的方向发展，经典的深层网络有 $\mathrm { V G G } ^ { [ 5 7 ] }$ ，但也有不少研究者致力于如何提升卷积网络各个模块的性能，因此诞生了 InceptionNet[58]。但是随着网络深度的不断增加，卷积网络反而出现了网络退化、梯度消失等问题，为了解决这一问题有研究者基于残差结构设计了ResNet 网络[59]。

而在深度人脸伪造领域，由于在进行人脸伪造时会受到计算机资源、样本质量、训练时长等外部因素的限制，通常只能够通过对人脸面部中心区域进行像素压缩或者进行仿射变换的方式对面部进行篡改，因此在伪造人脸图像中会出现人脸面部中心区域和人脸外部边缘区域分辨率不一致这种情况。而对于深度伪造检测来说，可以利用这种特殊性，使用卷积神经网络学习样本的相关特征，以此来对伪造人脸图像进行检测。

# 2.3.2 主流伪造检测方法

(1) XceptionNet

XceptionNet[60]旨在缓解随着网络的加深导致的参数过多问题，XceptionNet 在Inception-V3 的基础上加入 Bottleneck 的结构，降低参数量。为了能够在提升网络性能的同时减小网络的计算复杂程度，研究人员使用深度可分离卷积对特征图进行空间卷积，再使用 $1 \times 1$ 卷积让每个特征点学习到来自不同通道的特征信息。

XceptionNet 网络具有网络复杂度低、性能高等优点。网络一经提出就有研究者对其性能进行了实验验证，结果也证实了深度可分离卷积在虚假人脸伪造检测上的有效性。XceptionNet 模型具体结构如图 2.8 所示，从图中可以看到，XceptionNet 模型包含3 个 flow，每个 flow 由一个个 block 组成，每个 block 由残差网络与 SeparableConv2D层组合而成，block 与 block 之间采用 residue connection 的连接方式，即图中的加号。Entry flow 由 4 个 block 构成，Middle flow 由 8 个 block 构成，Exit flow 由 2 个 block 构成。

![](images/e820f2e6e9dde6afa3cab3e56cab317cd775d8de4576879e5adf016a1c933cb3.jpg)  
图 2.8 Xception 网络结构示意图Fig. 2.8 XceptionNet structure diagram

(2) MesoNet

传统的图像伪造检测技术难以在伪造视频检测中也展现较好的性能，Afchar 等人认为产生这种结果的原因是视频压缩而导致的噪声减弱，因此不能再单纯的通过对单帧图像进行噪声分析来判断一段视频是否为伪造视频。因此 Afchar 等人提出了 Meso-4和 MesoInception- $\boldsymbol { \cdot } \boldsymbol { 4 } ^ { [ 3 9 ] }$ 这两个检测网络来解决上述问题。如图 2.9 所示。从图 1）中可以看到 Meso-4 网络的构成，它一共有 6 个层，分别是 4 个卷积层和 2 个全连接层。与Meso-4 网络相似，MesoInception-4 网络在 Meso-4 网络的基础上进行优化，将前两层的普通卷积换成了 Inception 模块，并将传统 Inception 模块中 $5 { \times } 5$ 的普通卷积替换成$3 { \times } 3$ 的扩展卷积，避免在进行特征提取时提取到高语义特征；同时还在 $3 { \times } 3$ 的扩展卷积之前加入了 $1 \times 1$ 的卷积层，以此来减少参数降低计算量。图 2）中展示的是

MesoInception-4 网络中所使用的改进后的 Inception 模块的具体构成。

![](images/e1825880119d8009e6e44b1134db5a2740ab27865d1f8591b0a23a666493544b.jpg)  
图 2.9 MoseNet 网络结构图示意图Fig. 2.9 MoseNet structure diagram

(3) X-Ray

X-Ray 面部检测算法[61]由 Li 等人基于伪造图像中存在的混合边界提出。X-Ray 为一张灰度图像，从这个灰度图像中，可以知道输入的人脸图像是否可以视为两幅图像的混合，以此来判断输入图像是否是伪造图像。实验证明该算法在 FaceForensics＋＋上的 AUC 值得到了 0.9852 的高分，同时作者采用融合伪造 BI 进行交叉验证，验证了X-Ray 面部检测算法对于没见过的人脸伪造技术的伪造鉴别能力。

# 本章小结

本章从深度伪造人脸的生成和检测两个方面，分别介绍了人脸交换伪造技术以及人脸交换伪造检测技术相关基础原理，以及主流的人脸交换检测网络，并对这些方法进行了评述。

# 第三章 基于人眼瞳孔特性的FaceSwap-GAN 人脸伪造检测

# 3.1 引言

随着人工智能技术的快速演变，伪造乱象层出不穷，也越来越难以用肉眼发现。原始的 DeepFake 框架通用性较差，伪造出的人脸图像或视频质量不高，往往会有五官错位、模糊、肤色不匹配等等问题，而深度伪造技术因生成式对抗网络 GAN 的出现，生成的伪造人脸图像、视频的质量都得到了质的飞跃，一系列与 GAN 结合的生成网络应运而生，如 DCGAN、ProGAN、StyleGAN、FaceSwap-GAN 等。一些团队也开始开发一些深度伪造项目和工具，其中 FaceSwap-GAN 因引入感知对抗损失函数，有效消除了深度伪造过程中图像噪声问题，能生成更高质量的图像与视频。有人利用该技术在社交平台上用由 FaceSwap-GAN 合成的人脸图像作账户头像，以此来欺骗不知情的用户；还有网友恶搞明星制作低俗照片或视频，侵犯其肖像权，影响其声誉；更有甚者利用虚假图像视频操控政治，对国家之间友好交往造成影响。因此，伪造检测技术发展显得尤为重要。

以往针对 GAN 合成的人脸伪造检测技术，大多都是提取特征线索，然后训练分类器将其与真实图像区分开来，但是这些算法普遍存在缺陷，比如结果缺乏可解释性、鲁棒性较低、泛化性较差等。为了消除这些限制并探索出更加稳健的模型，研究人员不断进行探索，来自纽约州立大学的团队[63]提出了一种全新的方法，利用研究发现的GAN 生成人脸图像中瞳孔形状的不规则性进行人脸伪造检测，Shu Hu 等人[63]也提出可以利用人眼角膜特性进行伪造检测，这些基于人体生理和物理学特征的检测方法，以找寻的人体生物、物理学规律为依托，所以检测结果能提供直观的解释，而且对抗攻击也更加的稳健。但这些方法检测性能相对较低，并且对检测的人脸图像要求较高。对伪造模型生成的正脸高清伪造人脸图像可以做到较为有效的鉴别，但是对于质量较低的伪造人脸图像，以及存在一些小角度偏转的侧面人脸图像，则不能准确进行伪造检测，检测结果的精确度有待提升，检测时的漏检率也较高。

受到此类方法的启发同时为解决上述问题，本章在 Shu Hu 等人[63]提出的方法的基础上进行改进，提出一种新的基于人眼生理、物理学规律的 FaceSwap-GAN 人脸伪造检测方法，这种方法利用伪造人脸图像两眼之间的角膜镜面高光不一致性，来判断输入图像是否为伪造人脸图像，针对人眼图像虹膜分割精度低这一问题，本章通过改进U-Net[64]分割算法，在 U-Net 网络的编码阶段添加 SK-ResNext 模块，该模块在ResNext[65]模块后级联 SKNet[66](Selective Kernel Networks)模块，使得网络可以关注到更多尺度的特征，学习到更多更有用的特征信息，使得分割网络可以更快更精准地分割出人眼虹膜区域，接着使用自适应阈值的方式从分割出的人眼虹膜图像中提取角膜高光，通过计算左右眼角膜高光相似度分数进行伪造检测。

# 3.2 人眼的生物物理学规律

如图 3.1 所示是人眼解剖学结构图。从图中可以看出人眼的中心是虹膜和瞳孔，眼角膜是覆盖在虹膜外层的一层透明薄膜，角膜整体呈球形，表面具有反光性，当人眼捕获物体时，环境中的发光源或反射物在眼角膜上产生的光斑就称为角膜镜面高光。

![](images/54a148eb778a7dd4d94831787e9820a46c634dd14f1dafe92ff1e5021bf7fdfc.jpg)  
图 3.1 人眼解剖学结构图Fig. 3.1 Anatomy of a human eye

如图 3.2 所示，为真实人脸与 FaceSwap-GAN 合成人脸的角膜镜面高光对比图。

![](images/04d540a10c3fd1e86bbdde2ac5c65910cdf4978b32f1ce0c2edf51734d00239c.jpg)  
图 3.2 真实人脸（上）和 FaceSwap-GAN 合成人脸（下）的角膜镜面高光

Fig. 3.2 Corneal specular highlights for a real human face (up) and a FSGAN synthesizes face (below)

从图中可以看出，真实的图片由于被拍摄者两只眼睛看到的是同一场景，因此两眼的角膜镜面高光具有很强的相似性。FaceSwap-GAN 由于继承来自训练模型的真实人脸图像中的人脸特性，生成的人脸图像眼睛也有反光，但是生成的两眼角膜的反光呈现出不一致性。

# 3.3 本文方法

# 3.3.1 整体流程

为了利用 FaceSwap-GAN 因缺乏对人脸解剖学的理解，而导致生成的人脸图像中角膜高光点不一致这一特点。本文提出一种方法来自动比较两只眼睛的角膜镜面高光并评估它们的相似性。图 3.3 展示了本方法的基本流程。

![](images/c15094a0bbe0eb114dd1fea43c86bb0b76ee9aedf37ce3a1db64b1116a2b777c.jpg)  
图 3.3 获得角膜高光的基本流程图  
Fig. 3.3 Basic flow chart for obtaining corneal specular highlight

对于输入图像，首先截取眼部区域，然后运行人脸地标提取器 Dlib 来获取人脸面部地标关键点，如图 3.3 所示提取了眼睛上 6 个关键点地标信息，然后将两只眼睛的对应区域剪裁出来，在对两只眼睛对应区域进行适当裁剪后，使用改进的 U-Net 模型提取虹膜区域及其边界，得到分割后的虹膜区域，再使用自适应图像阈值分割方法分离角膜高光，将提取出的两只眼睛的角膜高光区域对齐，左眼提取的角膜高光记为 RL,右眼提取的角膜高光记为 RR，使用二者的IOU分数作为两眼相似度分数，IOU分数的取值范围为[0,1]，值越小表明 RL 与 RR 的相似性越低，因此输入图像更有可能是FaceSwap-GAN 生成的人脸图像。

# 3.3.2 虹膜分割

本文在 U-Net 的基础上进行改进，在模型编码阶段引入 ResNext 模块，用来进行特征提取，并在 ResNext 模块后级联 SKNet 模块，SKNet 模块可起到注意力机制作用，以非线性的方式聚合来自多个卷积核的信息，实现神经元自适应感受野大小，让网络可以关注到更多更有用的信息，提升网络的泛化能力，进一步提升虹膜分割的精确度。

# (1) 基本框架

基于 U-Net 的改进虹膜分割算法模型如图 3.4 所示。改进后的 U-Net 网络有预处理、编码器、解码器以及跳层连接四部分构成。

![](images/e77ba307db092a80b28b280ebbd3a620ee2d7dcb02a90b61550192d54fa0b4b3.jpg)  
图 3.4 基于改进 U-Net 的虹膜分割网络整体结构Fig. 3.4 Overall network structure of improved U-Net

具体来说：首先通过预处理模块将输入图像的通道数加深到 64。接着将图像输入到编码器中，编码器由串联的 SK-ResNext 模块和下采样层构成，SK-ResNext 模块由ResNext 模块级联 SKNet 模块组成，下采样层则使用步长为 2，大小为 $2 { \times } 2$ 的最大池化层进行下采样。编码器阶段一共有四个部分，第一部分通道数为 64 的特征图经过下采样层到达 3 个相串联的 SK-ResNext 模块，输出通道数为 128 的特征图，随后再经过一个下采样层以及 4 个串联的 SK-ResNext 模块将通道数扩展为 256，接着再经过一个下采样层和 6 个串联的 SK-ResNext 模块将通道数扩展为 512，最后再经过一个下采样层和 3 个串联的 SK-ResNext 模块将通道数扩展为 1024，完成编码阶段。解码器由卷积块和上采样层构成，与经典 U-Net 网络不同，为了达到减少计算量和参数量进而简化模型的目的，本模型去掉了解码器部分的 $3 { \times } 3$ 卷积层。上采样层选择使用反卷积层，其大小为 $2 { \times } 2$ ，步长为 2。同时采用跳层拼接的方式将上采样之后的特征图与相对应的具有相同分辨率的编码器输出的特征图进行连接，解决上采样操作之后产生的部分虹膜信息丢失问题。

$\textcircled{1}$ SK-ResNext 模块

为了更准确地完成虹膜图像分割任务，本文对 U-Net 网络编码阶段的基本模块进行改进，将 SK-ResNext 模块引入到编码阶段。图 3.5 为 SK-ResNext 模块的网络结构。

![](images/448e6b71180ba6eaae1fd2b1fed5d43aac29091c0b64a97bad66bbef8e19f614.jpg)  
图 3.5 SK-ResNext 模块网络结构  
Fig. 3.5 Network structure of SK-ResNext module

SK-ResNext 模块由 ResNext[65]模块后级联 SKNet[66]模块得到，其首先通过ResNext 模块进行特征提取，得到特征图，将得到的特征图分别送入到一个 $3 { \times } 3$ 的卷积层和一个 $5 { \times } 5$ 的卷积层，将经过卷积操作得到的特征图相加得到新的特征图，采用全局平均池化的方式对新的特征图进行压缩，得到大小为 $1 \times 1 \times \mathrm { C }$ 的实数列，接着将得到的实数列进行全连接操作，最后将原始各个通道上的输出与 a、 $\boldsymbol { \mathbf { b } }$ 两个激活函数相乘，最终得到带有注意力标定的新的特征图。

$\textcircled{2}$ ResNext 模块

ResNext 架构利用 Inception 的先拆分再转换最后合并的策略，最早被应用于图像分类领域。ResNext 架构改变了残差学习的单路径卷积方式，通过采取多路径分组卷积的方式提升网络性能。其具有网络结构简单、准确率高、参数量少、便于移植等优势，本章选择将其引入编码器改进 U-Net。

![](images/42143252aa651f8b06348cebc0e358cf1ae1ce24f5e75aa8b1d1dc9c45d1f1e9.jpg)  
图 3.6 ResNext 网络结构Fig. 3.6 Network structure of ResNext

图 3.6 为 ResNext 的网络结构图，图中 Path 表示分支数。计算如公式（3.1）所示。  
其中 $x$ 表示输入特征， $T _ { i } ( x )$ 为一个任意函数， ܿ表示网络输入宽度。

$$
\textstyle y = x + \sum _ { i = 1 } ^ { c } T _ { i } ( x )
$$

该网络输入有 256 个通道，通过使用 $1 \times 1$ 卷积，将输入拆分为低维张量，为了让分支具有相同的网络结构，选择尽可能多的使用分支并且不限制卷积核的大小。本章选择的分支数为 32，应用 $3 { \times } 3$ 的卷积层对数据进行转换，使用 $1 \times 1$ 卷积将变换输出的通道数目从 4 增加到 256，再对 32 个通道数为 256 的张量进行求和。对于 32 个分支，每个分支都是先将通道数缩小到 4 个通道，然后在不改变通道数的情况下接入 $3 { \times } 3$ 的卷积，最后将通道数恢复到 256，再对 32 个分支的输出张量进行求和，最后利用跨层连接将输入添加到结果中。

$\textcircled{3}$ SKNet 模块

为了能让 ResNext 网络在进行特征提取时获取到不同大小感受野的信息，学习到更多有用的特征，从而使得虹膜分割的准确率更高，本章将 SKNet 引入到网络中，通过添加 SKNet 模块，让网络对卷积核执行注意力机制，使得网络可以自适应选择合适大小的卷积核，因此对于输入图像，利用 SKNet 模块使得网络能够实现自适应感受野大小，从多种不同尺度的卷积核中聚集信息，让模型学习到更多有效特征，从而提升网络性能。SKNet 结构如图 3.7 所示。

![](images/c2c615cefb8b0e7377f6ee50b713080dc5a1ff0d1c0c41f4ef44e5e8e05ab755.jpg)  
图 3.7 SKNet 网络结构[66]Fig. 3.7 Network structure of SKNet[66]

SKNet 卷积有 3 个步骤：Split（拆分）、Fuse（融合）、Select（选择）。如图 3.7 所示，Split 操作是将输入特征图 X 分别通过一个 $3 { \times } 3$ 的卷积核和 $5 { \times } 5$ 的卷积核，分别生成两个特征图 $\widetilde { U }$ （图中黄色）和 $\widehat { U }$ （图中绿色），然后将得到的这两个特征图 $\widetilde { U }$ 和 $\widehat { U }$ 进行相加，生成ܷ。但是本文为了提升效率，使用空洞大小为 2 的 $3 { \times } 3$ 卷积核来代替 $5 { \times } 5$ 卷积核。 $U$ 计算方法如公式（3.2）所示。

$$
U = \widetilde { U } + \widehat { U }
$$

然后通过 $F _ { g p }$ 函数对U进行全局平均池化操作生成 $1 \times 1 \times \mathrm { C }$ 的特征图 $\pmb { s }$ ，从而嵌入全局信息， $\pmb { s }$ 为一个有 $\mathrm { ~  ~ c ~ }$ 个元素的列向量， $\pmb { s }$ 中的元素是通过空间维度 $H \times W$ ，缩小 $U$ 计算得来的，具体公式如（3.3）所示。

$$
\begin{array} { r } { \pmb { s } = F _ { g p } ( U ) = \frac { 1 } { H \times W } { \sum } _ { i + 1 } ^ { H } { \sum } _ { j = 1 } ^ { W } U ( i , \ j ) } \end{array}
$$

接着 $\pmb { s }$ 通过 $F _ { f c }$ 函数即一个全连接层生成 $d { \times } 1$ 的向量 $\pmb { z }$ 。通过全连接层实现此步，可以做到在提升效率的同时降低维度。

$$
\pmb { z } = F _ { f c } ( \pmb { s } ) = \delta ( B ( W ) )
$$

公式（3.4）中 $\delta$ 使用 ReLU 函数， $B$ 表示批归一化（Batch normalization），W 为权重矩阵， $W { \in } R ^ { d \times c }$ 。 $d$ 的取值是通过公式（3.5）计算确定的。公式中 $r$ 代表缩小的比率，$L$ 表示 $d$ 的最小值，本章将L设置为32。

$$
d = m a x ( c / r , \ L )
$$

从求和到得到 $| \pmb { z }$ 的过程为 Fuse 操作。接下来一步是 Select 操作，此步是 SKNet 的核心所在，通过此步即可在 $\pmb { z }$ 的指导下采用跨通道软注意自适应地选择不同空间尺度的信息。具体来说，利用 Fuse 操作得到的向量 $\pmb { z }$ 通过 $\pmb { a }$ 和 $\pmb { b }$ 两个函数，并将生成的函数值与原先的U෩和U෡相乘。其中 $\pmb { a }$ 和 $\pmb { b }$ 两个函数如公式（3.6）所示。

$$
\begin{array} { r } { \pmb { a } = \frac { e ^ { A z } } { e ^ { A z } + e ^ { B z } } \pmb { b } = \frac { e ^ { B z } } { e ^ { A z } + e ^ { B z } } } \end{array}
$$

其中 $A$ 、 $B \in R ^ { c \times d }$ ， $A , B$ 矩阵均是需要在训练之前初始化的，通过端到端训练得来，尺度为 $c \times d$ ，式中 $\mathsf { z } \in R ^ { d \times 1 }$ ，经过 Softmax 操作后，得到 $\pmb { a } \in \pmb { R } ^ { c \times 1 }$ ，由于 $\pmb { a }$ 和 $\pmb { b }$ 的函数值的加和等于 1，所以可以设置各个分支中输出的特征图的权重值，由于各个分支中卷积核的大小不同，所以也就实现了网络卷积核大小的自适应选择。

最后通过各个卷积核上的注意力权重得到最终的特征图ܸ，其公式如（3.7）所示。

$$
\begin{array} { r l } { V = \pmb { a } \cdot \widetilde { U } + \pmb { b } \cdot \widehat { U } } & { { } \ \pmb { a } + \pmb { b } = 1 } \end{array}
$$

(2) 损失函数

为了减轻图像正负类别数量不均衡对图像的分割效果产生的负面影响，使得分割精度更高，进而更好的区分真假人脸，本章设计了一种损失函数。该损失函数将 DiceLoss 损失和 Focal Loss 损失有机结合，并利用此函数训练改进的 U-Net 网络。改进损

失函数定义式如公式（3.8）所示：

$$
L = L _ { D i c e } + \lambda L _ { F o c a l }
$$

其中 Dice Loss 损失和 Focal Loss 损失的定义公式如（3.9）和（3.10）所示。

$$
\begin{array} { r } { L _ { D i c e } = C - \displaystyle \sum _ { c = 0 } ^ { c - 1 } \frac { T P _ { n } ( c ) } { T P _ { n } ( c ) + \alpha F N _ { P } ( c ) + \beta F P _ { p } ( c ) } } \end{array}
$$

$$
\begin{array} { r } { L _ { F o c a l } = - \frac { 1 } { N } \displaystyle \sum _ { c = 0 } ^ { c - 1 } \displaystyle \sum _ { n = 1 } ^ { N } g _ { n } ( c ) ( 1 - P _ { n } \left( c \right) ) ^ { 2 } l o g \big ( P _ { p } \left( c \right) \big ) } \end{array}
$$

其中 $C$ 表示某个特定的类别， $T P _ { n } ( c ) \cdot \ F N _ { P } ( c ) \cdot \ F P _ { p } ( c )$ 分别为类别的真阳性概率值、假阴性概率值、假阳性概率值， $P _ { n } ( c )$ 是像素 $n$ 为 $\mathrm { ~  ~ c ~ }$ 类的预测概率值； $g _ { n } ( c )$ 是像素$n$ 为 $c$ 类的真实情况； $c$ 代表总类别个数； $N$ 代表总像素数量； $\alpha$ 代表假阴性的惩罚权重值， $\beta$ 代表假阳性的惩罚权重值； $\lambda$ 代表ܦ݅ܿ݁ ܮ݋ݏݏ损失和 Focal Loss 损失之间的权重值。

ܦ݅ܿ݁ ܮ݋ݏݏ是由ܦ݅ܿ݁系数而得名的，ܦ݅ܿ݁系数可以用来评估两个样本之间的相似程度，其值越大意味着这两个样本越相似，ܦ݅ܿ݁系数的数学表达式如公式（3.11）所示：

$$
\begin{array} { r } { D i c e = \frac { 2 | X \cap Y | } { | X | + | Y | } } \end{array}
$$

其中ܺ表示真实分割图像的像素标签，ܻ表示模型预测分割图像的像素类别， $\vert X \cap Y \vert$ 表示 ܺ和ܻ之间交集元素的个数，近似为预测图像的像素与真实标签图像的像素之间的点乘，并将点乘结果相加。 $| X |$ 和|ܻ|分别表示ܺ、ܻ中元素的个数，近似为它们各自对应图像中的像素相加。ܦ݅ܿ݁ ܮ݋ݏݏ表达式如公式（3.12）所示：

$$
\begin{array} { r } { D i c e L o s s = 1 - D i c e = 1 - \frac { 2 | X \cap Y | } { | X | + | Y | } } \end{array}
$$

ܦ݅ܿ݁ ܮ݋ݏݏ可以有效缓解样本中前景与背景之间面积不平衡带来的不利影响，前景与背景不平衡也就是图像中大部分区域是不包含想要分割得到的目标的，只有小部分区域包含目标。本章想要提取的角膜高光区域与虹膜其它区域相比就是不平衡的，因此选择通过引进Dice Loss 来有效解决这种面积差异所带来的不利影响。

Focal Loss 是由何恺明团队提出，使用 Focal Loss 损失可以减轻因样本数据不平衡而对模型性能产生的负面影响。当类别样本数量不均衡时，通常会影响分类算法的效果，因此 Focal Loss 将易区分负例的 loss 权重降低，使得网络不会被大量的负例带偏，让训练变得更加稳定。其公式为：

$$
F L ( p _ { t } ) = - ( 1 - p _ { t } ) ^ { \gamma } l o g ( p _ { t } )
$$

其中：

$$
p _ { t } = \left\{ { \begin{array} { c } { p , ~ i f ~ y = 1 } \\ { 1 - p , ~ o t h e r w i s e } \end{array} } \right.
$$

式中ߛ为常数，当 $\scriptstyle \gamma = 0$ 时，式中 $F L ( p _ { t } )$ 就等同于普通的交叉熵损失函数。

# 3.3.3 角膜高光提取

受到图像分割的启发，基于角膜高光与虹膜区域的差异性考虑，本文选择使用阈值法对角膜高光进行提取，但考虑到在实际情况中人眼角膜产生的反射高光可能区域分散，大小不一，因此基于实际情况考虑，本文选择使用自适应阈值方法来提取角膜高光。

自适应阈值法的分割阈值是由像素周围的邻域像素的分布情况决定的，不同于传统的固定阈值分割，它可以自适应的确定分割的阈值，在图像中亮度较高的区域，二值化的阈值也相对较高，相反，亮度较低的区域，其二值化阈值也较低。此种方式适合处理光照不均的图像，普适性更强。

最大类间方差法即是一种图像自适应阈值分割算法，依据图像各个像素灰度值的不同，按照灰度值的大小将图像划分成前景区域和背景区域两个部分，前景区域即为想要得到的目标部分，能将目标区域与背景分离开来的值即为阈值，通过遍历不同的阈值，计算在不同阈值下对应的背景与前景之间的类间方差，找到取得类间方差极大值时的阈值，此阈值即为通过最大类间方差法求得的最优阈值。

利用改进 U-Net 分割出的虹膜区域图像为 I，输入图像宽、高分别为 $\omega , \ h$ ，灰度级为L。则图像包含的像素总数按公式（3.15）计算为：

$$
N = \omega \times h
$$

灰度范围 $i = 0 , 1 , 2 , \ldots . , L - 1$ ，根据图像像素 $\mathrm { ~  ~ N ~ }$ 和灰度范围 $[ 0 , L - 1 ]$ 求出每个灰度级的概率，公式如（3.16）所示,其中 $n _ { i }$ 表示灰度级 $i$ 的像素个数。

$$
\begin{array} { r } { P _ { i } = \frac { n _ { i } } { N } ~ i = 0 , 1 , 2 , \dots , L - 1 } \end{array}
$$

将前景（即高光区域）和背景的分割阈值记作T，前景为 $n _ { i }$ 类， $n _ { i }$ 由[0, T]之间的像素组成，背景部分为 $p _ { i }$ 类， $p _ { i }$ 由 $[ \mathrm { T } + 1 , \mathrm { L } - 1 ]$ 之间的像素组成。 $N _ { 0 }$ 为小于阈值的像素的个数， $N _ { 1 }$ 为大于阈值的像素的个数，则 $N { = } N _ { 0 } { + } N _ { 1 }$ ，因此可以计算出 $n _ { i }$ 和 $p _ { i }$ 的均值 $u _ { 0 }$ 和$u _ { 1 }$ 。公式如（3.17）所示。其中 $n _ { i }$ 类像素个数占整幅图像的比例记为 $\omega _ { 0 }$ ， $p _ { i }$ 类像素个数

占整幅图像的比例为 $\omega _ { 1 }$ 。

$$
\begin{array} { r } { \mathbf { u } _ { 0 } = \frac { \underset { i = 1 } { \sum } \mathbf { i } n _ { i } } { \mathbf { N } _ { 0 } } = \frac { \underset { i = 0 } { \sum } \mathbf { i } p _ { i } } { \underset { i = 0 } { \sum } } = \frac { \underset { i = 0 } { \sum } \mathbf { i } p _ { i } } { \underset { \omega _ { 0 } } { \sum } } ; \mathbf { u } _ { 1 } = \frac { \underset { i = 2 } { \sum } \mathbf { i } n _ { i } } { \mathbf { N } _ { 1 } } = \frac { \underset { i = 1 } { \sum } \mathbf { i } p _ { i } } { \underset { i = 1 } { \sum } } = \frac { \underset { i = 1 } { \sum } \mathbf { i } p _ { i } } { \underset { i = 1 } { \sum } } = \frac { \underset { i = T + 1 } { \sum } \mathbf { i } p _ { i } } { \omega _ { 1 } } } \\ { \underset { i = 0 } { \sum } p _ { i } } \end{array}
$$

则图像的总平均值为：

$$
\boldsymbol { u } = \boldsymbol { \omega } _ { 0 } \times \boldsymbol { u } _ { 0 } + \boldsymbol { \omega } _ { 1 } \times \boldsymbol { u } _ { 1 }
$$

类间方差的计算方法如下式所示：

$$
\delta ^ { 2 } = \omega _ { 0 } ( u _ { 0 } - u ) ^ { 2 } + \omega _ { 1 } ( u _ { 1 } - u ) ^ { 2 } = \omega _ { 0 } \omega _ { 1 } ( u _ { 0 } - u _ { 1 } ) ^ { 2 }
$$

采用遍历的方法计算各个灰度阶对应的像素个数，找到使类间方差 $\delta ^ { 2 }$ 最大的阈值T 即为最佳阈值。保存高于 T 的像素，即可得到角膜高光区域。将左眼提取的角膜高光记为RL，右眼提取的角膜高光记为RR。通过比较二者的相似性对图像进行鉴伪。

# 3.4 实验结果与分析

# 3.4.1 实验参数设置

# (1) 数据集

采用Iris 数据集训练分割网络，在测试时，从来自You Tube 以及 VidTIMIT 数据集中随机抽取得到的 36000 张真实人脸数据集中，随机选择 3240 张用做测试，同时从DF-TIMIT 数据集中 FaceSwap-GAN 数据子集中选取 8760 张伪造人脸用于测试，最终得到12000 张测试样本。

(2) 环境设置

本章所设计的伪造检测实验在 PC 端进行训练，使用的计算机编程语言为 Python、搭建PyTorch 深度学习框架，其它软硬件配置如下：

处理器：Intel Xeon W-2245 3.90GHz  
显卡：NVIDIA GeForce RTX 3090  
内存：64G  
操作系统：Windows 10  
软件平台：Anaconda-Python 3.7.3  
深度学习框架：PyTorch 1.2.0  
(3) 实验细节  
本章实验搭建 Pytorch 深度学习框架，采用 Adam 优化器对损失函数进行优化，设

置其超参数 $\beta _ { 1 } { = } 0 . 9$ ， $\scriptstyle { \beta _ { 2 } = 0 . 9 9 9 }$ ，损失函数中惩罚权重 $\alpha$ 和 $\beta$ 设置为 0.5，两个损失函数之间的权重ߣ同样设置为 0.5。批量大小 Batch-size 设置为 32，初始的学习率 LearningRate 设置为 0.0001，训练迭代次数为 64 个 epochs。

寻找最佳阈值时，通常情况下需要对 $0 { \sim } 2 5 5$ 各个灰度阶像素进行遍历，但是根据数据发现，人眼虹膜的灰度值主要分布在 $5 0 { \sim } 1 0 0$ 左右，因此为了减少计算量、提升检测速度，本文选择遍历 $1 0 0 { \sim } 2 5 5$ 灰度阶的像素，找到合适的阈值。

# 3.4.2 评价指标

为了评估本章算法的性能，本文选择使用IOU分数和 AUC 值进行评估。并结合混淆矩阵，对实验的精确度、漏检率、误检率等指标进行分析，综合各个指标评估本章所提伪造检测网络的性能。各项指标的具体介绍如下。

# (1) IOU分数

IOU分数是用来衡量两个集合之间相似度的度量，其表达式为公式（3.20）所示：

$$
\begin{array} { r } { \mathrm { I O U } ( \mathrm { A } , \mathrm { B } ) = \frac { | \mathrm { A } \cap \mathrm { B } | } { | \mathrm { A } \cup \mathrm { B } | } } \end{array}
$$

用几何图像可以直观形象的解释IOU指标，如图 3.9 所示，图中左上方的正方形代表的是给出的标准，右下方的正方形代表网络分割结果，二者相交得到的中间的小正方形代表 True Positive $( T P )$ ：表示真正，即预测为正，实际为正；图中右下角除去 TP部分的区域代表 False Positive $( F P )$ ：表示假正，即预测为正，实际为负；图中左上角除去 TP 部分的区域代表 False Negative (FN)：表示假负，即预测为负，实际为正；与此相关的还有一个指 True Negative (TN)：代表真负，即预测为负，实际为负。在本实验中把真实人脸图像定义为负类表示为 0，人脸伪造图像定义为正类表示为 1。

![](images/f63cbba6f530d7a36300acd1459644373df7459c854e3bf4854c80a998b9153d.jpg)  
图 3.9 IOU分数几何表示  
Fig. 3.9 Geometric representation of IOU fractions

(2) AUC 值

AUC 值(Area Under Curve)能直观的反映模型性能，其中 Curve 为 ROC 曲线，曲线的横轴为 FPR(False Positive Rate)，曲线的纵轴为 TPR(True Positive Rate)，TPR、FPR的计算公式如公式（3.21）、公式（3.22）所示：

$$
\begin{array} { r } { T P R = \frac { T P } { ( T P + F P ) } } \end{array}
$$

$$
\begin{array} { r } { F P R = \frac { F P } { ( F P + T N ) } } \end{array}
$$

AUC 值是指 ROC 曲线下面积，其范围为[0,1]，AUC 面积值越大越接近 1，证明模型的性能越好。AUC 的计算方法如公式（3.23）所示：

$$
\mathrm { A U C } = \frac { \sum _ { i \in p o s i t i v e C l a s s } r a n k _ { i n s _ { i } } – M \times ( M + 1 ) / 2 } { M \times N }
$$

其中，݌݋ݏ݅ݐ݅ݒ݁ܥ݈ܽݏݏ代表正类， $M , ~ N$ 分别代表真实样本的个数、负样本的个数，$r a n k _ { i n s _ { i } }$ 指的是按照预测分数排序后的第݅个顺序位置，代表第݅个样本的序号。

(3) 混淆矩阵

混淆矩阵是一种可以用来对训练后的模型的性能进行评估的一种评判方式，通过列出矩阵，更加直观清晰的展示模型预测正确与错误的数量，其中矩阵的行代表真实值，而列代表预测值。

# (4) 误检率与漏检率

误检率反映分类器或者模型正确预测正样本纯度的能力，减少将负样本预测为正样本，即负样本被预测为正样本占总的负样本的比例，在本实验中代表的是将真实的人脸图像预测为虚假的人脸图像所占总的真实样本的比例，其值越小，代表网络的性能越好。公式如（3.24）所示。

$$
\begin{array} { r } { E r r o r d e t e c t i o n r a t e = \frac { F P } { F P + T N } = \frac { F P } { N } } \end{array}
$$

漏检率是反映分类器或模型正确预测负样本纯度的能力，减少将正样本预测为负样本，即正样本被预测为负样本占总的正样本的比例，在本实验中代表的是将伪造人脸图像预测为真实人脸图像占总的伪造人脸图像的比例，其值越小，代表网络性能越好。公式如（3.25）所示。

$$
\begin{array} { r } { L e a k d e t e c t i o n r a t e = \frac { F N } { T P + F N } = \frac { F N } { P } } \end{array}
$$

(5) 精确度

精确度(ܣܿܿݑݎܽܿݕ)指的是预测正确的样本数量占据总的样本数量的比例，计算方法如公式（3.26）所示

$$
\begin{array} { r } { A c c u r a c y = \frac { T P + T N } { T P + T N + F P + F N } } \end{array}
$$

# 3.4.3 实验结果分析

(1) 实验结果展示

图 3.10 对实验中的部分结果图进行展示。以IOU值反映两只眼睛的角膜高光之间的差异，从输出的结果可以看出，在真实的人脸图像中，两眼之间的角膜高光呈现很高的相似性，两眼之间的角膜高光的IOU值较大，而 FaceSwap-GAN 生成的人脸图像中，两眼角膜高光之间相似性较低，得到的两眼角膜高光的IOU值较小。

![](images/fbe70780eefa796f637b873ae10c6b8a7dffcc018187afac3181472f11f37909.jpg)  
图 3.10 输出结果图及 IOU 值  
Fig. 3.10 Output result graph and IOU value

为验证本文方法的有效性，本文生成了真实人脸图像和 FaceSwap-GAN 生成的人脸图像的两眼角膜高光部分IOU值的分布。如图 3.11 所示。

![](images/d67d4a2dbcef8e6d3a8bc849201080393d9b054a932ad0b4ba7c07614a925cd0.jpg)  
图 3.11 IOU 值分布图  
Fig. 3.11 Distribution of IOU scores

从图 3.11 中可以看出两种图像的IOU值分布呈现很明显的不同，由 FaceSwap-GAN 生成的人脸图像的IOU值集中分布在 $0 . 2 { \sim } 0 . 3$ 左右区域，而真实图像的IOU值主要分布在 $0 . 7 { \sim } 0 . 8$ 左右的区域。印证了角膜高光的一致性是区分真实人脸图像和FaceSwap-GAN 生成的人脸图像的有效手段。通过IOU值的分布，本文选择 0.5 作为阈值，大于 0.5 则认为是真实人脸图像，小于 0.5 则认为是由 FaceSwap-GAN 生成的人脸图像。

(2) 对比实验结果分析

为了能更好地验证本章所提方法的有效性，通过对比实验进行分析，选择两种主流的基于规则的伪造检测方法作为对比。图 3.12 中展示的是在同一数据集下采用本文方法和两种主流的基于规则的鉴伪方法的 ROC 曲线。

从图 3.12 中的 ROC 曲线可以看出，与 Shu Hu 等人[63]以及 Hui Guo 等人[62]提出的鉴别方法相比，本文提出的方法得到的 ROC 曲线，最贴近左上角，AUC 值为 0.96 是三者中的最大值，检测效果最好，比 Shu Hu 等人[63]提出的检测方法得到的 AUC 值提高 0.02，说明本文所提出的方法具有不错的改进效果。与 Hui Guo 等人[62]提出的利用人眼瞳孔形状鉴伪的方法得到的 AUC 值提高了 0.07，说明与利用其它人体生物特性的鉴伪方法相比，本文提出的方法的检测效果也得到了提升。

![](images/95776531bddfd6a8142101fb7795dc375523a5e84497fb1b553236d51f5e0beb.jpg)  
图 3.12 ROC 曲线Fig. 3.12 The ROC curve

为了能更好的分析网络的性能，展示了实验结果混淆矩阵，如表 3.1 所示，实验以0.5 为阈值，图中右上、左上、右下、左下依次表示假阳性(FP)、真阴性(TN)、真阳性(TP)、假阴性(FN)。从表中可以看出，三种方法对于真实人脸图像的正确鉴别数量分别为 2587、2761 以及 2896 张，对于伪造人脸图像的鉴别正确数量分别为 8354、8426 以及 8573 张。综合来看本文提出的方法对于真实以及虚假人脸图片的正确鉴别数量最多；通过计算可以得出，三种方法的精确度分别为 $9 1 . 1 8 \%$ 、 $9 3 . 2 3 \%$ 、 $9 5 . 5 8 \%$ ，可以发现在使用相同数据集的情况下，本章所提方法是检测精确度最高的，与 Hui Guo 等人[62]提出的利用人眼瞳孔形状鉴伪的方法所得的精确度提高了 $4 . 4 \%$ ；与 Shu Hu 等人[63]所提方法的检测精确度提高了 $2 . 3 5 \%$ 。同时从表中可以得出三种方法的误检率分别为$2 0 . 1 5 \%$ 、 $1 4 . 7 8 \%$ 、 $1 0 . 6 2 \%$ ，漏检率分别为 $4 . 6 3 \%$ 、 $3 . 8 1 \%$ 、 $2 . 1 3 \%$ 。通过结果的对比可知在三种方法中，本章所提方法不论是误检率还是漏检率都是最低的，误检率与其它两种主流的基于规则的方法相比分别降低了 9.63、4.16 个百分点，漏检率分别降低了2.5、1.68 个百分点。

# 表 3.1 混淆矩阵

Table. 3.1 Confusion matrix 1）Hui Guo   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>2587</td><td>653</td><td>3240</td></tr><tr><td>标</td><td>Real</td><td>406</td><td>8354</td><td>8760</td></tr><tr><td>签</td><td>总计</td><td>2993</td><td>9007</td><td>12000</td></tr></table></body></html>

2）Shu Hu  

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>2761</td><td>479</td><td>3240</td></tr><tr><td>标</td><td>Real</td><td>334</td><td>8426</td><td>8760</td></tr><tr><td>签</td><td>总计</td><td>3095</td><td>8905</td><td>12000</td></tr></table></body></html>

3）Ours   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>2896</td><td>344</td><td>3240</td></tr><tr><td>标</td><td>Real</td><td>187</td><td>8573</td><td>8760</td></tr><tr><td>签</td><td>总计</td><td>3083</td><td>8917</td><td>12000</td></tr></table></body></html>

(3) 个例分析

经过实验发现，因为受到自然环境中的光线以及拍照时人的姿态以及拍摄角度的影响，除一般肖像照外，照片中的人眼角膜反光点相似程度有所下降，除此之外因受到照片清晰度等的影响，传统的以人眼生物、物理规则为评判标准进行的伪造检测实验中误检率较高。因此本章对具有代表性的测试图进行了个例分析，比较其在不同检测网络中的输出结果，如表 3.2 所示为部分人脸图像具体鉴别情况。

表 3.2 部分人脸图像具体鉴别情况  
Table 3.2 Specific identification of part of the face pictures   

<html><body><table><tr><td>人脸图像</td><td colspan="2"></td><td colspan="2"></td><td colspan="2"></td><td colspan="2"></td><td colspan="2"></td><td colspan="2"></td></tr><tr><td>指标与结果</td><td>IOU 值</td><td>判断 结果</td><td>IOU 值</td><td>判断 结果</td><td>IOU 值</td><td>判断 结果</td><td>IOU 值</td><td>判断 结果</td><td>IOU 值</td><td>判断 结果</td><td>IOU 值</td><td>判断 结果</td></tr><tr><td>Shu Hu</td><td>0.68</td><td>√</td><td>0.61</td><td>√</td><td>0.39</td><td>×</td><td>0.28</td><td>×</td><td>0.25</td><td>×</td><td>0.20</td><td></td></tr><tr><td>Hui Guo</td><td>0.31</td><td>×</td><td>0.26</td><td>×</td><td>0.24</td><td>×</td><td>0.22</td><td>×</td><td>0.23</td><td>×</td><td>0.19</td><td>×</td></tr><tr><td>Ours</td><td>0.73</td><td>√</td><td>0.66</td><td>√</td><td>0.58</td><td>√</td><td>0.53</td><td>√</td><td>0.37</td><td>×</td><td>0.27</td><td>×</td></tr></table></body></html>

通过表格可以发现，对六张较为特殊的真实人脸图像进行检测时 Hui Guo 等人[62]提出的方法误检率最高，Shu Hu 等人[63]提出的方法次之，本文提出的方法误检率最低。经过分析发现，Hui Guo 等人[62]提出的方法对所鉴别的人脸图片要求高，当出现图片清晰度较低，人眼部位模糊时很容易产生误检，且人眼瞳孔部位在图片中所占的像素数量较少，部分人种瞳孔颜色较深，很难精准分割出瞳孔区域并判断形状。Shu Hu等人[63]所提出的方法，利用 OpenCV 自带的霍夫检测算法对人眼虹膜区域进行检测，对虹膜的检测精确度不高，难以精准找到角膜高光区域，因此只对正脸肖像图具有很好的检测性能。而本文通过改进所提出的算法能够精准的确定虹膜区域并寻找到角膜高光，进而进行一致性的判断，因此对于一些旋转角度较小的侧脸图也能进行精准检测。

表 3.3 对不同偏转角度人脸图像的鉴别情况  
Table 3.3 Identification of face image with different deflection angles   

<html><body><table><tr><td>旋转 角度</td><td colspan="2">15°</td><td colspan="2">30°</td><td colspan="2">45°</td><td>旋转 角度</td><td colspan="2">15°</td><td colspan="2">30°</td><td colspan="2">45°</td></tr><tr><td>真实人 脸图像</td><td colspan="2"></td><td colspan="2"></td><td colspan="2"></td><td>虚假人 脸图像</td><td colspan="2"></td><td colspan="2"></td><td colspan="2"></td></tr><tr><td>指标与 结果</td><td>IOU 值</td><td>结 果</td><td>IOU 值</td><td>结 果</td><td>IOU</td><td>结 值果</td><td>指标与 结果</td><td>IOU 值</td><td>结 果</td><td>IOU 值</td><td>结 果</td><td>IOU 值</td><td>结 果</td></tr><tr><td>Shu Hu</td><td>0.69</td><td>√</td><td>0.33</td><td>×</td><td>0.27</td><td>×</td><td>Shu Hu</td><td>0.17</td><td>×</td><td>0.18</td><td>×</td><td>0.21</td><td>×</td></tr><tr><td>Hui Guo</td><td>0.35</td><td>×</td><td>0.30</td><td>×</td><td>0.23</td><td>×</td><td>Hui Guo</td><td>0.20</td><td>×</td><td>0.16</td><td>×</td><td>0.17</td><td>×</td></tr><tr><td>Ours</td><td>0.73</td><td>√</td><td>0.71</td><td>√</td><td>0.32</td><td>×</td><td>Ours</td><td>0.15</td><td>×</td><td>0.15</td><td>×</td><td>0.23</td><td>×</td></tr></table></body></html>

为了验证网络对不同旋转角度的侧脸图像的检测效果，本章挑选旋转角度分别为$1 5 ^ { \circ }$ 、 $3 0 ^ { \circ }$ 、 $4 5 ^ { \circ }$ 的真实人脸图像和伪造人脸图像进行测试，如表 3.3 展示的是基于规则的网络对不同偏转角度人脸图像的鉴别情况，前三张为旋转角度不同并且旋转角度依次增大的真实人脸图像，后三张为旋转角度不同且角度依次增大的伪造人脸图像，经过实验发现，三种检测方法对于虚假人脸图像都能进行正确的判别，但是对于真实的人脸图像会出现不同程度的判断错误。

综上所述，利用人体物理学生物学特性进行伪造检测的方法，检测网络的误检率总是比漏检率高，也就是说相比于把伪造人脸图像识别成真实人脸图像，检测网络更容易将真实的人脸图像识别为伪造的人脸图像，出现该现象的原因是在现实世界中拍照过程中很难保证不受到外界因素的影响，但经过本文对鉴伪网络的改进，鉴伪网络的误检率和漏检率都得到了一定程度的降低。

# 本章小结

本章通过使用改进的 U-Net 模型对人眼各部分区域进行精确分割，利用人眼生物、物理学特征对 FaceSwap-GAN 生成的虚假人脸图像进行鉴别。首先对本章提出的鉴伪网络结构进行介绍，对网络中涉及到的各个模块进行功能讲述，最后通过对比实验以及个例分析验证本文所提方法的有效性。

# 第四章 基于特征融合的视频人脸伪造检测

# 4.1 引言

在第三章中设计了基于人眼瞳孔特性的 FaceSwap-GAN 人脸伪造检测方法，针对人脸交换中利用 FaceSwap-GAN 生成的伪造人脸图像进行鉴别，取得一定效果，但是除了伪造图片之外，现实生活中也存在大量的伪造人脸视频，真假难辨的视频同样给社会生活带来困扰，甚至一些不法分子会以此骗取钱财，也会有人以此煽动言论，搅乱政治。

鉴于此，本章对人脸交换伪造检测技术进行进一步研究，经过调研了解到通常伪造人脸视频所用到的方法是将视频进行拆解，分成单个视频帧，再一帧一帧的对视频中的人脸进行篡改，接着将篡改后的视频帧进行拼接，恢复成原本的顺序，最终得到伪造的视频。但是在进行视频拼接后，伪造视频中相邻帧之间出现了明显的不连续性，帧与帧之间可能存在较为明显的差异。这种差异包括亮度、光照、颜色等等，同时因为受到伪造条件的限制，在伪造的人脸视频中也常常会出现眼部区域形变以及局部区域模糊等特定现象，而这些痕迹没有出现在未经篡改的视频中。因此可以利用帧与帧之间的差异性以及帧内伪影对伪造人脸视频进行鉴别。基于此，本章提出一种基于特征融合的视频人脸伪造检测方法，设计了一种双分支网络，分别提取视频帧内特征信息与视频帧间差异信息，本章选择使用改进后的 EfficientNetB4 网络作为帧内特征提取网络，选择使用孪生网络作为帧间差异获取网络。最后将提取的眼部区域特征和帧间差异特征进行加和，送入全连接层和 Softmax 分类器实现对伪造人脸视频的检测。

# 4.2 本文方法

# 4.2.1 总体结构

本章所提的伪造人脸检测算法具体如下：首先获取视频的帧序列，然后对输入的视频帧图像进行人脸面部地标提取，根据关键点截取出眼部区域，之后分为两路，一路利用改进的特征提取网络 EfficientNetB4 对输入的人眼图像进行特征提取，寻找帧内伪影；另一路利用孪生网络寻找帧间差异，最后通过加和的方式得到拼接后的特征，将拼接后的特征送入全连接层利用 Softmax 分类器对输入的视频进行真假检测，判断输入视频是否为通过人脸交换方式伪造的虚假人脸视频。具体框架图如图4.1 所示。

![](images/59fa72f2d3e937c6b243fb31e88e27fcce781db05548171e3e5ede6bc7738ae8.jpg)  
Fig. 4.1 Block diagram of video face forgery detection model based on feature fusio

# 4.2.2 特征提取网络

在本章实验中选择 EfficientNetB4 作为主干网络，为了能让网络在空域上提取特征时更加关注眼部特征，同时解决 EfficientNetB4 网络因包含大量的小尺度卷积层，产生的长期依赖问题，导致的网络在进行特征提取时具有一定局限性。本章通过在网络中添加注意力机制来解决这一问题，使得网络可以在空域提取出更多有用的特征，用于后续的人脸伪造检测任务。通过在主干网络 EfficientNetB4 中添加注意力模块，以此来突出伪影区域，减小其它较为平坦区域对鉴伪任务的影响。改进后的特征提取网络可以输出含有注意力的特征图，这可以使得网络能提取到更加有用的帧内人眼特征。

如图 4.2 所示，为本章设计的基于 EfficientNetB4 的特征提取网络的具体网络结构图。本章选择将注意力模块添加在 11 层与 12 层之间，输入图像在经过前面 11 层卷积之后，得到大小为 $2 8 \times 2 8 \times 5 6$ 的特征图，再经过自注意力层，得到添加了注意力的特征图，后续再经过移动倒置瓶颈 MBConv 模块以及卷积模块得到大小为 $7 { \times } 7 { \times } 1 7 9 2$ 且含有注意力的特征图，将此作为特征提取网络所提取到的帧内特征。

![](images/b312db16a29853b5ae49295761f076a5b8be813dc89970bdf6a2248a76f947c5.jpg)  
图 4.1 基于特征融合的视频人脸伪造检测模型框架图  
图 4.2 基于自注意力机制和 EfficientNetB4 的特征提取网络

Fig. 4.2 Feature extraction network based on the self-attention mechanism and EfficientNetB4

EfficientNet 系列[34]由 Le 等人提出，根据网络大小的不同分为 B0-B7 共 8 个网络，其由普通卷积模块和移动倒置瓶颈 MBConv 模块[67]所构成，其中 EfficientNet B0尺寸最小，其速度快但精度较低，EfficientNetB7 尺寸最大，精度高但速度相对较慢。

EfficientNetB1-B7 是在 EfficientNetB0 的基础上，利用神经架构搜索技术对输入的参数：分辨率、网络深度、网络宽度三者进行调整得到的。表 4.1 所展示的是EfficientNetB0 至 EfficientNetB7 的网络信息。表中的 TOP-1 和 TOP-5 指的是 8 个EfficientNet 系列网络在 ImageNet 数据集上的精确度。

表 4.1 EfficientNetB0-B7 信息  
Table 4.1 Information of EfficientNetB0-B7   

<html><body><table><tr><td></td><td>B0</td><td>B1</td><td>B2</td><td>B3</td><td>B4</td><td>B5</td><td>B6</td><td>B7</td></tr><tr><td>参数量</td><td>5.3M</td><td>7.8M</td><td>9.2M</td><td>12M</td><td>19M</td><td>30M</td><td>43M</td><td>66M</td></tr><tr><td>TOP-1</td><td>77.3%</td><td>79.2%</td><td>80.3%</td><td>81.7%</td><td>83.0%</td><td>83.7%</td><td>84.2%</td><td>84.4%</td></tr><tr><td>TOP-5</td><td>93.5%</td><td>94.5%</td><td>95.0%</td><td>95.6%</td><td>96.3%</td><td>96.7%</td><td>96.8%</td><td>97.1%</td></tr></table></body></html>

观察表 4.1 可知，相比于 EfficientNetB5，EfficientNetB4 的参数量少了 $3 6 \%$ 左右，而在 ImageNet 数据集上的 TOP-1 精确度只降低了 $0 . 8 3 \%$ 左右，TOP-5 精确度降低$0 . 4 1 \%$ 左右。而相比于 EfficientNetB4，EfficientNetB3 网络的参数量同样减少了 $3 6 \%$ 左右，但是其在 ImageNet 数据集上的 TOP-1 精确度却降低了 $1 . 5 \%$ 左右，TOP-5 精确度降低了 $0 . 7 2 \%$ 左右。综合各项指标和实际情况，本章选择对 EfficientNetB4 网络进行改进，将其作为本章所设计的针对人脸交换伪造的检测网络中，用于对帧内信息进行提取的网络。

本文使用的 EfficientNetB4 具体的参数情况如表 4.2 所示。EfficientNetB4 网络有 9个阶段，网络输入图像大小为 $2 2 4 \times 2 2 4 \times 3$ ，经过三个步骤：首先是卷积核大小为 $3 { \times } 3$ 的卷积层，将输入图像通道数进行扩张，同时将特征图尺寸大小减半，最终得到大小为 $1 1 2 \times 1 1 2$ 输出通道数为 48 的特征图。之后再经过重复堆叠的 MBConv 结构，得到大小为 $7 { \times } 7$ 输出通道数为 448 的特征图，最后经过 $3 { \times } 3$ 的卷积层最终提取到的帧内特征图大小为 $7 { \times } 7$ 通道数为1792。

EfficientNetB4 网 络 通 过 $\lceil 3 2 *$ width_coefficients $+ 0 . 5 *$ divisor൧//divisor $*$ divisor运算对每个阶段的输入通道数进行扩张，其中divisor为 8 的倍数，//代表取整除运算符，若最终求得的通道数为 30，则近似取为 32，若最终求得的通道数为 25，则近似取为 24。对于 EfficientNetB4 网络而言width_coefficients $= 1 . 4$ ，divisor $= 8$ ，最终每个阶 段 的 输 出 通 道 数 为 $( [ 3 2 , 1 6 , 2 4 , 4 0 , 8 0 , 1 1 2 , 1 9 2 , 3 2 0 , 1 2 8 0 ] * 1 . 4 + 0 . 5 * 8 ) / / 8 * 8 =$ [48,24,32,56,112,160,272,448,1792]，在同一个阶段中不同的 MBConv 模块之间的输入输出通道数相同且步长 stride $_ { = 1 }$ ，保证同一个阶段不同 MBConv 模块输出的特征图的尺寸和通道数相同。

表 4.2 EfficientNetB4 参数  
Table 4.2 Parameters of EfficientNetB4   

<html><body><table><tr><td>阶段 (stage)</td><td>操作 (operator)</td><td>分辨率 (Resolution)</td><td>通道数 (Channels)</td><td>层数 (Layers)</td></tr><tr><td>1</td><td>Conv3x3</td><td>112×112</td><td>48</td><td>1</td></tr><tr><td>2</td><td>MBConV1,k3×3</td><td>112×112</td><td>24</td><td>2</td></tr><tr><td>3</td><td>MBConV6,k3×3</td><td>56×56</td><td>32</td><td>4</td></tr><tr><td>4</td><td>MBConV6,k5×5</td><td>28×28</td><td>56</td><td>4</td></tr><tr><td>5</td><td>MBConV6,k3×3</td><td>14×14</td><td>112</td><td>6號</td></tr><tr><td>6號</td><td>MBConV6,k5×5</td><td>14×14</td><td>160</td><td>6號</td></tr><tr><td>7</td><td>MBConV6,k5×5</td><td>7×7</td><td>272</td><td>8號</td></tr><tr><td>8號</td><td>MBConV6,k3×3</td><td>7×7</td><td>448</td><td>2</td></tr><tr><td>9</td><td>Conv3×3</td><td>7×7</td><td>1792</td><td>1</td></tr></table></body></html>

(1) 移动倒置瓶颈 MBConv 模块

本章使用的 EfficientNetB4 网络中同一个阶段中不同的 MBConv 模块之间的输入输出通道数相同，其中使用到的移动倒置瓶颈MBConV6 结构如图4.3 所示。

MBConv 结构主要由一个起到升维作用的普通 $1 \times 1$ 逐点卷积，大小为 $\mathbf { k } { \times } \mathbf { k }$ 的

Depthwise Conv 卷积，SENet 模块，一个降维作用的普通 $1 \times 1$ 逐点卷积，及 add 构成。

逐点卷积是将 $\mathrm { H } { \times } \mathrm { W } { \times } \mathrm { C }$ 的输入与 $\mathbf { n }$ 个普通的 $1 \times 1$ 卷积核进行卷积操作，收集每个点的特征。 $\mathbf { n }$ 为输入特征矩阵维度的倍数。也就代表了 MBConV1 以及 MBConV6 中的数字 1 和 6。

Depthwise Conv 卷积是将 $\mathrm { H } { \times } \mathrm { W } { \times } \mathrm { C }$ 的输入分成 C 组，然后对每一组做 $3 { \times } 3$ 或 $5 { \times } 5$ 的卷积，收集每个通道的空间特征。

SE 模块由一个全局平均池化，两个全连接层组成。第一个全连接层的节点个数是输入 MBConv 结构的特征矩阵通道数的ଵ，且使用 Swish 激活函数。第二个全连接层的节点个数等于深度卷积层输出的特征矩阵通道数，使用 sigmoid 激活函数。

Dropout 层只有当 EfficientNet 网络中同一个移动倒置瓶颈 MBConv 连续出现时才会出现。

对于使用的 MBConV6 模块，输入的特征图经过该模块之后通道数会先扩展 6倍，然后经过 Depthwise Conv 卷积和 SE 模块，最后把通道数收缩回原始通道数。

![](images/9606c1990bd0fafffa8c20d6f52bf1eeede4b3e7a50af4581adb6fd43874885b.jpg)  
图 4.3 MBConV6 模块  
Fig. 4.3 Mobile inverted bottleneck MBConV6 module

(2) 自注意力机制

自注意力可以很好的捕捉向量之间的相关性，通俗的来讲就是将一些机器所不能理解的信息转换成其理解的信息的过程。例如“小狗不能接住抛出去的玩具，因为它太累了”，对于我们而言，可以很简单的判断出来“它”是指小狗，但是对于机器而言，很难判断“它”到底指的是什么，而自注意力机制可以让机器将“它”与小狗联系起来。

在本章中，改进的 EfficientNetB4 的网络结构是在原始的 EfficientNetB4 网络中加入自注意力模块，使得输出的特征图带有注意力，能捕捉到更有效的特征。本章使用的自注意力模块如图 4.4 所示。

结合图 4.4，自注意机制具体计算步骤如下：

首先将输入的特征图 $X _ { i }$ 通过 $1 \times 1$ 卷积操作，分别转换至三个特征空间 $f ( x ) , \ g ( x )$ 和 $h ( x )$ ，具体转换如公式（4.1）所示。

$$
f ( x ) = W _ { f } X _ { i } , \ g ( x ) = W _ { g } X _ { i } , \ h ( x ) = W _ { h } X _ { i }
$$

公式中 $W _ { f } \in R ^ { \bar { c } \times c }$ ， $W _ { g } \in R ^ { \bar { c } \times c }$ ， $W _ { h } \in R ^ { \bar { c } \times c }$ ，分别代表三种不同滤波器的滤波权值，其中 ܿ̅ = ௖。

接着将进行完转置操作后的 $f ( x )$ 与 $g ( x )$ 相乘，将相乘后的结果送入 Softmax 当中，经过归一化处理后得到了注意力图 $\beta _ { j , \textit { i } ^ { \circ } }$ 。如公式（4.2）所示， $\boldsymbol { \beta } _ { j }$ ，௜表示模型在合成第݆个区域时对第݅个位置的处理程度。

$$
\beta _ { j , \textit { i } } = S o f t m a x \big ( f ( x _ { i } ) ^ { T } , \ g ( x _ { i } ) \big )
$$

将得到的包含局部信息的注意力图 $\boldsymbol { \beta } _ { j }$ ，௜与通过 $1 \times 1$ 卷积得到的全局空间信息 $h ( x _ { i } )$ 相结合得到自注意力层的输出。如公式（4.3）所示。

$$
O _ { j } = \sum _ { i = 1 } ^ { N } \beta _ { j , \ i } h ( x _ { i } )
$$

最终将输入的特征图 $x _ { i }$ 与乘上一个比例参数的自注意力层的输出 $O _ { j }$ 相加即可得到添加自注意力的特征图 $y _ { i }$ 。具体计算步骤如公式（4.4）所示。公式中的比例参数ߛ是个可以学习的参数，该参数的初始值为 0。

$$
y _ { i } = \gamma O _ { j } + x _ { i }
$$

![](images/c2dc8295421e19d7abbd20fea20525220e26bdefc23b0ad944456ccf4ed445ae.jpg)  
图 4.4 自注意力模块  
Fig. 4.4 Self-attention module

# 4.2.3 帧间差异特征表示

本章通过孪生网络提取帧间差异特征，孪生网络是 2020 年 ZHAG Yixuan[68]等人提出的。孪生网络基于神经网络获得特征向量，相似度用欧氏距离来度量，网络结构简单，通过共享权重的卷积网络得到输入图像的特征向量，最小化相同类别之间损失，最大化不同类别之间损失进行迭代训练，得到一个能判断两张输入图片相似性的网络。

本章方法：具体通过提取视频中的帧序列，然后利用 Dlib 人脸面部地标提取库对所提取的视频帧中的人脸进行关键点检测，提取人脸关键点，以此为依据进行人脸检测，截取出视频帧图像中的眼部区域，再经过 EfficientNetB4 特征提取网络对输入的眼部区域图像进行特征提取，两个分支分别对输入的样本图像进行特征提取，在孪生网络的训练阶段，对比损失会依据相似度标签来不断优化两分支中提取到的特征，然后在测试阶段，度量在两分支中提取到的特征的相似程度。具体框架如图 4.5 所示。

![](images/87f63059d4ae47c3ab1775eb9b9c5c06e603ad6b2ae7aa45bf34a55dbb84107f.jpg)  
图 4.5 孪生网络检测方法示意图  
Fig. 4.5 Schematic diagram of the Siamese-network-based detection method

对比损失 $L$ 具体计算方法如公式（4.5）所示。其中 $\mathbf { \nabla } _ { m }$ 表示为相似度阈值，ܻ为相似度标签，若ܻ等于 1，则代表两分支的输入样本相似；若ܻ等于 0，则代表两分支中的输入样本不相似。孪生网络两分支分别提取的两个特征的相似程度表示为 $D$ 。

$$
L = \textstyle { \frac { 1 } { 2 } } ( Y D ^ { 2 } + ( 1 - Y ) ( m a x ( m - D , 0 ) ) ^ { 2 } )
$$

相似程度 $D$ 用欧氏距离来表示，计算如公式（4.6）所示，两分支中所提取的特征向量分别表示为 $\mathrm { A } { = } ( \mathsf { a } _ { 1 } , \mathsf { a } _ { 2 } , . . . , \mathsf { a } _ { \mathrm { n } } )$ 以及 $\mathrm { B } { = } ( \mathsf { b } _ { 1 } , \mathsf { b } _ { 2 } , . . . , \mathsf { b } _ { \mathrm { n } } )$ ， $\mathbf { n }$ 为特征 A 与 B 的特征维度。

$$
\mathrm { { D } = \mathrm { { D } \big ( A , \partial { \cdot } \partial { B } \big ) = \| A - B \| _ { 2 } = \sqrt { ( a _ { 1 } - b _ { 1 } ) ^ { 2 } + ( a _ { 2 } - b _ { 2 } ) ^ { 2 } + \dots + ( a _ { n } - b _ { n } ) ^ { 2 } } } }
$$

对比损失的引入可以有效解决网络提取特征不理想问题，当孪生网络两个分支所提取的特征差距较大或较小时，即表明网络提取的特征不理想，此时对比损失函数值会进行相应的增大或减小，以此方式来改进所提取的特征。

本章使用的差异特性表示方式如下。首先从视频中提取相邻两张人脸图像眼部区域的有效特征，然后将提取的有效特征做差并取绝对值。以此方式来描述人脸视频的帧与帧之间的差异，度量提取的两帧之间的相似度。差异表示具体算法如公式（4.7）所示。其中X与Y分别表示从相邻两帧中提取到的特征向量， $\mathfrak { n }$ 表示特征X和Y的特征维度。将此作为孪生网络提取到的帧间差异信息，用于后续的伪造检测。

$$
\operatorname { d i f f } ( \mathrm { X } , \mathrm { Y } ) = ( | \mathrm { x } _ { 1 } - \mathrm { y } _ { 1 } | , | \mathrm { x } _ { 2 } - \mathrm { y } _ { 2 } | , \dots , | \mathrm { x } _ { \mathrm { n } } - \mathrm { y } _ { \mathrm { n } } | )
$$

# 4.2.4 Softmax 分类器

经过特征提取网络提取到的帧内特征，以及经过孪生网络提取到的帧间差异特征经过相加拼接，再经过全连接层，最终使用二分类器 Softmax 对提取到的特征进行分类，实现伪造检测。

![](images/10e606213a0de1dda7cdea6451f64d1ab2745244d3dca969a27c8753a9bed6e6.jpg)  
图 4.6 利用全连接和 Softmax 进行多分类结构图

Fig. 4.6 Multi-classification structure diagram using full connection and Softmax

本章中通过特征提取网络提取到的帧内人眼特征以及通过孪生网络提取到的帧间差异特征，经过拼接后得到 $7 { \times } 7 { \times } 3 5 8 4$ 的向量，人脸伪造检测最终对应两类标签，本章使用的全连接层的神经元个数为 4096，使用 $7 { \times } 7 { \times } 3 5 8 4 { \times } 4 0 9 6$ 的全局卷积实现，全连接层的权重矩阵大小为 $7 \times 7 \times 3 5 8 4 \times 2$ ，通过计算得到真假两类的得分，利用 Softmax 计算出属于各类的概率值，实现真假分类。如图 4.6 所示为利用全连接层和 Softmax 进行多分类结构图。

全连接层的作用是将特征图展平，将输入向量与权重矩阵相乘再加上偏置，将得到的实数进行映射，得到其属于各类的得分，接着 Softmax 会将得分映射成为概率，同时保证概率之和为1。全连接层的输出计算公式如（4.8）所示：

$$
Z _ { j } = W _ { j } \cdot X + b _ { j } = W _ { j 1 } \cdot X _ { 1 } + W _ { j 2 } \cdot X _ { 2 } + \cdots + W _ { j n } \cdot X _ { n } + b _ { j }
$$

Softmax 分类器可以解决多分类问题，类标签 y 可以取 $\mathbf { k }$ 个不同的值，当 $\mathbf { k }$ 取 2时，即为一个二分类器。当 Softmax 对类别进行预测时，对于给定的输入 $\mathbf { \sigma } _ { \mathbf { X } }$ ，用假设函数估算出对每一个类别的概率值，即输入属于每一种类别的概率，最终预测结果为概率值大的一类。

Softmax 分类器的输入是一个 $\mathbf { n }$ 维向量，向量中元素为任意实数的评分值，输出也是一个 $\mathfrak { n }$ 维向量，输出中的每个元素表示为 ${ \widehat { y _ { \iota } } } = P ( y = i | x )$ 。其中每个 $\widehat { y _ { \iota } }$ 元素映射在[0,1]区间上，而且整个向量的和为1。Softmax 函数的形式如公式（4.9）所示：

$$
{ \widehat { y _ { \imath } } } = { \frac { e ^ { x _ { i } } } { \sum _ { j = 1 } ^ { n } e ^ { x _ { j } } } }
$$

其中， $y _ { i }$ 表示 Softmax 的输出概率值， $x _ { i }$ 为 Softmax 的输入值， $\mathfrak { n }$ 为 Softmax 的输入值个数。

对于输入，Softmax 会计算出其属于真实人脸以及属于虚假人脸这两种类别的得分数值，利用 sigmoid 函数把所有的得分值映射成一个概率值，最后对最终正确分类所占的概率求取log 值再取负号得到最终的损失值，损失计算公式如公式（4.10）所示：

$$
\begin{array} { r } { L _ { i } = - \mathrm { l o g } ( \frac { e ^ { x _ { i } } } { \sum _ { j = 1 } ^ { n } e ^ { x _ { j } } } ) } \end{array}
$$

公式中的 $e ^ { x _ { i } }$ 表示进行值域映射，对于 $e ^ { x }$ 这一函数，越大的值映射完之后越大，越小的值映射完之后越小，这样使得两种分类的得分之间的差异更大，可以更加容易辨别出输入的类别。在得到两个类别的得分值之后，需要求得概率值，两个分类的概率值相加需为 1，为了使得概率值相加为 1，需要进行规划，即用每一个得分除以总的得分，最后得到各个类别的概率。规划过程使用到 log 函数，当正确分类的概率越大时，损失函数值越小，即 log 函数中，当 x 的值为 1 时，y 的取值为 0；当正确分类的概率越小时损失函数值越大，即 log 函数中当 x 取值为 0 到 1 之间时，y 的值为负数，也正因此损失计算公式中在log 前要取负。

Softmax 分类器能够扩大分数的差距，即使得分函数的分数结果差别都不大，通过Softmax 分类器，就能够使得分数的差距进一步拉大，使得分类效果更加明显。因此本章选择使用Softmax 分类器进行真假分类。

# 4.3 实验结果与分析

# 4.3.1 实验参数设置

(1) 环境设置

本章所设计的伪造检测实验在 PC 端进行训练，使用的计算机编程语言为 Python、搭建PyTorch 深度学习框架，其它软硬件配置如下：

处理器：Intel Xeon W-2245 3.90GHz  
显卡：NVIDIA GeForce RTX 3090  
内存：64G  
操作系统：Windows 10  
软件平台：Anaconda-Python 3.7.3  
深度学习框架：PyTorch 1.2.0

(2) 数据集

本章采用构建的 FSV 数据集进行训练并测试，本数据集包含 2950 段人脸视频，共计 66000 张人脸图像。其中使用 H.264 编码器恒定速率量化参数为 23 的高清画质数据集中包含 15000 张人脸图像，按照 4:1 的比例划分训练集与测试集。量化参数为 40 的低清画质数据集人脸图像数量为15000，训练集、测试集划分比例同样为4:1。

本章在实验之前进行数据预处理，具体操作：首先获取视频的帧序列，然后对输入的视频帧图像进行人脸面部地标提取，使用 Dlib 库定位出 26 个眼部区域关键点，根据关键点截取出眼部区域，如图 4.7 所示为 FSV 数据集的视频截图。如图 4.8 所示为根据关键点截取出的眼部区域图像。

![](images/3cd114db76dc6b16c056080af6e5303307db6f52142d25a29c2cdae391f4ef55.jpg)  
图 4.7 FSV 数据集视频截图

![](images/a32ff6fd4df6f37f35c727ea9629cae179aa5afc10c9f6addd87318c7d3316e0.jpg)  
Fig. 4.7 Screenshot of FSV dataset video   
图 4.8 截取的眼部区域图像  
Fig. 4.8 A captured image of the eye area

(3) 实验细节

本章实验搭建 Pytorch 深度学习框架，设置输入图像大小为 $2 2 4 \times 2 2 4$ ，经过特征提取网络提取帧内特征，经过孪生网络提取帧间差异特征。在训练检测网络时，首先获取训练集数据，设置读取数据时小批量数目为 256，并生成迭代器，接着初始化模型参数，设置 num_inputs $\scriptstyle \mathtt { \_ { d 0 9 6 } }$ ，num_outputs $_ { . = 2 }$ ，初始化参数时，权重参数设置为：均值为 0 标准差为 0.01 的正态分布，采用小批量随机梯度下降法对模型进行优化，设置批量大小(Batch-size)为 32，初始的学习率(Learning Rate)设置为 0.0001，迭代 64 个epochs。

# 4.3.2 评价指标

本章采用精确度和 AUC 值进行评价，并输出混淆矩阵，用于后续分析。评价指标的具体介绍同3.4.2。

# 4.3.3 实验结果分析

(1) 对比实验分析

本文基于 FSV 数据集中的两种不同画质人脸图像数据进行实验，一个实验只采用改进的 EfficientNetB4 网络对输入图像的眼部特征进行提取进行伪造检测；另一个实验将利用 EfficientNetB4 网络提取的眼部特征和利用孪生网络提取的帧间差异特征融合进行伪造检测，并将两种方法与主流的人脸伪造检测方法进行对比，结果如表4.5 所示。

# 表 4.5 对比实验结果

Table 4.5 Comparative test results   

<html><body><table><tr><td rowspan="2">Method</td><td colspan="2">Accuracy</td></tr><tr><td>HQ</td><td>LQ</td></tr><tr><td>MesoNet</td><td>0.752</td><td>0.743</td></tr><tr><td>X-Ray</td><td>0.732</td><td>0.718</td></tr><tr><td>XceptionNet</td><td>0.835</td><td>0.806</td></tr><tr><td>Xception+Attention</td><td>0.864</td><td>0.847</td></tr><tr><td>Ours（only eyes）</td><td>0.866</td><td>0.832</td></tr><tr><td>Ours（eyes+consecutive frame)</td><td>0.915</td><td>0.875</td></tr></table></body></html>

通过表格中的数据分析可以得出，只采用改进的 EfficientNetB4 网络对眼部特征进行提取进而进行伪造检测，相比于 MesoNet、X-Ray、XceptionNet 而言其检测精确度得到了一定程度上的提升，不论是对高清画质的人脸视频还是对低清画质的人脸视频均有所提升，但是相比于添加了注意力机制的 XceptionNet 而言，单单使用提取到的眼部特征对低清画质的人脸视频进行伪造检测时效果并不理想，对高清画质的人脸视频进行伪造检测时检测效果与添加了注意力机制的 XceptionNet 网络不相上下，而通过将EfficientNet B4 网络提取的眼部特征和利用孪生网络提取的帧间差异特征进行融合有效的提升了检测精确度，相比较于 MesoNet、X-Ray、XceptionNet、Xception+Attention这些主流检测方法，本文所提出的基于特征融合的伪造检测方法，在对于高清画质的视频进行伪造检测时的检测精确度分别提升了 $0 . 1 6 3 \%$ 、 $0 . 1 8 3 \%$ 、 $0 . 0 8 \%$ 、 $0 . 0 5 1 \%$ ，对于低清画质的视频的检测精确度分别提升了 $0 . 1 3 2 \%$ 、 $0 . 1 5 7 \%$ 、 $0 . 0 6 9 \%$ 、 $0 . 0 2 8 \%$ 。通过与其它主流检测方法在同一数据集上测试得到的检测精确度对比，可以证明本章所提方法的有效性，同时通过本章两个实验之间的对比，也证明了特征融合方法确实对伪造检测网络的性能提升具有一定的作用，将帧间差异特征引入后网络的鉴伪能力得到了提升，在检测高清画质视频时精确度提升了 $0 . 0 4 9 \%$ ；检测低清画质视频时精确度提升了 $0 . 0 4 3 \%$ 。

如图 4.9 所示为基于特征融合的视频人脸伪造检测方法在不同画质数据集下的ROC 曲线。图 1）表示在高清画质数据集下的 ROC 曲线，从图中可以看到 MesoNet、X-Ray、XceptionNet、Xception $\cdot ^ { + }$ Attention 等方法得到的曲线下面积 AUC 值分别为0.82、0.79、0.84、0.89，而本文提出的基于眼部区域特征融合的检测方法得到的 AUC值为 0.92，比主流方法高出 $0 . 0 3 \mathrm { \sim } 0 . 1 3$ ，比仅基于眼部特征训练的网络进行检测得到AUC 值高出 0.02。图 2）表示在低清画质数据集下的 ROC 曲线，从图中可以看到MesoNet、X-Ray、XceptionNet、Xception+Attention 等方法得到的曲线下面积分别为0.79、0.77、0.81、0.85，在对低清画质的人脸伪造视频进行检测时，本文所提方法得到的 AUC 值最高，其 AUC 值为 0.89，比主流方法高 $0 . 0 4 { \sim } 0 . 1 2$ ，比仅基于眼部特征训练的网络进行伪造检测得到的AUC 值高0.06。

![](images/f5f796f0b3f918d7fffa7ad7f55002a24851e51cf61343cf65bd6d818276fc50.jpg)  
图 4.9 ROC 曲线Fig. 4.9 The ROC curve

为了能更加详细地了解本章所提出的检测网络的检测能力，在高清画质和低清画质两种情况下对伪造人脸视频进行测试时，得出的检测结果的混淆矩阵如表 4.6 和表4.7 所示。在本实验中把真实人脸图像定义为负类（Fake）表示为 0，人脸伪造图像定义为正类（Real）表示为 1。

通过混淆矩阵可以得出：不论是在对高清画质的视频还是对低清画质的视频进行伪造检测时，本章所提的基于特征融合的方法正确识别真实人脸的数量和正确鉴别伪造人脸的数量都是最多的，本章所提方法的检测精确度是最高的。同时通过混淆矩阵可以看出，在对高清画质的视频进行检测时，采用特征融合的方法将真实人脸图像预测为虚假人脸图像的数量为 275 张，将虚假人脸图像预测为真实人脸图像的数量为286 张，误检率为 $7 . 6 4 \%$ ，漏检率为 $9 . 5 3 \%$ ；在对低清画质的视频进行检测时，本章方法将真实人脸图像预测为虚假人脸图像的数量为 352 张，将虚假人脸图像预测为真实人脸图像的数量为 473 张，误检率为 $9 . 7 8 \%$ ，漏检率为 $1 5 . 7 7 \%$ 。相同情况下误检率、漏检率最低。能更为有效地将真实人脸图像与虚假人脸图像区分开来。

# 表 4.6 高清画质视频数据集混淆矩阵

1）X-Ray   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>2769</td><td>831</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>938</td><td>2062</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3707</td><td>2893</td><td>66000</td></tr></table></body></html>

Table 4.6 High resolution video dataset confusion matrix   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>2854</td><td>746</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>891</td><td>2109</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3745</td><td>2855</td><td>66000</td></tr></table></body></html>

3）XceptionNet   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>3089</td><td>511</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>578</td><td>2422</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3667</td><td>2933</td><td>66000</td></tr></table></body></html>

2）MesoNet   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>3187</td><td>413</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>485</td><td>2515</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3672</td><td>2928</td><td>66000</td></tr></table></body></html>

5）Ours（only eyes）  

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>3196</td><td>404</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>480</td><td>2520</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3676</td><td>2924</td><td>66000</td></tr></table></body></html>

4）Xception+Attention   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>3325</td><td>275</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>286</td><td>2714</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3611</td><td>2989</td><td>66000</td></tr></table></body></html>

6）Ours（eyes+consecutive frame）

# 表 4.7 低清画质视频数据集混淆矩阵

1）X-Ray   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>2697</td><td>903</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>958</td><td>2042</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3655</td><td>2945</td><td>66000</td></tr></table></body></html>

Table 4.7 Low resolution video dataset confusion matrix   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>2771</td><td>829</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>867</td><td>2133</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3638</td><td>2962</td><td>66000</td></tr></table></body></html>

3）XceptionNet   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>2961</td><td>639</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>641</td><td>2359</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3602</td><td>2998</td><td>66000</td></tr></table></body></html>

2）MesoNet   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>3125</td><td>475</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>535</td><td>2465</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3660</td><td>2940</td><td>66000</td></tr></table></body></html>

5）Ours（only eyes）  

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>3074</td><td>526</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>583</td><td>2417</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3657</td><td>2943</td><td>66000</td></tr></table></body></html>

4）Xception+Attention   

<html><body><table><tr><td></td><td colspan="4">预测标签</td></tr><tr><td>真</td><td></td><td>Fake</td><td>Real</td><td>总计</td></tr><tr><td>实</td><td>Fake</td><td>3248</td><td>352</td><td>3600</td></tr><tr><td>标</td><td>Real</td><td>473</td><td>2527</td><td>3000</td></tr><tr><td>签</td><td>总计</td><td>3721</td><td>2879</td><td>66000</td></tr></table></body></html>

6）Ours（eyes+consecutive frame）

(2) 个例分析

为了能更加直观地看出各个模型对不同人脸交换伪造方法的检测能力，本章挑选出具有代表性的测试视频帧图片进行了个例分析，对比结果如表 4.8 所示。表格中前两张为来自 You Tube 的真实人脸图像，后四张是分别来自 Faceswap、FaceShifter 以及DeepFake 的伪造人脸图像。表格中标签列的√代表本张图片为真实人脸图片，结果列的√代表网络将本张图片判别为真实人脸图片；同理，标签列的 $\times$ 代表本张图片为伪造的人脸图像，结果列的 $\times$ 代表网络将本张图片判断为虚假人脸图片。

表 4.8 代表性测试视频帧图片检测结果  
Table 4.8 Representative test video frame image detection results   

<html><body><table><tr><td>图像来源</td><td colspan="2">You Tube</td><td colspan="2">You Tube</td><td colspan="2">Faceswap</td><td colspan="2">FaceShifter</td><td colspan="2">DeepFake</td></tr><tr><td>人脸图像</td><td colspan="2"></td><td colspan="2"></td><td colspan="2"></td><td colspan="2"></td><td colspan="2"></td></tr><tr><td>指标与结果</td><td>标签</td><td>结果</td><td>标签</td><td>结果</td><td>标签</td><td>结果</td><td>标签</td><td>结果</td><td>标签</td><td>结果</td></tr><tr><td>MesoNet</td><td>√</td><td></td><td>7</td><td>×</td><td>X</td><td>×</td><td>×</td><td>×</td><td>×</td><td>√</td></tr><tr><td>X-Ray</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td><td></td></tr><tr><td>XceptionNet</td><td></td><td>√</td><td></td><td></td><td>×</td><td>×</td><td>×</td><td>√</td><td>×</td><td>×</td></tr><tr><td>Xception+At</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td></tr><tr><td>Ours （only eyes)</td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td></td><td>×</td><td>√</td><td>×</td><td>×</td></tr><tr><td>Ours （eyes consecutive frame）</td><td></td><td>√</td><td>√</td><td>√</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td><td>×</td></tr></table></body></html>

从表格中可以看到本文提出的将视频帧内特征与视频帧间差异特征相结合的方式检测效果是最好的，与其它主流的鉴伪方法相比检测效果得到了提升。同时当只使用眼部特征进行伪造检测时，从表中的结果可以看出，检测效果是欠佳的，而通过将帧内特征与帧间差异信息相结合的方式可以在一定程度上提升网络的伪造检测的能力。

同时在表 4.8 中还可以看出，网络对于使用 Faceswap 以及 DeepFake 伪造的人脸视频检测效果更佳，可能是因为这两种方法在伪造过程中因为对内脸的仿射，使得伪造后的眼部区域出现了较为明显的伪影，伪造视频帧与帧之间也存在较大差异，而FaceShifter 伪造的人脸视频，在进行初步伪造后会进行第二步优化，使得眼部区域的伪影变得不明显，因此网络对于 FaceShifter 伪造的人脸视频检测效果欠佳。但本章所提出的基于特征融合的伪造检测方法相比于主流方法而言，对于 Faceswap、DeepFake以及FaceShifter 伪造的人脸视频鉴伪效果有所提升。

为了研究依据不同间隔帧数提取到的差异信息进行检测，对伪造检测精确度的影响，本章针对高清视频数据集提取不同间隔的视频帧进行帧间差异计算，通过取$\mathbf { M } { = } 1$ 、2、3、4 进行实验，结果如表4.9 所示。

表 4.9 高清画质数据集下，不同间隔帧数的检测精确度  
Table 4.9 The detection accuracy with different interval frames based on high quality datase   

<html><body><table><tr><td>相隔 帧数</td><td>MesoNet</td><td>X-Ray</td><td>XceptionNet</td><td>Xception+At</td><td>Ours （only eyes)</td><td>Ours （eyes +consecutive frame)</td></tr><tr><td>M=1</td><td>0.698</td><td>0.674</td><td>0.743</td><td>0.807</td><td>0.809</td><td>0.847</td></tr><tr><td>M=2</td><td>0.752</td><td>0.732</td><td>0.835</td><td>0.864</td><td>0.866</td><td>0.915</td></tr><tr><td>M=3</td><td>0.726</td><td>0.702</td><td>0.782</td><td>0.836</td><td>0.827</td><td>0.868</td></tr><tr><td>M=4</td><td>0.715</td><td>0.694</td><td>0.768</td><td>0.814</td><td>0.816</td><td>0.853</td></tr></table></body></html>

通过分析表 4.9 可知，当间隔帧数 M 等于 2 时检测精确度最高，与间隔帧数取 1相比精确度明显提升，但是当逐步提高间隔帧数时，各个模型的检测精确度却逐步下降，间隔帧数越多，精确度反而越低。因此可以得出结论，在进行伪造人脸视频检测时，模型的精确度并不是随着间隔帧数的增加而逐步增加的，当间隔帧数为 2 时，模型的检测精确度最高。

# 本章小结

本章提出一种基于特征融合的视频人脸伪造检测方法，将视频帧内信息与视频帧间差异信息相结合。本章先依次对提取帧内信息的网络、提取帧间差异信息的网络以及 Softmax 分类器进行介绍，对网络中涉及到的各个模块进行功能讲解，然后介绍了数据预处理方法，最后通过对比实验分析以及个例分析，对本文所提方法进行分析讨论最终得出结论。

# 第五章 总结与展望

# 5.1 总结

随着深度伪造技术的不断发展，基于深度学习的人脸交换伪造变得流行，经过伪造后的人脸图像、视频很难用肉眼分辨，这些真假难辨的图片、视频借助互联网传播起来，给社会带来一定程度的信任危机。因此本文针对人脸交换伪造方法的检测技术进行研究，具体从如下几个方面开展研究工作：

归纳总结分析目前流行的深度伪造人脸生成技术和深度人脸伪造检测技术，简述人脸生成、人脸重现、属性编辑、人脸交换等深度伪造人脸生成技术的原理及发展史，并从图像级伪造检测技术与视频级伪造检测技术两个角度对深度人脸伪造检测技术的主流方法进行归纳并简短评述。

针对图像，提出基于人眼生理、物理学规律的 FaceSwap-GAN 人脸伪造检测方法，这种方法利用伪造人脸图像两眼之间的角膜镜面高光不一致性，来判断输入图像是否为伪造人脸图像，与传统的检测方法相比，该方法可解释强。该方法先利用 Dlib获取人脸关键点，截取眼部区域，再利用添加 SK-ResNext 模块的 U-Net 模型提取虹膜区域，使用最大类间方差法自适应的确定合适的分割阈值分离角膜高光，将左右眼角膜高光对齐，并计算两眼角膜高光的IOU值，以此值作为判断标准，大于 0.5 为真实人脸图像，反之为伪造人脸图像。实验结果表明，该方法在 FaceSwap-GAN 数据集上测试，得到的 AUC 值为 0.96、精确度为 $9 5 . 5 8 \%$ 、误检率为 $10 . 6 2 \%$ 、漏检率为 $2 . 1 3 \%$ ，与另外两种基于规则的主流检测方法相比检测效果有所提升，并且在正脸肖像图以及小角度偏转的侧脸图上也都取得了不错的检测效果。

针对视频，提出一种基于特征融合的视频人脸伪造检测方法。该方法采用双分支网络，将视频帧内特征信息与视频帧间差异信息相结合，首先抽取视频帧，利用 Dlib获取人脸关键点坐标截取眼部区域图像，利用加入注意力机制的 EfficientNetB4 网络提取帧内特征，用孪生网络获取相邻帧之间的差异信息，然后将眼部区域特征与帧间差异特征进行加和拼接，最终使用 Softmax 分类器实现对人脸视频的真伪检测。实验结果表明，加入自注意力机制的 EfficientNetB4 网络能更好的提取到有用的特征，与主流的鉴伪网络相比检测性能得到了提升，但是与添加了注意力机制的 XceptionNet 相比稍逊一筹。但是引入帧间差异特征之后，通过特征融合的方式，将加入自注意力机制的EfficientNetB4 网络提取到的眼部特征与孪生网络提取到的帧间特征相结合，使得网络的检测性能得到了提升，不论在对高清画质的视频进行检测还是对低清画质的视频进行检测时，其伪造检测的精确度都得到了不同程度的提升，在对高清画质的视频进行检测时，与主流方法相比精确度最高提升了 $0 . 1 8 3 \%$ ，在对低清画质的视频进行检测时，与主流方法相比精确度最高提升了 $0 . 1 5 7 \%$ 。同时在高清画质视频集上进行检测时得到的 AUC 值也比主流方法提升了 $0 . 0 3 \mathrm { \sim } 0 . 1 3$ ；在低清画质视频集上得到的 AUC 值为比主流算法提高 $0 . 0 4 { \sim } 0 . 1 2$ 。

# 5.2 展望

本文针对人脸交换伪造方法的检测技术进行研究，设计两种针对人脸交换的伪造检测方法，实验结果表明已取得一定成果，但是仍有需要改进的地方，为此对后续的研究工作安排如下：

本文提出的基于人眼瞳孔特性的 FaceSwap-GAN 人脸伪造检测方法相较于之前的基于规则的鉴伪方法的检测效果得到了提升。但通过实验发现，本章所提出的依据双眼角膜高光一致性的检测方法存在一定的局限性，例如在对稍大角度偏转的侧面人脸图像的检测效果不佳，且在闭眼人脸图像、人物戴墨镜、周围光线黑暗等场景下不能有效的对伪造人脸图像进行检测。为此在未来可以深度探究对伪造侧脸图像的检测方法，探寻可以作为判别依据的相关人体生物、物理学特征，除了基于规则的检测方式外，也可以探索其它检测方法。

本文提出的基于特征融合的伪造人脸视频检测方法，相比于主流的人脸伪造检测算法而言，检测效果得到一定程度的提升。将视频的帧内信息与视频的帧间信息相结合使得对伪造的检测精确度得到了提升。但是本文提出的基于特征融合的鉴伪网络也存在些许不足，还有很多可以改进和提升的地方，例如由于受到一些客观因素的影响，本文选择对 EfficientNetB4 网络进行改进，以此作为特征提取网络，但是没有探寻使用 EfficientNet 系列其它网络作为特征提取网络进行伪造检测的相关实验，未来可以开展相关的实验工作。同时除了将帧内空域特征与帧间差异特征结合之外，也可以考虑将帧内的频域特征与帧间的差异特征相结合，这也是未来可以去积极探索的方向。除此之外，在选择特征融合方式上，也可以大胆尝试其它方式，并通过消融实验判断改进效果。

本文选择使用人眼特性以及人眼部区域进行伪造检测，是因为经过调查研究发现，在伪造图像及伪造视频中，眼部区域通常存在较为明显的伪影，且人眼存在生成模型所不能伪造的一些生物学特性。以此为基础，今后也可以选择对脸部其它区域进行研究，也可以对全脸进行研究，进行相关的鉴伪实验。

# 参 考 文 献

[1] 曹申豪，刘晓辉，毛秀青，等. 人脸伪造及检测技术综述[J]. 中国图像图形学报，2022，27(04)： 1023-1038.   
[2] Online. ZAOAPP [EB/OL]. 2019. https://.zaoapp.net.   
[3] Online. Faceapp [EB/OL]. 2019. http://www.faceapp.com.   
[4] GitHub. Faceswap [EB/OL]. 2018. http://github.com/MarekKowalski/FaceSwap/.   
[5] Li Y, Chang M-C, Lyu S. In ictu oculi: Exposing ai created fake videos by detecting eye blinking[C]. 2018 IEEE International Workshop on Information Forensics and Security (WIFS), 2018: 1-7.   
[6] Haliassos A, Vougioukas K, Petridis S, et al. Lips don't lie: A generalisable and robust approach to face forgery detection[C]. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021: 5039-5049.   
[7] Yang X, Li Y, Lyu S. Exposing deep fakes using inconsistent head poses[C]. ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019: 8261-8265.   
[8] Hochreiter S, Schmidhuber J. Long short-term memory[J]. Neural computation, 1997, 9(8): 1735- 1780.   
[9] George D, Lehrach W, Kansky K, et al. A generative vision model that trains with high data efficiency and breaks text-based CAPTCHAs[J]. Science, 2017, 358(6368): 2600-2612.   
[10] Nguyen H H, Yamagishi J, Echizen I. Capsule-forensics: Using capsule networks to detect forged images and videos[C]. ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2019: 2307-2311.   
[11] Tariq S, Lee S, Kim H, et al. Detecting both machine and human created fake face images in the wild [C]. The 2nd International Workshop on Multimedia Privacy and Security, 2018: 81-87.   
[12] Goodfellow I J, Pouget-Abadie J, Mirza M, et al. Generative adversarial networks[J]. Communications of the ACM, 2020, 63(11): 139-144.   
[13] Online. Open AI [EB/OL]. 2016. https://openai.com.   
[14] Online. GhatGPT [EB/OL]. 2023. https://openai.com/blog/chatgpt.   
[15] Online. Stable Diffusion [EB/OL]. 2022. https://stablediffusionweb.com.   
[16] Alec Radford, Luke Metz,Soumith Chintala. Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.[J]. arXiv preprint arXiv:1511.06434, 2015.   
[17] Karras T, Aila T, Laine S, et al. Progressive growing of gans for improved quality, stability, and variation[J]. arXiv preprint arXiv:1710.10196, 2017.   
[18] Karras T, Laine S, Aila T. A Style-Based Generator Architecture for Generative Adversarial Networks.[C]. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019, 4396-4405.   
[19] Huang X, Belongie S. Arbitrary style transfer in real-time with adaptive instance normalization[C]. Proceedings of the IEEE international conference on computer vision, 2017: 1501-1510.   
[20] Thies J, Zollhofer M, Stamminger M, et al. Face2face: Real-time face capture and reenactment of rgb videos[C]. Proceedings of the IEEE conference on computer vision and pattern recognition, 2016: 2387-2395.   
[21] He Z, Zuo W, Kan M, et al. Attgan: Facial attribute editing by only changing what you want[J]. IEEE transactions on image processing, 2019, 28(11): 5464-5478.   
[22] Yao X, Newson A, Gousseau Y, et al. A latent transformer for disentangled face editing in images and videos[C]. Proceedings of the IEEE/CVF international conference on computer vision, 2021: 13789- 13798.   
[23] Xu Y, Yin Y, Jiang L, et al. TransEditor: transformer-based dual-space GAN for highly controllable facial editing[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022: 7683-7692.   
[24] GitHub. Deepfales [EB/OL]. 2017. http://github.com/DeepFakes/faceswap.   
[25] Korshunova I, Shi W, Dambre J, et al. Fast face-swap using convolutional neural networks[C]. Proceedings of the IEEE international conference on computer vision, 2017: 3677-3685.   
[26] Huang Y, Juefei-Xu F, Wang R, et al. Fakelocator: Robust localization of gan-based face manipulations via semantic segmentation networks with bells and whistles[J]. arXiv preprint arXiv :2001.09598, 2020.   
[27] Github. FaceSwap-GAN [EB/OL]. 2018. http://github.com/shaoanlu/FaceSwap-GAN.   
[28] Natsume R, Yatagawa T, Morishima S. Rsgan: face-swapping and editing using face and hair representation in latent spaces[J]. arXiv preprint arXiv:1804.03447, 2018.   
[29] McCloskey S, Albright M. Detecting GAN-generated imagery using color cues[J]. arXiv preprint arXiv:1812.08247, 2018.   
[30] Khodabakhsh A, Ramachandra R, Raja K, et al. Fake face detection methods: Can they be generalized?[C]. 2018 international conference of the biometrics special interest group (BIOSIG), IEEE, 2018: 1-6.   
[31] 孙鹏，郎宇博，巩家昌，等. 拼接篡改伪造图像的色彩偏移量不一致取证方法[J]. 计算机辅助设

计与图形学学报，2017，29(8)：1405-1415.

[32] Dang H, Liu F, Stehouwer J, et al. On the detection of digital face manipulation[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern recognition, 2020: 5781-5790.   
[33] Choe J, Shim H. Attention-based dropout layer for weakly supervised object localization[C]. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019: 2219- 2228.   
[34] Tan M, Le Q. Efficientnet: Rethinking model scaling for convolutional neural networks[C]. International Conference on Machine Learning, 2019: 6105-6114   
[35] Qian Y, Yin G, Sheng L, et al. Thinking in frequency: Face forgery detection by mining frequencyaware clues[C]. Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23– 28, 2020, Proceedings, Part XII. Cham: Springer International Publishing, 2020: 86-103.   
[36] Güera D, Delp E J. Deepfake video detection using recurrent neural networks[C]. 2018 15th IEEE international conference on advanced video and signal based surveillance (AVSS), 2018: 1-6.   
[37] Sabir E, Cheng J, Jaiswal A, et al. Recurrent convolutional strategies for face manipulation detection in videos[J]. Interfaces (GUI), 2019, 3(1): 80-87.   
[38] Cho K, Van Merriënboer B, Gulcehre C, et al. Learning phrase representations using RNN encoderdecoder for statistical machine translation[J]. arXiv preprint arXiv:1406.1078, 2014.   
[39] Afchar D, Nozick V, Yamagishi J, et al. Mesonet: a compact facial video forgery detection network[C]. 2018 IEEE international workshop on information forensics and security (WIFS), 2018: 1-7.   
[40] Amerini I, Galteri L, Caldelli R, et al. Deepfake video detection through optical flow based cnn[C]. Proceedings of the IEEE/CVF international conference on computer vision workshops, 2019: 0-0.   
[41] Masi I, Killekar A, Mascarenhas R M, et al. Two-branch recurrent network for isolating deepfakes in videos[C]. Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part VII 16. Springer International Publishing, 2020: 667-684.   
[42] Simonyan K, Zisserman A. Two-stream convolutional networks for action recognition in videos[J].arXiv preprint arXiv :1409. 1556, 2014.   
[43] Rossler A, Cozzolino D, Verdoliva L, et al. Faceforensics $^ { + + }$ : Learning to detect manipulated facial images[C]. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019: 1- 11   
[44] Wodajo D, Atnafu S. Deepfake video detection using convolutional vision transformer[J]. arXiv preprint arXiv:2102.11126, 2021.   
[45] Fernandes S, Raj S, Ortiz E, et al. Predicting heart rate variations of DeepFake videos using neural ode[C]. Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops, 2019: 0-0.   
[46] Ciftci U A, Demir I, Yin L. Fakecatcher: Detection of synthetic portrait videos using biological signals[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.   
[47] Institute of Automation，Chinese Academy of Sciences. CASIA Iris Image Database［DB/OL］. ［2019-07-16］. http:// biometrics.idealtest.org/   
[48] Rossler A, Cozzolino D, Verdoliva L, et al. Faceforensics $^ { + + }$ : Learning to detect manipulated facial images[C]. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2019: 1- 11   
[49] Online.A high school student created a fake 2020 US candidate twitter verified it.[EB/OL]. 2020. http s://www.cnn.com.   
[50] Online. SFWdeepfakes [EB/OL]. 2020. https://www.reddit.com/r/SFWdeepfakes/.   
[51] Github. LiveSpeechPortraits [EB/OL]. 2020. http://yuanxunlu.github.io/projects/LiveSpeechPortraits/.   
[52] Online. News [EB/OL]. 2022. https://www.thepaper.cn/newsDetail_forward_17262083.   
[53] NirkinY, Keller Y, Hassner T. FSGAN: Subject agnostic face-swapping and reenactment[C]. Proceedings of the IEEE/CVF international conference on computer vision, 2019: 7184-7193.   
[54] Li L, Bao J, Yang H, et al. Faceshifter: Towards high fidelity and occlusion aware face-swapping[J]. arXiv preprint arXiv:1912.13457, 2019.   
[55] UMEDA M. Advances in recognition methods for handwritten Kanji characters[J]. IEICE TRANSACTIONS on Information and Systems, 1996, 79(5): 401-410.   
[56] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[J]. Communications of the ACM, 2017, 60(6): 84-90.   
[57] Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition[J]. arXiv preprint arXiv:1409.1556, 2014.   
[58] Szegedy C, Liu W, Jia Y, et al. Going deeper with convolutions[J]. arXiv preprint arXiv:1409.4842, 2015, 12.   
[59] He K, Zhang X, Ren S, et al. Deep residual learning for image recognition[C]. Proceedings of the IEEE conference on computer vision and pattern recognition, 2016: 770-778.   
[60] Chollet F. Xception: deep learning with depthwise separable convolutions [C]. IEEE Conference on Computer Vision and Pattern Recognition, 2017: 1251-1.   
[61] Li L, Bao J, Zhang T, et al. Face x-ray for more general face forgery detection[C]. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020: 5001-5010.   
[62] Guo H, Hu S, Wang X, et al. Eyes tell all: Irregular pupil shapes reveal GAN-generated faces[C]. ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022: 2904-2908.   
[63] Hu S, Li Y, Lyu S. Exposing GAN-generated faces using inconsistent corneal specular highlights[C]. ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2021: 2500-2504.   
[64] Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical image segmentation[C]. Medical Image Computing and Computer-Assisted Intervention–MICCAI, 2015: 234-241.   
[65] Xie S, Girshick R, Dollár P, et al. Aggregated residual transformations for deep neural networks[C]. Proceedings of the IEEE conference on computer vision and pattern recognition, 2017: 1492-1500.   
[66] Li X, Wang W, Hu X, et al. Selective kernel networks[C]. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2019: 510-519.   
[67] Sandler M, Howard A, Zhu M, et al. Mobilenetv2: Inverted residuals and linear bottlenecks[C]. Proceedings of the IEEE conference on computer vision and pattern recognition, 2018: 4510-4520.   
[68] 张怡喧，李根，曹纭，等. 基于帧间差异的人脸篡改视频检测方法[J]. 信息安全学报，2020， 5(2)：49-72.