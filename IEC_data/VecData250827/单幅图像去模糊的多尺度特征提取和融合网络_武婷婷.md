南京邮电大学学报(自然科学版)   
Journal of Nanjing University of Posts and Telecommunications(Natural Science Edition)   
ISSN 1673-5439,CN 32-1772/TN

# 《南京邮电大学学报(自然科学版)》网络首发论文

题目：  
作者：  
收稿日期：  
网络首发日期：  
引用格式：

网络首发：在编辑部工作流程中，稿件从录用到出版要经历录用定稿、排版定稿、整期汇编定稿等阶段。录用定稿指内容已经确定，且通过同行评议、主编终审同意刊用的稿件。排版定稿指录用定稿按照期刊特定版式（包括网络呈现版式）排版后的稿件，可暂不确定出版年、卷、期和页码。整期汇编定稿指出版年、卷、期、页码均已确定的印刷或数字出版的整期汇编稿件。录用定稿网络首发稿件内容必须符合《出版管理条例》和《期刊出版管理规定》的有关规定；学术研究成果具有创新性、科学性和先进性，符合编辑部对刊文的录用要求，不存在学术不端行为及其他侵权行为；稿件内容应基本符合国家有关书刊编辑、出版的技术标准，正确使用和统一规范语言文字、符号、数字、外文字母、法定计量单位及地图标注等。为确保录用定稿网络首发的严肃性，录用定稿一经发布，不得修改论文题目、作者、机构名称和学术内容，只可基于编辑规范进行少量文字的修改。

出版确认：纸质期刊编辑部通过与《中国学术期刊（光盘版）》电子杂志社有限公司签约，在《中国学术期刊（网络版）》出版传播平台上创办与纸质期刊内容一致的网络版，以单篇或整期出版形式，在印刷出版之前刊发论文的录用定稿、排版定稿、整期汇编定稿。因为《中国学术期刊（网络版）》是国家新闻出版广电总局批准的网络连续型出版物（ISSN 2096-4188，CN 11-6037/Z），所以签约期刊的网络版上网络首发论文视为正式出版。

# 单幅图像去模糊的多尺度特征提取和融合网络

武婷婷，万少杰（南京邮电大学 理学院，江苏 南京 210023）

摘要：近年来，多层网络在图像去模糊领域取得了较大进展，但其性能受限于特征提取和残差连接。为解决这些问题，该文提出了一种多尺度融合网络(MSFN)用于图像去模糊，通过多尺度输入与输出，增强了对图像特征的提取能力。MSFN 利用其特征自适应细节增强(ADE)模块和跨尺度特征融合(CSFF)模块，在不同网络深度上捕获不同尺度的特征，优化了特征提取过程，并有效融合了多尺度信息。实验结果表明，所提出的算法在定量分析上表现出色，而且在主观视觉效果上也得到了显著提升，这些结果充分证明了该文提出网络的卓越性能。

关键词：图像去模糊；深度学习；多尺度；细节增强；特征融合中图分类号：TN911 文献标识码：A

Multi-Scale Feature Extraction and Fusion Network for Single Image Deblurring

WU Tingting and WAN Shaojie (College of Science, Nanjing University of Posts and Telecommunications, Nanjing 210023, China)

Abstract：In recent years, significant advancements have been made in the field of image deblurring through multi-layer networks, but their performance is constrained by feature extraction and residual connections. To address these issues, a Multi-Scale Feature Extraction and Fusion Network (MSFN) for image deblurring is proposed in this paper. The core concept of the network is to enhance the extraction capability of image features through multi-scale input and output. Additionally, MSFN employs its feature Adaptive Detail Enhancement (ADE) modules and CrossScale Feature Fusion (CSFF) modules to capture features at different scales across various network depths, optimizing the residual connection process and effectively integrating multi-scale information. Experimental results demonstrate that the proposed algorithm excels in quantitative analysis and achieves a significant improvement in subjective visual effects, which fully demonstrates the excellent performance of the network proposed in this paper.

Key words：image deblurring, deep learning, multiple scale, detail enhancement, feature fusion

图像去模糊是一个经典的计算机视觉问题[1-4]，它的目标是仅凭一张模糊的图像来恢复出原本清晰的图像。这个过程可用数学公式来描述，即模糊图像是由原始清晰图像与模糊核进行卷积操作后，再加上一些随机噪声形成的图像，表示为：

$$
\pmb { I } = \pmb { K } * \pmb { S } + \pmb { n }
$$

这里 $\boldsymbol { \mathbf { \mathit { I } } }$ 表示模糊图像， $\kappa$ 表示模糊核， $s$ 表示清晰图像， $\textbf { \em n }$ 表示随机噪声。因为不同组的清晰图像 $s$ 和模糊核 $\kappa$ 能产生相同的模糊图像 $\boldsymbol { \mathit { \Pi } } _ { I }$ ，所以在已知模糊图像 $\boldsymbol { \mathsf { \Pi } } _ { I }$ 的情况下，公式(1)求解清晰图像是一个不适定问题。

为得到清晰图像，现有的方法提出各种各样的先验[5-7]，使得去模糊问题变得适定。这类先验需要大量的经验和研究推测得到，所以很难进行推广。为改善这一问题，最近提出的方法使用卷积神经网络(Convolutional Neural Networks, CNN)，通过在大规模数据中捕获自然图像的信息来隐式学习更适用的先验。基于CNN 的方法去模糊效果显著主要得益于其模型的设计，现已有许多方法[8-11]开发用于图像恢复的网络模块和功能单元，包括残差学习、空洞卷积、注意力机制、密集连接、编码器-解码器和生成模块。然而，这类方法的模型都是基于单层设计的，虽然可以关注到图像的全局信息，但是随着网络层数的加深，图像中复杂的结构可能会被忽略，从而降低去模糊任务的性能。

因此人们将多层网络用于图像去模糊[12-14]，不过这些方法性能上依旧存在瓶颈。首先，多层网络只使用单一尺度的输入可能无法捕捉到所有重要的特征，特别是在不同尺度上变化的特征。其次，仅使用简单的卷积层来提取每一层的图像，虽然节省模型的算力，但通常只能捕获局部邻域内的信息，会限制它们提取特征的能力，造成信息损失。最后，直接地将各层通过残差相连接，虽有助于解决深层网络训练中的退化问题，却未能充分利用不同尺度上图像的相关信息，网络性能反而下降。

为突破先前工作的局限性，本文提出了一种多尺度特征提取和融合网络(Multi-ScaleFeature Extraction and Fusion Network, MSFN)。该网络探索图像的多尺度输入与输出，以利用跨尺度的图像信息进行单幅图像去模糊。接着，本文开发一个有效的通道自适应细节增强(Adaptive Detail Enhancement, ADE)模块和跨尺度特征融合(Cross-Scale Feature Fusion,CSFF)模块，将所提出模块嵌入到一个端到端的可训练网络中，并且在通用数据集上取得良好的去模糊效果。

本文的主要贡献如下：

(1) 提出一种新颖的多尺度方法，并结合创新性的 CSFF 模块，将图像去模糊任务细化为三个子任务。通过多尺度的输入和输出，网络能够在不同深度上拥有不同大小的感受野，有效捕捉模糊图像中的边缘信息。同时，将网络多个尺度上的特征进行有效融合，避免简单特征连接导致的信息损失，从而获得更全面的图像表示，显著提升了复杂视觉任务的性能。(2) 进一步提出一个有效的 ADE 模块，该模块在每层都充分提取图像的细节和纹理。通过优化特征的提取过程，ADE 模块帮助网络在保持全局上下文的同时，也不丢失部分特征，避免图像的局部模糊。

# 1 相关工作

# 1.1 基于 CNN 的方法

随着深度学习的发展，直接使用 CNN 评估清晰图像的方法层出不穷。在[15]中，Nah等人开发出一种多尺度 CNN，将模糊图像由大尺寸到小尺寸逐一加入网络中训练，在效果上相较之前的方法有较大改善。为更好探索图像的多尺度信息，Tao 等人[16]提出一个更大规模的网络，Gao等人[9]提出共享参数和嵌套残差连接。但是更深和更宽的网络容易在训练数据上过拟合，从而降低模型在未知数据上的泛化能力。Zhang 等人[17]开发一种深层多模块网络，将前一阶段的特征连接起来以便于对下一阶段进行估计。在[18]中，Cui 等人开发一个多分支模块和内容感知模块，局部动态地将特征分解为单独的频率子带，然后使用通道注意力权重来强调特征中有用的部分。但在特征融合的过程中，这类方法直接串联特征向量可能会导致在高层次特征传递过程中的关键视觉信息遭受损失，进而影响模型对复杂计算机视觉问题的全面表征能力。

# 1.2 基于 Transformer 的方法

由于 Transformer 可以对全局上下文进行建模，现已在许多高级视觉任务中取得重大进展。然而 Transformer 应用于图像会导致计算成本增加，为缓解这种状况，Zamir 等人[12]使用图像特征深度域中的点积缩放注意力机制，将不同特征的信息沿通道维度进行传播。虽然能快速去模糊，但没有充分探索图像中的空间信息。Wang 等人[13]提出一种基于 UNet 的Transformer 方法，该方法使用非重叠窗口的自注意机制来进行单幅图像去模糊。尽管使用分割策略降低其计算成本，但多层神经网络仅依赖单一尺度的输入数据，可能无法为特征提取模块提供足够的上下文信息，影响网络对图像的整体理解。

![](images/82e4feb97d1520fe0ec4b2e531a2fd5360ccb3360e9c529c437583b0bc2d4cc7.jpg)  
图 1 MSFN 结构

# 2 网络结构

本文的目标是构建一种用于图像去模糊的 MSFN 模型，该模型在增强图像边缘细节的同时，有效防止了局部模糊现象。为打破去模糊的瓶颈，模型中使用特征增强和融合模块。使得该模型相较于单尺度网络，能够同时专注于图像的全局信息和局部信息。本章首先介绍MSFN 的整体框架(如图1)，接着描述新引入的核心模块：ADE 模块和CSFF 模块，最后讨论损失函数的使用。

# 2.1 整体框架

给定一张模糊图像 $\pmb { J } \in R _ { \lambda } ^ { H \times W \times C }$ ，其中 $H _ { : }$ ，W， $C$ 分别表示图像的高度，宽度和通道维度。首先采用二阶段的下采样处理，生成三种尺寸的图像输入模型。由于模糊图像中的边缘不够清晰，多尺度输入有助于模型捕捉丰富的边缘信息，从而提升细节的识别能力。模型中每层使用 $3 \times 3$ 的卷积核提取图像特征，并引入 ADE 模块捕获局部纹理信息，增强对图像的理解能力。ADE 模块能够整合不同分辨率的特征，适应模糊图像的非均匀性，针对性地处理不同区域。在最低层通过三类 ADE，分别将提取到的特征进行编码、精炼、解码。上层则利用 CSFF 模块，不仅从左边三个 ADE 编码模块中学习到不同尺度的信息，还融合下层 ADE 解码模块中的图像特征，以恢复模糊图像中丢失的高频细节信息。每层 ADE 解码后，通过 $3 \times 3$ 卷积与残差学习得到去模糊的图片。最终串联多个ADE 和CSFF 模块形成一个多尺度特征提取和融合的框架，达到去模糊效果。

Ihigh 日日日：日日日08日 $I _ { 1 }$ →Ienhanced 通道拼接 +通道拼接

![](images/cfe0acb29a8dec9f82d23783f895a8e922a95b32a865537c10bda4c02b3e2786.jpg)  
图 2 ADE 结构  
图 3 CSFF 结构

# 2.2 自适应细节增强(ADE)模块

尽管多尺度网络仅使用 $3 \times 3$ 卷积层提取特征已取得一定效果，但面对模糊图像中的复杂空间结构，这种方法仍显不足。模糊图像中，物体以不同的速度或方向移动会导致不同区域上的局部模糊。对此，本文通过使用 ADE 模块来专注于图像局部主体结构进一步改善特征提取。ADE 模块分为两部分：(a)细节融合和(b)特征增强，如图2 所示。

在细节融合部分，给定高分辨特征张量 $\pmb { I } _ { h i g h } \in { \cal R } ^ { H \times W \times C 1 }$ 和低分辨率特征张量$\pmb { I _ { \imath } } \in \boldsymbol { R } ^ { H \times W \times C 2 }$ 。首先 $I _ { \phantom { } _ { 1 } }$ 通过两对 $3 \times 3$ 和 $\mathrm { 1 \times 1 }$ 的卷积提取特征得到 $I _ { z }$ ，然后使用通道拼接$I _ { \phantom { } _ { 1 } }$ 和 $I _ { z }$ 得到处理后的低分辨率特征 $I _ { \it { 3 } }$ 。为学习到更丰富的细节纹理，对 $I _ { \it { 3 } }$ 进行 $1 \times 1$ 和$3 \times 3$ 的卷积，与上层的高分辨率特征有效地融合得到 $I _ { f u s i o n }$ 。当不存在上层特征时，则不需要细节融合。细节融合过程用数学公式表达为：

$$
\begin{array} { r } { \pmb { I _ { 2 } } = \ b { f _ { 1 } } \ \pmb { f _ { 3 } } \ \pmb { f _ { 1 } } \ \pmb { f _ { 3 } } \ \pmb { I _ { 1 } } } \end{array}
$$

$$
\pmb { I _ { \mathscr { s } } } = \pmb { f _ { c } } ~ \pmb { I _ { \mathscr { t } } } , \pmb { I _ { \mathscr { z } } }
$$

$$
I _ { f u s i o n } = I _ { h i g h } + f _ { 3 } \ f _ { 1 } \ I _ { 3 }
$$

其中 $f _ { 1 }$ 表示 $1 \times 1$ 卷积函数， $f _ { 3 }$ 表示 $3 \times 3$ 卷积函数， $f _ { c }$ 表示通道拼接函数。全局特征提供图像的整体信息，而局部特征捕捉图像的细节。两者的融合为模型提供了全面的视角，增强其对数据的理解。

Ding 等人[19]提出，通过采用大尺寸卷积核(如 $3 1 \times 3 1$ )，能够有效扩展模型的感受野，从而使得模型能够捕获图像中的全局信息以及复杂的形状特征，这一设计使得模型在图像

分类、分割和识别等任务上取得了优异的表现。受到[19]的启发，本文在特征增强部分加入两对 $7 \times 7$ 和 $1 \times 1$ 的卷积。最后使用残差连接得到增强后的特征 $I _ { e n h a n c e d }$ 。数学公式表达为：

$$
{ \cal I } _ { e n h a n c e d } = { \cal I } _ { \imath } + f _ { \imath } f _ { \imath } f _ { \imath } f _ { \imath } f _ { \imath } { \cal I } _ { f u s i o n }
$$

其中 $f _ { 7 }$ 表示 $7 \times 7$ 卷积函数。本文采用 $7 \times 7$ 卷积核的设计，相较于较小卷积核，能够有效扩大模型感受野，同时相比较大卷积核，模型更专注于提取图像局部主体结构的特征。这样设计使得模型在处理局部模糊这一关键难点时更为有效，从而契合去模糊的实际需求。

# 2.3 跨尺度特征融合(CSFF)模块

目前常用的多尺度融合方法通过整合单一尺度图像在不同层级的特征图，来提升模型对不同尺度对象的感知能力。这种技术首先提取浅层和深层的特征图，通过上采样将深层特征图放大至较低分辨率，并与浅层特征图进行逐像素相加，实现特征融合。但这类方法可能限制信息流动，使网络难以捕捉复杂特征。此外，模糊核扩散效应导致高频信息损失，降低图像清晰度，这进一步强调网络在处理模糊图像时需要更强的信息整合能力。因此在MSFN 中，本文将三个ADE 编码模块和一个ADE 解码模块之间引入一个CSFF 模块。不同于多尺度融合方法，CSFF 模块通过融合来自多尺度图像的各层级特征图，为模型提供了从局部到全局的多层次上下文信息。这种方法使模型能够更深入地理解图像内容，并在复杂环境中提高去模糊能力，具体实现方法如下。

图 1 中的两个CSFF 模块在处理前三个编码模块输出的方式不相同。给定编码器 1、编码器2 和编码器3 的特征张量 $\ b X _ { \ b { t } } \in { \ b R } ^ { \ b { H } \times \ b { W } \times 4 8 } , \quad \ b X _ { \ b { 2 } } \in { \ b R } ^ { ( { H } / { 2 } ) \times ( { W } / { 2 } ) \times 9 6 }$ 和 $X _ { g } \in R ^ { ( H / 4 ) \times ( W / 4 ) \times 1 9 2 }$ ，解码器的特征张量 $\pmb { Y } _ { \pmb { \imath } } \in \pmb { R } ^ { H 1 \times W 1 \times C } \cup$ 。上层的CSFF模块需要将 $X _ { z }$ 上采样2倍， $X _ { _ { 3 } }$ 上采样4倍；下层 CSFF 模块则需要将 $X _ { _ { 1 } }$ 下采样 2 倍， $X _ { \mathfrak { s } }$ 上采样 2 倍。图 3 展示了上层CSFF 模块的结构。该模块首先将三个编码器得到的深层特征经过上述处理后拼接在一起，再进行 $1 \times 1$ 和$3 \times 3$ 卷积。在学习过程中，通过随机归零部分权重降低节点间的相互依赖，再与 $X _ { \iota }$ 进行残差连接得到融合特征 $X _ { 4 } ^ { \gamma }$ 解码器的张量 $Y _ { _ { I } }$ 通过 $3 \times 3$ 卷积和像素重组后，再与 $X _ { _ { 4 } }$ 拼接，最终进行 $1 \times 1$ 卷积得到不同尺度融合后的特征。CSFF 模块使用数学公式表达为：

$$
\pmb { X } _ { 2 } = \ b { f } _ { u p } ^ { 1 } \ \pmb { X } _ { 2 }
$$

$$
\pmb { X } _ { 3 } = f _ { u p } ^ { 2 } \pmb { X } _ { 3 }
$$

$$
X _ { _ { \it 4 } } = X _ { _ { \it 1 } } + f _ { _ { \it d } } f _ { _ { 3 } } f _ { _ { 1 } } f _ { _ { c } } X _ { _ { \it 1 } } , X _ { _ { \it 2 } } , X _ { _ { \it 3 } }
$$

$$
X _ { _ { f u s i o n } } = f _ { 1 } \ f _ { _ c } \ X _ { _ { 4 } } , f _ { _ p } \ f _ { _ 3 } \ Y _ { _ { 1 } }
$$

其中 $f _ { u p } ^ { i }$ 表示上采样至原始尺寸的 $2 ^ { i }$ 倍， $f _ { d }$ 表示随机失活函数， $f _ { p }$ 表示像素重组函数。本文提出的 CSFF 相较残差连接具有显著优势：首先，CSFF 通过融合不同尺度的特征，

有效减少过程中因重复上采样和下采样函数导致重要信息丢失的影响。其次，该模块利用多尺度特征丰富每一层的表示，为复杂视觉任务提供更全面的特征理解。最后，CSFF 的多尺度特性使模型能够灵活适应输入变化，有效处理各种分辨率的模糊图像。

# 2.4 损失函数

本文提出一种多尺度网络架构，旨在通过不同尺度的图像处理，提高模型去模糊能力。该网络结构由三个尺度组成，分别对应输入图像尺寸的 1 倍、1/2 倍和 1/4 倍。这种策略有助于平衡模型对细节和全局特征的处理，并有效提取多级边缘信息。为训练网络，本文构建了一个复合损失函数，评估模型输出与清晰图像之间的差异，并指导模型的优化过程。损失函数定义如下：

$$
L = \sum _ { i = 0 } ^ { 2 } \lambda _ { i } \left. f _ { d o u n } ^ { i } \textbf { \em S } - \boldsymbol { Z } _ { i } \right. _ { 2 } ^ { 2 }
$$

其中， $s$ 表示清晰图像， $\boldsymbol { Z } _ { i }$ 表示第 $i$ 尺度的输出图像，fi 函数表示下采样至 $1 / 2 ^ { i }$ 倍。$\lambda _ { i }$ 表示第 $i$ 尺度的权重参数。通过这种权重分配，为不同尺度的特征提供差异化的学习信号，确保模型在训练过程中对每个尺度给予适当的重视，从而促进多尺度学习。

# 3 实验与评估

本节通过将 MSFN 模型与一些先进的方法进行比较，分别使用定性分析和定量分析的方式验证模型在图像去模糊中的有效性。在定量分析中，首先使用峰值信噪比[20](PeakSignal-to-Noise Ratio, PSNR)和结构相似性(Structural Similarity Index Measure, SSIM)来客观地比较不同方法去模糊效果的差异。PSNR 与 SSIM 定义如下：

$$
\begin{array}{c} \begin{array} { r c l } { \displaystyle \langle \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \big \mathbf { \Theta } ^ { \prime } \bigtriangleup \mathbf { \Theta } ^ { \prime } } \\ { \displaystyle \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \big \bigtriangleup } \end{array} \textstyle \overset { \mathrm { S S N R } = 1 0 \mathrm { l g } ( \frac { H W } { \bigstar - y \bigstar _ { 2 } ^ { 2 } } ) } { \bigstar \bigtriangleup \bigtriangleup \bigtriangleup \bigtriangleup \big \phi } ^ { 2 }  \end{array}
$$

其中， $\textbf { \em x }$ 与 $\textbf {  { y } }$ 表示两张数值在(0,1)之间的图像， $\boldsymbol { \mu } _ { \boldsymbol { x } }$ 与 $\boldsymbol { \mu } _ { _ y }$ 表示 $\textbf { \em x }$ 与 $\textbf {  { y } }$ 的平均亮度， $\sigma _ { x } ^ { 2 }$ 与 $\sigma _ { y } ^ { 2 }$ 表示 $\scriptstyle { \mathbf { { \boldsymbol { x } } } }$ 与 $\textbf {  { y } }$ 的亮度方差， $\sigma _ { x y }$ 表示亮度协方差，c1 与c2 为常数。较高的PSNR 和SSIM 值表示图像的细节、纹理等视觉特征得到较好的保留，去模糊效果较好。

为深入评估 MSFN 在图像去模糊领域的性能，本文进一步分析其在目标检测任务中的应用，并量化对检测精度的实际提升。采用平均精度[21](Average Precision, AP)作为衡量指标，来评估去模糊后的目标检测准确度。AP 的计算方法具体如下：

$$
P _ { \mathrm { i n t e r p } } \ R \ = \operatorname* { m a x } _ { \tilde { R } : \tilde { R } \geq R } P \ \tilde { R }
$$

$$
A P = \frac { 1 } { 1 1 } \sum _ { R \in \{ 0 , 0 . 1 , . . . , 0 . 9 , 1 \} } P _ { \mathrm { i n t e r p } } \ R
$$

其中召回率 $R$ 可在{0, 0.1,…, 0.9, 1}中设定阈值， $P _ { \mathrm { i n t e r p } } \ R$ 表示在大于等于阈值 $R$ 的样本中

选取精确率的最大值。AP 指标综合考虑不同召回率下的精确率，以评估模型的整体性能。因此，对于同一种目标检测算法，去模糊效果越好，图像越清晰，目标检测越准确，从而AP 值也越高。

表 1 测试集上的去模糊比较  

<html><body><table><tr><td rowspan="2">模型</td><td colspan="2">GoPro</td><td colspan="2">HIDE</td></tr><tr><td>PSNR(dB)</td><td>SSIM</td><td>PSNR(dB)</td><td>SSIM</td></tr><tr><td>DeblurGAN[8]</td><td>28.70</td><td>0.858</td><td>24.51</td><td>0.871</td></tr><tr><td>DeepDeblur[15]</td><td>29.08</td><td>0.914</td><td>25.73</td><td>0.874</td></tr><tr><td>Zhang et al.[22]</td><td>29.19</td><td>0.931</td><td></td><td></td></tr><tr><td>DeblurGAN-v2[23]</td><td>29.55</td><td>0.934</td><td>26.61</td><td>0.875</td></tr><tr><td>SRN[16]</td><td>30.26</td><td>0.934</td><td>28.36</td><td>0.915</td></tr><tr><td>Shen et al.[24]</td><td>30.26</td><td>0.940</td><td>28.39</td><td>0.930</td></tr><tr><td>Gao et al.[9]</td><td>30.90</td><td>0.935</td><td>29.11</td><td>0.913</td></tr><tr><td>DBGAN[10]</td><td>31.10</td><td>0.942</td><td>28.94</td><td>0.915</td></tr><tr><td>MTRNN[25]</td><td>31.15</td><td>0.945</td><td>29.15</td><td>0.918</td></tr><tr><td>DMPHN[17]</td><td>31.20</td><td>0.940</td><td>29.09</td><td>0.924</td></tr><tr><td>Suin et al.[26]</td><td>31.85</td><td>0.948</td><td>29.98</td><td>0.930</td></tr><tr><td>SPAIR[27]</td><td>32.06</td><td>0.953</td><td>30.29</td><td>0.931</td></tr><tr><td>MIMO-UNet+[28]</td><td>32.45</td><td>0.957</td><td>29.99</td><td>0.930</td></tr><tr><td>MPRNet[29]</td><td>32.66</td><td>0.959</td><td>30.96</td><td>0.939</td></tr><tr><td>Restormer[12]</td><td>32.92</td><td>0.961</td><td>31.22</td><td>0.942</td></tr><tr><td>Uformer[13]</td><td>33.05</td><td>0.962</td><td>30.89</td><td>0.940</td></tr><tr><td>MSSNet[30]</td><td>33.01</td><td>0.961</td><td>30.79</td><td>0.938</td></tr><tr><td>NAFNet[14]</td><td>33.08</td><td>0.963</td><td>31.22</td><td>0.943</td></tr><tr><td>FSNet[18]</td><td>33.29</td><td>0.963</td><td>31.05</td><td>0.941</td></tr><tr><td>MSFN(本文)</td><td>33.64</td><td>0.967</td><td>31.59</td><td>0.947</td></tr></table></body></html>

# 3.1 数据集

本文使用两个数据集：GoPro 数据集[15]和HIDE 数据集[24]。GoPro 数据集专为图像去模糊设计，包含2103组训练图像和1111组测试图像，每组包括一张模糊图像及其对应的清晰图像。HIDE 数据集采用类似合成方法，专注于行人图像，包含 6397 组训练图像和 2025 组测试图像。模型仅利用 GoPro 数据集进行网络训练，并在 GoPro 和 HIDE 的测试集上进行评估，以检验MSFN 的去模糊效果和泛化能力。

# 3.2 实验设置

本文采用适应性矩估计算法(Adaptive Moment Estimation, Adam)训练 MSFN 模型，初始学习率为 $1 \times 1 0 ^ { - 4 }$ ，通过余弦退火策略逐步降至 $1 \times 1 0 ^ { - 6 }$ 。随着学习率的降低，批次大小减少至[4, 2, 1]，图像尺寸增大至[128, 256, 512]。数据增强包括随机裁剪，旋转和饱和度调整。损失函数中， $\lambda _ { _ 1 } = 1$ ， $\lambda _ { 2 } = 1 / 2$ ， $\lambda _ { 3 } = 1 / 4$ 。

# 3.3 去模糊结果与分析

首先将本文的去模糊结果与当前最先进的方法得到的结果进行比较，这些方法包括基于 CNN 和基于 Transformer 的算法。所有的方法都在 GoPro 训练集上训练，并且在 GoPro和 HIDE 测试集上进行比较。通过直接引用已有实验数据的结果，对于缺少数据的方法，则根据原作者的参数设置进行测试，来确保实验的公正性。

表1展示在GoPro和HIDE测试集上的定量评估结果，本文提出的方法在PSNR和SSIM上均取得了最优值。在 GoPro 测试集上，与基于 CNN 的方法 FSNet[18]相比，本方法在PSNR 上提升 0.35dB，在 SSIM 上提升 0.004；与基于 Transformer 的 Uformer[13]相比，PSNR提升 $0 . 5 9 \mathrm { d B }$ ，SSIM 提升 0.005。在 HIDE 测试集上，与 NAFNet[14]相比，PSNR 提升 $0 . 3 7 \mathrm { d B }$ ，SSIM 提升 0.004；与 Restormer[12]方法相比，PSNR 提升 0.37dB，SSIM 提升 0.005。这些结果表明，本方法在图像去模糊任务中具有显著的性能优势。

为验证MSFN 去模糊算法的效果，本文在GoPro 和HIDE 测试集上选取代表性图像，并与顶尖去模糊算法进行对比。对比结果在图 4 和图 5 中展示，旨在深入分析算法在细节特征处理上的性能。

![](images/bb6932d42edc9e95d11c1cf8939a172aea62fcbefdcc1621e87fac91805093c7.jpg)  
图 4 GoPro 数据集上的实验结果

图 4 展示了在 GoPro 数据集上，本研究提出的方法与其他方法之间的视觉比较。实验结果表明，MSFN 模型能够更准确地恢复叶片的纹理细节，同时保持边缘的清晰度。在汽车图像的实验中，着重考察汽车牌照区域的去模糊性能。本文所提出的算法在汽车牌照的去模糊处理上显示出了卓越的性能，能够有效地提高牌照文字的可识别性。另外在 HIDE数据集的实验中，本算法在衣物纹理的去模糊效果上显著优于现有技术，并且在眼睛、鼻子和嘴巴等关键区域的清晰度和细节表现上提供卓越的视觉质量。这进一步表明本文提出的多尺度网络在图像细节恢复方面具有显著优势。

![](images/1d0ced5219e44614f924a21471a4f0b8820061a56b6ac59567e7f486e6a712a6.jpg)  
图 5 HIDE 数据集上的实验结果

# 3.4 目标识别性能增强

为全面评估 MSFN 在图像去模糊领域的性能，本节探讨去模糊技术对目标检测任务精度的影响。首先选用业界认可的先进算法YOLOv10进行目标检测。由于缺乏专门针对目标检测的模糊数据集，本文基于HIDE 测试集进行扩展构建新数据集，并且利用AP 评估去模糊效果。目标检测模型的高精度性将直接反映去模糊算法在提升图像清晰度方面的有效性。

表 2 HIDE 数据集去模糊后目标检测不同类别的 AP 值  

<html><body><table><tr><td>模型</td><td>行人 手提包</td><td>背包 自行车</td><td>雨伞</td><td>汽车</td><td>平均值</td></tr><tr><td>Blur</td><td>0.648 0.272</td><td>0.412</td><td>0.365 0.585</td><td>0.280</td><td>0.427</td></tr><tr><td>MTRNN[25]</td><td>0.891 0.676</td><td>0.744</td><td>0.683 0.837</td><td>0.601</td><td>0.739</td></tr><tr><td>DMPHN[17]</td><td>0.875 0.681</td><td>0.743</td><td>0.695 0.846</td><td>0.605</td><td>0.741</td></tr><tr><td>MIMO-UNet+[28]</td><td>0.903 0.659</td><td>0.753</td><td>0.705 0.878</td><td>0.616</td><td>0.752</td></tr><tr><td>MPRNet[29]</td><td>0.917 0.703</td><td>0.780</td><td>0.712 0.901</td><td>0.606</td><td>0.770</td></tr><tr><td>Restormer[12]</td><td>0.919 0.718</td><td>0.779</td><td>0.735 0.896</td><td>0.621</td><td>0.778</td></tr><tr><td>NAFNet[14]</td><td>0.917 0.725</td><td>0.788</td><td>0.755 0.897</td><td>0.625</td><td>0.785</td></tr><tr><td>FSNet[18]</td><td>0.922 0.703</td><td>0.796</td><td>0.752 0.895</td><td>0.614</td><td>0.780</td></tr><tr><td>MSFN(本文)</td><td>0.924 0.743</td><td>0.803</td><td>0.760</td><td>0.900 0.657</td><td>0.798</td></tr></table></body></html>

本研究从HIDE 测试集中挑选六个最常见且具有代表性的类别构建数据集。表2展示使用不同去模糊技术后，YOLOv10 模型在这些类别上的 AP 及其总体平均值。与平均值排名第二的NAFNet[14]相比，本文提出的模型在平均值上提升0.013。在六个类别上，AP提升分别为 0.007、0.018、0.015、0.005、0.003 和 0.032。除在雨伞类别上略有不足外，MSFN 在所有类别上均取得最佳成绩，这充分证明模型在去模糊方面的出色效果。

图6 展示不同去模糊方法对YOLOv10 目标检测性能的影响。结果显示，去模糊技术能有效提高模型对行人的检测准确率，尤其是 MSFN 模型，显著提升检测精度和行人识别数量。这验证了本文方法在图像清晰度恢复和细节增强方面的优势，对增强目标检测系统的实用性和稳定性具有关键作用。

![](images/306ff8c523277b9ee0577f8a092fc30b123e34e76ffb20e724a03d2417bb81aa.jpg)  
图 6 HIDE 数据集去模糊后目标检测的结果

# 4. 结束语

本文针对现有网络在图像去模糊领域的局限性，提出一种创新的 MSFN，通过引入多尺度输入与输出机制，显著提升模型特征的提取和利用效率，从而更好地恢复图像中的边缘信息和局部模糊。MSFN 的核心贡献在于其 ADE 模块和 CSFF 模块，这两个模块的结合不仅优化了特征提取过程，还有效地融合多尺度信息，从而在图像去模糊任务中实现显著性的提升。实验结果表明 MSFN 在定量和定性分析上的卓越表现，这不仅能展示网络本身的高性能，也验证每个模块的有效性。特别是 ADE 模块在细节纹理信息的提取上可以发挥重要作用，而 CSFF 模块则通过有效融合不同尺度的特征，为复杂视觉任务提供更全面的图像表示。但在实验中注意到在多尺度特征融合和上采样阶段可能会不自觉地放大图像中的噪声，尤其是在处理高动态范围或复杂纹理的图像时，这种噪声放大现象尤为明显。为解决这一问题，未来的工作将探索创新的自适应滤波器，能够根据图像内容动态调整参数，以实现更有效的噪声抑制，从而提高图像去模糊的整体性能。

参 考 文 献   
[1] 李海波, 邵文泽. 图像盲去模糊综述:从变分方法到深度模型以及延伸讨论[J].南京邮电大学 学报（自然科学版）, 2020, 40(5): 11. LI Haibo, SHAO Wenze. Blind image deblurring: An overview from variational approaches to deep representation models and beyond[J]. Journal of Nanjing University of Posts and Telecommunications (Natural Science Edition), 2020, 40(5): 11. (in Chinese)   
[2] CUI Y, KNOLL A. Exploring the potential of channel interactions for image restoration[J]. Knowledge-Based Systems, 2023, 282(2): 1153-1169.   
[3] PHAM B D, TRAN P, TRAN A, et al. Blur2Blur: Blur conversion for unsupervised image deblurring on unknown domains[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2024: 2804-2813.   
[4] 赵吴帆, 武文娜, 武婷婷. 各向异性的l0 正则化图像平滑方法[J].南京邮电大学学报（自然科 学版）, 2024, 44(4): 131-138. ZHAO Wufan, WU Wenna, WU Tingting. An anisotropic l0 regularized image smoothing method[J]. Journal of Nanjing University of Posts and Telecommunications (Natural Science Edition) , 2024, 44(4): 131-138. (in Chinese)   
[5] XU L, ZHENG S C, JIA J Y. Unnatural l0 sparse representation for natural image deblurring[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2013: 1107- 1114.   
[6] PAN J S, SUN D Q, PFISTER H, et al. Blind image deblurring using dark channel prior[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2016: 1628- 1636.   
[7] ZHANG M N, FANG Y Y, NI G X, et al. Pixel screening based intermediate correction for blind deblurring[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2022: 5892-5900.   
[8] KUPYN O, BUDZAN V, MYKHAILYCH M, et al. DeblurGan: Blind motion deblurring using conditional adversarial networks[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8183-8192.   
[9] GAO H Y TAO X, SHEN X Y, et al. Dynamic scene deblurring with parameter selective sharing and nested skip connections[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 3848-3856.   
[10] ZHANG K H, LUO W H, ZHONG Y R, et al. Deblurring by realistic blurring[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2020: 2737-2746.   
[11] CUI Y, REN W, CAO X, et al. Revitalizing convolutional network for image restoration[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024, 47(1): 1-16.   
[12] ZAMIR S W, ARORA A, KHAN S, et al. Restormer: Efficient transformer for high-resolution image restoration[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2022: 5728-5739.   
[13] WANG Z D, CUN X D, BAO J M, et al. Uformer: A general u-shaped transformer for image restoration[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2022: 17683-17693.   
[14] CHEN L Y, CHU X J, ZHANG X Y, et al. Simple baselines for image restoration[C] // European Conference on Computer Vision. 2022: 17-33.   
[15] NAH S, HYUN K T, MU L K. Deep multi-scale convolutional neural network for dynamic scene deblurring[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 3883-3891.   
[16] TAO X, GAO H Y, SHEN X Y, et al. Scale-recurrent network for deep image deblurring[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 8174- 8182.   
[17] ZHANG H G, DAI Y C, LI H D, et al. Deep stacked hierarchical multi-patch network for image deblurring[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2019: 5978-5986.   
[18] CUI Y N, REN W Q, CAO X C, et al. Image restoration via frequency selection[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023, 46(2): 1093-1108.   
[19] DING X H, ZHANG X Y, HAN J G, et al. Scaling up your kernels to 31x31: Revisiting large kernel design in cnns[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2022: 11963-11975.   
[20] HORE A, ZIOU D. Image quality metrics: psnr vs. ssim[C] // Proceedings of 20th International Conference on Pattern Recognition. 2010: 2366-2369.   
[21] RAVPREET K, SARBJEET S. A comprehensive review of object detection with deep learning[J]. Elsevier Digital Signal Processing, 2023, 132(3): 1013-1029.   
[22] ZHANG J W, PAN J S, REN J, et al. Dynamic scene deblurring using spatially variant recurrent neural networks[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018: 2521-2529.   
[23] KUPYN O, MARTYNIUK T, WU J, et al. Deblurgan-v2: Deblurring (orders-of-magnitude) faster and better[C] // Proceedings of the IEEE International Conference on Computer Vision. 2019: 8878-8887.   
[24] SHEN Z Y WANG W G, LU X K, et al. Human-aware motion deblurring[C] // Proceedings of the IEEE International Conference on Computer Vision. 2019: 5572-5581.   
[25] PARK D, KANG D U, KIM J, et al. Multi-temporal recurrent neural networks for progressive non-uniform single image deblurring with incremental temporal training[C] // European Conference on Computer Vision. 2020: 327-343.   
[26] SUIN M, PUROHIT K, RAJAGOPALAN A N. Spatially-attentive patch-hierarchical network for adaptive motion deblurring[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2020: 3606-3615.   
[27] PUROHIT K, SUIN M, RAJAGOPALAN A N, et al. Spatially-adaptive image restoration using

distortion-guided networks[C] // Proceedings of the IEEE International Conference on Computer Vision. 2021: 2309-2319.   
[28] CHO S J, JI S W, HONG J P, et al. Rethinking coarse-to-fine approach in single image deblurring[C] // Proceedings of the IEEE International Conference on Computer Vision. 2021: 4641-4650.   
[29] ZAMIR S W, ARORA A, KHAN S, et al. Multi-stage progressive image restoration[C] // Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2021: 14821- 14831.   
[30] KIM K, LEE S, CHO S. Mssnet: Multi-scale-stage network for single image deblurring[C] // European Conference on Computer Vision. 2022: 524-539.

![](images/81a34aae5d64fbb2234718514bfeade4ed4af2f3e5df00dd0c215bd7353c5074.jpg)