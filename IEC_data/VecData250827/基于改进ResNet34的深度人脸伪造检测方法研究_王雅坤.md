# 基于改进ResNet34的深度人脸伪造检测方法研究

王雅坤，张宝林，王兆成，马琬雲，郭仕佳（河北工业大学 电子信息工程学院 天津 300401）

摘要 深度人脸伪造技术的非法应用对人民群众的财产安全造成了严重危害，当前伪造视频检测准确率低且面对新型伪造技术的泛化能力差。针对以上问题，提出了一种注意力机制改进ResNet34 的深度人脸伪造检测方法：引入了高效通道注意力模块，无需进行降维和升维的操作，从而保留了原始通道特征的信息完整性；将伪造图像中的伪影区域提取出来，作为主干特征输入模型，增强模型对人脸局部特征的检测性能；利用多尺度滑动窗口和不同的混合函数来生成带有伪影区域位置信息的标注，方便伪影检测模块对伪影特征的检测。实验结果表明，本研究方法效果显著，在 $\mathrm { F F } + + \left( \mathrm { c } \ o { 2 3 } \right) ,$ 数据集上检测准确率为$9 7 . 8 8 \%$ ，AUC 值为 $9 9 . 8 4 \%$ 。

关 键 词 深度人脸伪造； ResNet34；高效通道注意力；伪影检测；多尺度滑窗中图分类号 TP391.4 文献标志码 A

# Research on deep face forgery detection method based on improved ResNet34

WANG Yakun, ZHANG Baolin, WANG Zhaocheng, MA Wanyun, GUO Shijia （ School of Electronics and Information Engineering, Hebei University of Technology, Tianjin 300401, China ）

Abstract The illegal application of deep face forgery technology has caused serious harm to the people’s property secu⁃ rity, and the current forgery video detection accuracy is low and the generalization ability is poor in the face of new forg⁃ ery technology. Aiming at solving the above problems, an attention mechanism is proposed to improve the deep face forg⁃ ery detection method of ResNet34. First, an efficient channel attention module is introduced, which eliminates the need for dimensionality reduction and dimensionality upgrading, thus preserving the information integrity of the original chan⁃ nel features; second, the artifact region in the forged image is extracted and input into the model as a backbone feature, which enhances the model’s performance of detecting local features of the face; third, the labeling with the position infor⁃ mation of the artifact region is generated by using the multiscale sliding window and different mixing functions , which fa⁃ cilitates the detection of artifact features by the artifact detection module. The experimental results show that the method of this study has a detection accuracy of $9 7 . 8 8 \%$ and an AUC value of 99.84% on the FF $^ { + + }$ (c23) dataset, and compared with the latest methods, the method of this study has the best generalization ability, which proves the validity of the meth⁃ od proposed in this study.

Key words deep face forgery; ResNet34; efficient channel attention; artifact detection; multi-scale sliding window

# 0 引言

随着科技的飞速发展，深度伪造技术已逐渐成为一种潜在的巨大威胁。基于Deepfakes 等技术的“换脸”视频在国内国际网络上泛滥，这种技术能够将视频中的人脸替换为目标人物，制造出伪造的动作或场景[1-5]。真实的形象被肆意篡改，用于传递伪造信息，这类经过篡改的视频经过多重传播之后可能会影响社会稳定，甚至影响国家安全。

为了解决上述问题，众多研究者致力于开发深度伪造视频中人脸伪造的检测方法[6-8]。由于深度伪造视频在帧级上存在的差异和伪影，帧级检测方法能够有效地捕捉到这些异常，从而实现对深度伪造视频的准确检测[9]。俞洋等[10]通过对图像中局部特征进行掩码处理，提高了网络对关键区域的关注程度。Guera等[11]使用卷积网络对视频帧的图像特征进行提取，然后训练RNN模型进行深度人脸伪造检测。Zhao等[12]通过多头注意力增强充分检测视频帧图像的细微局部细节，进而区分真实和伪造。上述方法由于仅针对局部区域进行检测，导致泛化能力较差。

与帧级检测关注每一帧的细微差异不同，视频级检测更注重整个视频序列的连贯性和一致性。视频级检测通过分析整个视频的结构、运动轨迹、光照变化等全局特征，来识别是否存在深度伪造的可能性。Masi等[13]使用双流网络对伪造视频进行检测，一方面放大频带频率，另一方面利用压缩自然面部特征，在空域中进行检测，提升了跨数据集的检测性能。Zheng等[14]使用网络在强化时间序列信息传递能力的同时，需要克服由噪声和数据压缩等引起的时间序列不一致性问题。这些问题可能会导致检测过程中的关键信息丢失，从而削弱模型的鲁棒性。

现有的深度人脸伪造检测技术对高质量图像或视频的分类难度较高，各个方案只能达到较低的检测准确率。此外，当前不同数据集采用的深度伪造技术多种多样，一种算法很难精准检测多种伪造人脸。一方面，从高质量图像局部区域提取的伪影特征可以有效地引起检测器关注，并引导分类器精准分类。另一方面，研究人员容易忽略注意力机制在提升网络模型分类性能和特征表示能力方面的优势，因为伪造视频在提取帧时会发现帧间不一致性的伪影，这一特征容易被注意力机制关注。

针对现有问题，本研究提出了一种注意力机制改进ResNet34模型的深度人脸伪造检测方法。本研究主要从提取伪影特征和引入注意力机制两方面入手。首先，引入了高效通道注意力机制，保留原始通道特征的信息完整性，提升特征表示能力，增强模型的分类性能；其次，通过伪影检测模块增强对高质量视频帧局部特征的关注，提高模型检测准确率；最后，设计了一种多尺度人脸交换方法，通过对伪影区域准确标注和交换，指导伪影检测模块对伪影精准定位，并进一步丰富了训练集中的伪影特征。本研究通过消融实

验验证了各个模块的有效性，并且在进行跨数据集实验时保持高检测准确率的同时还展现出了最佳的泛化能力。

# 1 理论基础

本研究采用 He 等[15]在 2015 年的 ImageNet 比赛上使用的ResNet34模型作为骨干网络。如图 1所示，每个残差块都包含一个短路连接，这有助于信息的传递和梯度的流动。ResNet34模型能够提取视频帧中的关键特征，通过对这些特征的学习和分析，有效识别伪造视频中的异常和不一致性。ResNet34模型还具有较高的稳定性和泛化能力，能够处理不同来源、不同伪造手段的视频数据，并保持较高的检测准确率。

在图 2中，左侧为实线的残差结构，主要由两层$3 { \times } 3$ 卷积层组成，右侧连接线是捷径分支。右侧为虚线的残差结构，通过捷径分支进行降维处理。

# 2 模型结构

为了更精确地检测伪造痕迹，且有效区分伪造特征，构建了如图 3所示的基于改进ResNet34的深度人脸伪造检测模型。首先，在图像输入阶段通过多尺度人脸交换模块给图像上伪影区域进行精细化位置标注

![](images/730845ced52e6a2e799053423c3a564bddff36fe453b50054edef386edbbf822.jpg)  
图 1 残差结构原理图

![](images/d15ba40ef8b62f10d8801e8f2322ebdc2179c9ffea663a57e0ed9b94453b9e51.jpg)  
Fig.1 Schematic diagram of residual structure   
图 2 ResNet34 的 $\mathrm { C o n v 2 \_ x }$ 结构Fig.2 $\mathrm { C o n v 2 \_ x }$ structure of ResNet34

并生成新型伪造图像；然后，引入了高效通道注意力模块，通过全局平均池化和快速卷积操作，极大地提高了模型的分类性能；其次，将主干特征送入伪影检测模块，利用图像上的多尺度锚点来检测图像上的伪影区域；最后，将伪影检测模块的输出送入全连接层生成最终预测结果。

![](images/87f4f18aad3ebf3b85a8f7c13c87d509d2a9d678e2efc562488c5e62c65cfdbb.jpg)  
图 3 基于改进 ResNet34 的深度人脸伪造检测模型 Fig.3 Deep face forgery detection model based on improved ResNet34

# 2.1 高效通道注意力模块

$\mathrm { H u }$ 等[16]通过对SENet 将信息降维过程中产生数据丢失缺陷的改进，得到了高效通道注意力（EfficientChannel Attention，ECA）模块。捕捉通道之间的信息交互，提升了模型准确度，且系统的复杂度增加较少。在复杂背景下，伪造图像的特征并不明显，因此增加ECA 模块有助于提升伪造特征的提取。图4 为ECA模块的结构图。

![](images/dba4b3559e7cda93bf3066915a489ee0e23971f46d7e079894a55401f709b95a.jpg)  
图 4 ECA 模块结构示意图  
Fig.4 Schematic diagram of the ECA module structure

ECA模块主要将输入的特征图进行全局平均池化和快速卷积操作，卷积核 $k$ 大小可以通过式（1）获得：

$$
k = \psi ( C ) = \left| \frac { \log _ { 2 } ( C ) } { \gamma } + \frac { b } { \gamma } \right| _ { \mathrm { o d d } } ,
$$

式中： $C$ 代表通道维数； $\psi$ 表示映射； $\gamma$ 和 $b$ 是映射常数， $\gamma$ 通常设置为2， $b$ 通常设置为1； $| \cdot | _ { \mathrm { o d d } }$ 表示该

数向上取整的最小奇数。

# 2.2 伪影检测模块

伪影检测模块 （Artifact Detection Module，ADM）的总体结构如图 5所示。ADM以从主干提取的特征作为输入，基于多尺度锚点检测伪影区域的位置。具体地说，在主干的末端增加了4 个不同尺度的额外层，其中特征图的大小随着元组（ $7 { \times } 7$ ， $5 { \times } 5$ ， $3 { \times } 3$ ， $1 { \times } 1$ ）的减小而减小。在训练阶段，多尺度检测模块被放置在前3个额外的层之后，利用图像上的多尺度默认锚来检测伪造图像上的伪影区域。多尺度检测模块在每个额外的层之后增加一个检测器和一个分类器，以输出图像上每个默认锚的位置偏移量（ $\mathrm { . } N { \times } 4$ ）和类别的置信度（ $\mathbf { \nabla } N { \times } 2$ ，即假或真锚）。如果默认锚框与伪影区域的基本真值之间的并集交点（IOU）大于阈值，则默认锚框被注释为假。此外，ADM最终的 $1 { \times } 1$ 特征图创建了与骨干网络末端的短连接，这进一步丰富了ADM学习的伪影特征。然后，它的输出被馈送到全连接层中，以生成最终预测。总之，ADM用来确定在多尺度锚点中是否存在伪影区域。这样的架构有助于本研究的模型减弱对图像全局身份特征的关注，减少了伪造特征遗漏的影响。

![](images/c34deab01aef11a197c1bd3dfe72354cf6080941e393b55436b69e5d24e130d9.jpg)  
图 5 伪影检测模块Fig.5 Artifact detection module

# 2.3 多尺度人脸交换模块

伪影检测模块的训练需要图像上伪影区域的精细和局部位置注释，这在公共深度伪造数据集[17-18]中未标出。为此，本研究提出了多尺度人脸交换（Multi-scale Face Swap，MFS）方法，该方法使用多尺度滑动窗口和不同的混合函数来生成带有伪影区域位置标注的新的伪图像。此外，新的伪图像还进一步丰富了训练集中的伪影特征。

为了生成带有伪影区域位置标注的新伪图像，MFS通过全局交换和部分交换两种方式处理成对的伪图像和源图像。在局部交换过程中，MFS首先选择一个随机大小的滑动窗口来定位伪影区域。为了找到最可能存在伪影的局部区域，可以通过式（2）选择滑动窗口：

$$
x _ { t } , y _ { t } = \arg _ { x , y } \operatorname* { m a x } \sum _ { i = x } ^ { x + h } \sum _ { j = y } ^ { y + w } \mathrm { D S S I M } ( I _ { \mathrm { F } } , I _ { \mathrm { S } } ) _ { i , j } \ ,
$$

式中： $x _ { t } , y _ { t }$ 表示选择的滑动窗口的高度和宽度；DSSIM(·) 表示结构差异，当值较大时意味着该区域更有可能包含伪造痕迹； $^ { \cdot } x , y ^ { \cdot }$ 表示滑动窗口在图像上的顶部位置； ${ \mathbf { } } h , w$ 表示滑动窗口的高度和宽度； $I _ { \mathrm { { F } } } , I _ { \mathrm { { s } } }$ 则表示伪图像和源图像。然后，基于所选择的滑动窗口，计算掩码 $M$ 来生成新的伪图像。通过裁剪假图像上的滑动窗口区域并生成新的假图像 $\left( { I _ { \mathrm { { F } } } } ^ { \prime } \right)$ 来获得伪影区域的地面真实值，公式为

$$
{ I _ { \mathrm { e } } } ^ { \prime } \mathrm { = B L E N D I N G } ( I _ { \mathrm { e } } , I _ { \mathrm { s } } , M ) \ ,
$$

式中： BLENDING(·) 表示不同的混合方法[19]。以Alpha 混合[19]为例： ${ I _ { \mathrm { { F } } } } ^ { \prime } = I _ { \mathrm { { F } } } \times M + I _ { \mathrm { { s } } } \times ( 1 - M )$ 。 ${ I _ { \mathrm { F } } } ^ { \prime }$ 的伪影区域位置是 $\left[ x _ { t } , y _ { t } , x _ { t } + h , y _ { t } + w \right]$ 。在全局交换过程中，滑动窗口大小等于源图像。MFS 随后生成类似于Facex-ray的新的假图像[19]，提供了更多具有弹性变形的面部区域。

# 2.4 损失函数

本研究总损失函数 $( L )$ 是全局分类损失 $L _ { \mathrm { c l s } }$ 和检测损失 $L _ { \mathrm { d e t } }$ 的加权和， $\beta$ 是控制 $L _ { \mathrm { c l s } }$ 和 $L _ { \mathrm { d e t } }$ 之间权衡的正标量：

$$
{ \cal L } = \beta { \cal L } _ { \mathrm { d e t } } + { \cal L } _ { \mathrm { c l s } } \ \mathrm { ~ o ~ }
$$

$L _ { \mathrm { c l s } }$ 是衡量最终预测准确度的交叉熵损失，也就是图像的真伪。 $L _ { \mathrm { d e t } }$ 是用来指导ADM学习的检测损失，它包含置信度损失 $\left( L _ { \mathrm { c o n f } } \right)$ 和位置损失 $\left( L _ { \mathrm { l o c } } \right)$ 。 $L _ { \mathrm { c o n f } }$ 是衡量每个锚（即伪锚或真锚）的预测结果的二进制交叉熵损

失， $L _ { \mathrm { l o c } }$ 是一种平滑的L1损失[20]以测量ADM预测和伪影区域的基本真实之间的位置偏移。 $\boldsymbol { L } _ { \mathrm { d e t } }$ 的计算公式为

$$
L _ { \mathrm { d e t } } = \frac { 1 } { N } ( L _ { \mathrm { c o n f } } ( x , c ) + \alpha L _ { \mathrm { l o c } } ( x , l , g ) ) ~ ,
$$

式中： $N$ 为正锚框的个数，即伪锚； $\scriptstyle x \in \{ 0$ ，1}为默认锚与伪影区域基础真值匹配的指标； $\mathbf { \Psi } _ { c }$ 表示类置信度； $l$   
和 $\boldsymbol { g }$ 表示ADM预测框和伪影区域基础真值框； $\alpha$ 表示正权重。

# 3 实验结果及分析

# 3.1 实验参数配置

本研究使用Dlib库[21]中的函数对输入图像进行人脸区域定位，并通过5点法进行人脸裁剪，大小设置为 $2 2 4 \times 2 2 4$ ，以提高检测准确率。为了提高模型训练过程中的稳定性和对超参数的敏感度，训练中使用Ad⁃aMod[22]优化器，并动态调整学习率，初始学习率设置为0.001，学习率为每5 个epoch 衰减1 次，衰减率为$0 . 0 0 0 5$ 。本研究将Batchsize设置为32，共训练20个epoch。实验运行环境如表1所列。

如图 6 为不同初始学习率的测试准确率对比图，AdaMod 提高了优化器的收敛性，无需预热，并且降低了对学习率的敏感性。对比动量随机梯度下降（SGDM）、自适应矩估计优化器（Adam）[22] 、均方根传播优化器（RMSProp），AdaMod提高了模型在训练阶段的准确率。

# 3.2 数据集

本研究采用的2个大规模伪造人脸公开数据集分别为FaceForensics $^ { + + }$ （简称 ${ \mathrm { F F } } { + } { + }$ ）[17]和Celeb-DF[18]，其中， $\mathrm { F F } { + } { + }$ 数据集由1 000 个真实视频以及4 000 个伪造视频组成，对于真实视频的划分采用 $7 2 0 : 1 4 0 :$ 140的比例，720个视频用于训练，140个视频用于验证，140 个视频用于测试。本研究采用经H.264 编码技术压缩的高质量版本（c23）和低质量版本（c40）。由590个截取自YouTube的真实样本和由各种伪造技术产生的5 639 个样本共同构成了Celeb-DF，其中深度伪造视频由加强的DeepFakes 技术产生，伪造特点更加隐蔽，适合用于做跨数据集的测试。

# 3.3 评价指标

表1 运行环境Tab.1 Operating environment  

<html><body><table><tr><td>类别</td><td>配置</td></tr><tr><td>显卡</td><td>RTX 2080Ti</td></tr><tr><td>CPU</td><td>2.4 GHz Intel(R)</td></tr><tr><td>内存</td><td>62GB</td></tr><tr><td>操作系统</td><td>Ubuntu 18.04</td></tr><tr><td>CUDA版本</td><td>11.4</td></tr><tr><td>Python版本</td><td>Python3.8</td></tr></table></body></html>

![](images/a31d4de2cbd2c2567a1c41da6cc78a9b8fc824a45387938356c254d608d4ebc2.jpg)  
图 6 不同优化器的测试准确率Fig.6 Test accuracies for different optimizers

采用准确度分数 Acc （Accuracy）以及 ROC曲线下面积 AUC （Area Under Curve）作为本研究的评估指标。Acc的定义为

$$
\mathrm { A c c } = { \frac { \mathrm { T P } + \mathrm { T N } } { \mathrm { T P } + \mathrm { T N } + \mathrm { F P } + \mathrm { F N } } } \ ,
$$

式中：TP 为预测准确的真样本；TN 为预测准确的假样本；FP 为预测错误的假样本；FN 为预测错误的真样本。

AUC 可以充分表现分类器的性能。在坐标图中，假阳率是横坐标，代表原始视频误判为伪造视频；真阳率为纵坐标，代表伪造视频判断正确。

# 3.4 实验结果分析

3.4.1 在 ${ \mathrm { F F } } { + } { + }$ 上的视频帧检测性能分析

实验结果如表2所示。AdfNet 模型[23]提出了自适应计算噪声残差的方法学习图像特征，并通过分级时序网络放大了伪造视频的帧间差异，但其在泛化能力方面仍有待提升。TI2Net 模型[24]通过捕捉同一身份在视频帧中的面部差异来识别伪造视频，在高质量数据集上取得了较优的性能。本研究采用的方法通过精确标注指导多尺度默认锚点定位伪影区域，提升了检测准确率和分类效果。在检测准确率方面SIM模型[25]通

过对视频中局部不一致的部位进行编码，达到了最优的性能，但本研究方法与之相近，且在AUC 方面，本研究方法达到了 $9 9 . 8 4 \%$ ，进一步表明本研究采用的模型在高质量视频检测与分类方面具有较好的性能。

# 3.4.2 在 $\mathrm { F F + + }$ 上的视频检测性能分析

由表3 数据可知，FDFL 模型[31]提出自适应频率特征生成模块对伪造视频中的差异进行提取，但其忽略了空域特征，无法达到最优的检测效果和分类性能。CNN-ViT 模型[30]通过卷积神经网络和 Trans⁃former相结合的方式，提高了单卷积网络的检测准确率，但分类性能不足。本研究提出的方法，通过引入高效通道注意力模块，保留了完整的特征信息，增强了分类性能，且在检测准确率方面也到达了最优。

# 3.4.3 在 Celeb-DF 上的跨数据集评估

表4 数据通过在 $\operatorname { F F } + + \left( \operatorname { c } 2 3 \right)$ 上进行训练，然后在Celeb-DF 上测试获得，进而评估模型的泛化能力。本研究提出的方法在 $\mathrm { F F } + + \left( \mathrm { c } 2 3 \right)$ 上达到了$9 9 . 8 4 \%$ ，同时在 Celeb-DF 仍拥有 $78 . 4 5 \%$ 的AUC，说明该方法具有较优的泛化性能。SLADD 模型[34]在$\operatorname { F F } + + ( \operatorname { c } 2 3 )$ 上保持 $9 8 . 4 \%$ 准确率的同时，在Celeb-DF上AUC 比本研究方法高 $1 . 2 5 \%$ ，经分析得，SLADD的泛化能力取决于对抗性训练和自监督任务，如果自监督任务设计不当，就会使检测性能下降。而本研究提出的方法是半监督任务，通过伪影检测模块精准的检测并区分伪造特征，避免了自监督任务中的预训练设计失误，因此本研究所提方法仍具有较佳的泛化性能。

# 3.4.4 交叉验证

$\mathrm { F F } { + } { + }$ 数据集上4 种伪造方式的单独测试结果如表 5 所示。其中，MesoNet 和 Xception 在大多数情况下性能低于本研究所提方法，因为这两种方法过度关注图像全局特征，所以泛化能力表现不佳。而本研究所提方法充分关注了局部特征，避免了伪造特征的遗漏，使模型表现出优异的分类性能。

# 3.5 消融实验

# 3.5.1 模型改进策略对比

为详细验证改进策略在 $\mathrm { F F } { + } { + }$ 高质量数据集上的有效性，数据如表6 所示。采用递进添加模块的方法，加入ECA 模块后，检测精度略有提升；加入ECA 和ADM 模块后在检测精度和分类效果方面均有显著提升；当ResNet34 网络加上所有模块后，检测性能和分类性能达到最佳。

为了更加直观地看到改进后模型的性能增益，图 7对类激活热力图分别进行比较。图中颜色较深部分更加被模型关注。与骨干模型进行对比，在使用本研究所提方法后，模型将关注重点转移到了人脸局部区域上，且更加关注局部形状不规则的伪造区域，这使模型针对面部的局部区域的检测准确率大幅提升。

Tab.2 Performance of video frame detection on $\mathbf { F } \mathbf { F } { + } { + }$   

<html><body><table><tr><td colspan="4">dataset %</td></tr><tr><td rowspan="2">方法</td><td colspan="2">c23</td><td colspan="2">c40</td></tr><tr><td>Acc</td><td>AUC</td><td>Acc</td><td>AUC</td></tr><tr><td>MesoNet[26]</td><td>83.10</td><td>1</td><td>70.47</td><td>1</td></tr><tr><td>SIM[25]</td><td>98.93</td><td>1</td><td>96.78</td><td>1</td></tr><tr><td>RFM[27]</td><td>95.32</td><td>98.96</td><td>81.75</td><td>89.30</td></tr><tr><td>Xception[17]</td><td>1</td><td>92.30</td><td>1</td><td>83.93</td></tr><tr><td>AdfNet[23]</td><td>97.41</td><td>99.38</td><td>88.43</td><td>91.53</td></tr><tr><td>TI2Net[24]</td><td>97.93</td><td>99.51</td><td>92.89</td><td>95.31</td></tr><tr><td>SDT28]</td><td>97.97</td><td>99.76</td><td>74.62</td><td>79.26</td></tr><tr><td>HCIL[29]</td><td>97.92</td><td>98.32</td><td>1</td><td></td></tr><tr><td>CNN-ViT[30]</td><td>97.57</td><td>99.17</td><td>89.71</td><td>89.27</td></tr><tr><td>本文方法</td><td>97.88</td><td>99.84</td><td>92.67</td><td>96.81</td></tr></table></body></html>

表 2 $\mathbf { F F } { + } { + }$ 数据集视频帧检测性能  
表4 跨数据集性能 AUC 比较  

<html><body><table><tr><td>方法</td><td>Acc</td><td>AUC</td></tr><tr><td>Xception[17]</td><td>1</td><td>92.50</td></tr><tr><td>F3-Net[32]</td><td>98.95</td><td>99.30</td></tr><tr><td>FDFL[31]</td><td>96.69</td><td>99.30</td></tr><tr><td>CNN-ViT[30]</td><td>97.57</td><td>99.17</td></tr><tr><td>ABMT[33]</td><td>97.33</td><td>1</td></tr><tr><td>本文方法</td><td>99.03</td><td>99.36</td></tr></table></body></html>

Tab. 3 Video detection performance on $\mathbf { F } \mathbf { F } { + } { + }$ dataset %   
Tab.4 Cross-library performance AUC comparison %   
表 5 交叉验证性能（AUC）  

<html><body><table><tr><td>方法</td><td>FF++</td><td>Celeb-DF</td></tr><tr><td>Xception[17]</td><td>95.5</td><td>65.5</td></tr><tr><td>F3-Net[32]</td><td>98.1</td><td>65.2</td></tr><tr><td>MADD[12]</td><td>99.8</td><td>67.4</td></tr><tr><td>SLADD[34]</td><td>98.4</td><td>79.7</td></tr><tr><td>SPSL[35]</td><td>96.91</td><td>76.88</td></tr><tr><td>CNN-ViT[30]</td><td>99.17</td><td>67.81</td></tr><tr><td>TI2Net[24]</td><td>99.51</td><td>66.65</td></tr><tr><td>HCIL[29]</td><td>98.32</td><td>79.06</td></tr><tr><td>本文方法</td><td>99.84</td><td>78.45</td></tr></table></body></html>

表 3 $\mathbf { F } \mathbf { F } { + } { + }$ 数据集视频检测性能  
Tab.5 Cross-Validation Performance (AUC)   
%   

<html><body><table><tr><td rowspan="2">训练</td><td rowspan="2">方法</td><td colspan="4">测试</td></tr><tr><td>DF</td><td>F2F</td><td>FS</td><td>NT</td></tr><tr><td rowspan="3">DF</td><td>Xception</td><td>99.11</td><td>74.55</td><td>39.21</td><td>73.97</td></tr><tr><td>MesoNet</td><td>96.92</td><td>47.10</td><td>60.51</td><td>76.98</td></tr><tr><td>本文方法</td><td>99.30</td><td>74.65</td><td>66.32</td><td>82.48</td></tr><tr><td rowspan="3">F2F</td><td>Xception</td><td>82.21</td><td>99.12</td><td>55.86</td><td>67.32</td></tr><tr><td>MesoNet</td><td>73.68</td><td>97.58</td><td>52.32</td><td>65.39</td></tr><tr><td>本文方法</td><td>88.56</td><td>99.72</td><td>65.74</td><td>69.21</td></tr><tr><td rowspan="3">FS</td><td>Xception</td><td>59.23</td><td>65.76</td><td>99.01</td><td>55.68</td></tr><tr><td>MesoNet</td><td>52.37</td><td>61.85</td><td>98.12</td><td>52.64</td></tr><tr><td>本文方法</td><td>65.21</td><td>73.41</td><td>99.67</td><td>60.43</td></tr><tr><td rowspan="3">NT</td><td>Xception</td><td>85.65</td><td>67.32</td><td>50.83</td><td>97.19</td></tr><tr><td>MesoNet</td><td>88.54</td><td>63.92</td><td>52.30</td><td>85.73</td></tr><tr><td>本文方法</td><td>95.72</td><td>73.96</td><td>51.44</td><td>98.76</td></tr></table></body></html>

# 3.5.2 帧数影响评估

多种帧数选择策略如图8所示，当视频帧数超过300 帧时，选择合适的帧数能够使准确率提升。选择过少的帧会导致训练样本的不均衡和不完整性，这可能会使模型训练不足；相反，如果选择过多的帧，可能会造成样本之间的冗余，从而使模型过度拟合。由图8可知，当选取35帧作时，模型的检测准确率达到峰值，随后收敛。

# 3.5.3 分类效果

图9 为在 $\mathrm { F F } + + \left( \mathrm { c } 2 3 \right)$ 上测试的ROC 曲线图。由图9中数据可知，本研究所提方法比ResNet34在AUC值方面从0.954 4 提升到了0.998 4，表明该方法在真假人脸方面具有更好的分类效果，能够使预测样本与实际样本之间的误差更小。

# 4 结论

深度伪造技术的出现使得欺诈等犯罪行为更加方便，如何防止人们被高质量伪造视频所欺骗，成为当下亟待解决的问题。为解决上述问题，在 $\mathrm { R e s N e t } 3 4$ 模型的基础上引入了高效通道注意力模块，有效保留了图像特征的完整性，并设计伪影检测模块和多尺度人

表6 消融实验性能  
Tab.6 Performance of ablation experiments %   

<html><body><table><tr><td colspan="3">模型</td><td rowspan="2">Acc</td><td rowspan="2">AUC</td></tr><tr><td>ResNet34</td><td>ECA</td><td>ADM MFS</td></tr><tr><td>√</td><td></td><td></td><td>94.36</td><td>95.44</td></tr><tr><td>√</td><td>√</td><td></td><td>94.74</td><td>96.68</td></tr><tr><td>√</td><td>√</td><td>√</td><td>96.32</td><td>97.73</td></tr><tr><td>√</td><td>√</td><td>√ √</td><td>97.88</td><td>99.84</td></tr></table></body></html>

![](images/d93961df68672384abc18ebae47ce52aa26a6377b61e572181d49dad19858a3f.jpg)  
图 7 改进策略 CAM 热力图Fig.7 CAM heat map of improvement strategy

脸交换模块，减弱对图像全局特征的关注，减少了伪造特征遗漏的影响。实验结果表明，该研究所提方法在 ${ \mathrm { F F } } { + } { + }$ 数据集内和不同数据集间具有优异的检测性能，且跨数据集时体现了较佳的泛化能力。该方法在$\operatorname { F F } + + \left( \operatorname { c } 2 3 \right)$ 数据集上检测准确率为 $9 7 . 8 8 \%$ ，AUC值为 $9 9 . 8 4 \%$ 。该项研究结果必将为减少诈骗犯罪行为和维护社会稳定贡献力量。

![](images/bca7e41d9eb230db3137ca998189edb67e18d9f4e67f8535b49e34f0a67c56a5.jpg)  
图 8 帧数对准确率的影响折线图Fig.8 The effect of frame count on accuracy line chart  
图 9 在 $\mathrm { F F ^ { + + } } .$ 上测试的 ROC 图Fig.9 ROC images tested on $\mathrm { F F ^ { + + } }$

1.0 ResNet34（area = 0.954 4） 0.8 ResNet34+DA （area = 0.966 8） 0.4 ResNet34+DA+ADM （area = 0.987 6） 0.2 ResNet34+DA+ADM+ECA （area = 0.998 4） 0 0 0.2 0.4 0.6 0.8 1.0 假阳率

# 参考文献：

[1] 卓文琦，李东泽，王伟，等. 面向轻量级深度伪造检测的无数据模型压缩[J]. 中国图象图形学报，2023，28（3）：820-835.[2] DU M N，PENTYALA S，LI Y N，et al. Towards generalizable deepfake detection with locality-aware AutoEncoder[EB/OL]. arXiv：1909.05999[2024-06-22]. https：//arxiv. org/abs/1909. 05999v2.

[4] GOODFELLOW I，POUGET-ABADIE J，MIRZA M，et al. Generative adversarial networks[J]. Communications of the ACM，2020，63（11）：139-144.   
[5] 张溢文，蔡满春，陈咏豪，等. 融合空间特征的多尺度深度伪造检测方法[J]. 计算机工程，2024，50（7）：240-250.   
[6] GUO H，HU S， ural network for detecting GAN-generated faces[J]. IEEE Access，2022，10：32574-32583.   
[7] DONG X Y，BAO J celebrities from DeepFake with identity consistency transformer[C]// 2022 IEEE/CVF Confer⁃ ence on Co tion（CVPR）. New Orleans. IEEE，2022：9468-9478.   
[8] 孙炜晨，田青，罗 视频深度伪造检测技术及应用[J]. 警察技术，2023（1）：10-16.   
[9] SHIOHARA K， Detecting deepfakes with self-blended images[C]// 2022 IEEE/CVF Conference on Computer Vision and Pattern Rec⁃ ognition（CVPR）. New Orleans. IEEE，2022：18699-18708.   
[10] 俞洋，袁家斌，蔡纪元，等. 基于非关键掩码和注意力机制的深度伪造人脸篡改视频检测方法[J]. 计算机科学，2023，50（11）：160-167.   
[11] GUERA D，DEL E J. Deepfake video detection using recurrent neural networks[C]// 2018 15th IEEE International Conference on Advanced Video and Signal Based Surveillance（AVSS）. Auckland. IEEE，2018：1-6.   
[12] ZHAO H Q，WEI T Y，ZHOU W B，et al. Multi-attentional deepfake detection[C]// 2021 IEEE/CVF Conference on Computer Vision and Pattern Rec⁃ ognition（CVPR）. Nashville. IEEE，2021：2185-2194.   
[13] MASI I，KILLEKAR A，MASCARENHAS R M，et al. Two-branch recurrent network for isolating deepfakes in videos[C]//Lecture Notes in Computer Science. Cham：Springer International Publishing，2020：667-684.   
[14] ZHENG Y L，BAO J M，CHEN D，et al. Exploring temporal coherence for more general video face forgery detection[C]// 2021 IEEE/CVF Internation⁃ al Conference on Computer Vision（ICCV）. Montreal. IEEE，2021：15024-15034.   
[15] HE K M，ZHANG X Y，REN S Q，et al. Deep residual learning for image recognition[C]// 2016 IEEE Conference on Computer Vision and Pattern Recognition（CVPR）. Las Vegas. IEEE，2016：770-778.   
[16] HU J，SHEN L，ALBANIE S，et al. Squeeze-and-excitation networks[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence，2020，42 （8）：2011-2023.   
[17] ROSSLER A，COZZOLINO D，VERDOLIVA L，et al. FaceForensics++：learning to detect manipulated facial images[C]// 2019 IEEE/CVF Interna⁃ tional Conference on Computer Vision（ICCV）. Seoul. IEEE，2019：1-11.   
[18] LI Y Z，YANG X，SUN P，et al. Celeb-DF：a large-scale challenging dataset for DeepFake forensics[C]// 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition（CVPR）. Seattle. IEEE，2020：3207-3216.   
[19] LI L Z，BAO J M，ZHANG T，et al. Face X-ray for more general face forgery detection[C]// 2020 IEEE/CVF Conference on Computer Vision and Pat⁃ tern Recognition（CVPR）. Seattle. IEEE，2020：5001-5010.   
[20] GIRSHICK R. Fast R-CNN[C]// 2015 IEEE International Conference on Computer Vision（ICCV）. Santiago. IEEE，2015：1440-1448.   
[21] WANG J K，WU Z X，OUYANG W H，et al. M2TR：multi-modal multi-scale transformers for deepfake detection[EB/OL]. arXiv：2104.09770[2024- 06-22]. https：//arxiv. org/abs/2104. 09770v3.   
[22] DING J，REN X，LUO R，et al. An adaptive and momental bound method for stochastic learning[EB/OL]. arXiv：1910.12249[2024-06-22]. https：// arxiv.org/abs/1910.12249.   
[23] 李家春，李博文，林伟伟. AdfNet：一种基于多样化特征的自适应深度伪造检测网络[J]. 华南理工大学学报（自然科学版），2023，51（9）：82- 89.   
[24] LIU B P，LIU B，DING M，et al. TI2Net：temporal identity inconsistency network for deepfake detection[C]// 2023 IEEE/CVF Winter Conference on Applications of Computer Vision（WACV）. Waikoloa. IEEE，2023：4680-4689.   
[25] GU Z H，CHEN Y，YAO T P，et al. Delving into the local：dynamic inconsistency learning for DeepFake video detection[J]. Proceedings of the AAAI Conference on Artificial Intelligence，2022，36（1）：744-752.   
[26] AFCHAR D，NOZICK V，YAMAGISHI J，et al. MesoNet：a compact facial video forgery detection network[C]// 2018 IEEE International Workshop on Information Forensics and Security（WIFS）. Hong Kong. IEEE，2018：1-7.   
[27] WANG C R，DENG W H. Representative forgery mining for fake face detection[C]// 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition（CVPR）. Nashville. IEEE，2021：14923-14932.   
[28] ZHANG D C，LIN F Z，HUA Y Y，et al. Deepfake video detection with spatiotemporal dropout transformer[C]// Proceedings of the 30th ACM Interna⁃ tional Conference on Multimedia. Lisboa. ACM，2022：5833-5841.   
[29] GU Z H，YAO T P，CHEN Y，et al. Hierarchical contrastive inconsistency learning for Deepfake video detection[C]//Lecture Notes in Computer Sci⁃ ence. Cham：Springer Nature Switzerland，2022：596-613.   
[30] 李颖，边山，王春桃，等. CNN结合Transformer的深度伪造高效检测[J]. 中国图象图形学报，2023，28（3）：804-819.   
[31] LI J M，XIE H T，LI J H，et al. Frequency-aware discriminative feature learning supervised by single-center loss for face forgery detection[C]// 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition（CVPR）. Nashville. IEEE，2021：6454-6463.   
[32] QIAN Y Y，YIN G J，SHENG L，et al. Thinking in frequency：face forgery detection by mining frequency-aware clues[C]//Lecture Notes in Computer Science. Cham：Springer International Publishing，2020：86-103.   
[33] WASEEM S，ABU-BAKAR S A R S，OMAR Z，et al. Multi-attention-based approach for deepfake face and expression swap detection and localiza⁃ tion[J]. EURASIP Journal on Image and Video Processing，2023，2023（1）：14.   
[34] CHEN L，ZHANG Y，SONG Y B，et al. Self-supervised learning of adversarial example：towards good generalizations for deepfake detection[C]// 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition（CVPR）. New Orleans. IEEE，2022：18689-18698.   
[35] LIU H G，LI X D，ZHOU W B，et al. Spatial-phase shallow learning：rethinking face forgery detection in frequency domain[C]// 2021 IEEE/CVF Con⁃ ference on Computer Vision and Pattern Recognition（CVPR）. Nashville. IEEE，2021：772-781.