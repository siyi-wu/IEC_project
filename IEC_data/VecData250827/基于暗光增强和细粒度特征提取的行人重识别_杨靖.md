# 基于暗光增强和细粒度特征提取的行人重识别

杨　 靖，　 谷灵康∗，　 夏周祥安徽工程大学 计算机与信息学院， 安徽 芜湖 241000

摘　 要： 针对现实环境中由于光照变化和色差等因素导致的行人重识别准确率低的问题，设计了基于暗光增强和细粒度特征提取的行人重识别模型。 首先利用暗光增强模块对输入的行人图像进行图像增强，优化图像信息，之后将图像输入改进后的 Vision Transformer 模型，该模型引入条件位置编码和拼图补丁促使网络关注行人图像的细粒度特征信息，增强对行人特征的提取能力。 最后设计出 DAD⁃T 模块减少模型复杂度，缓解性能下降的问题。 以此在性能无损的情况下提高训练效率，加快训练速度。 在 DukeMTMC⁃ReID、MSMT17 和 Market⁃1501 三个行人重识别公开数据集上进行实验，并与主流网络模型进行比较。 结果表明，所提模型的Rank⁃1 与 mAP 指标相比当前主流模型有所提升，具有较高的识别准确率。

关键词： 暗光增强；细粒度；Vision Transformer 模型；行人重识别中图分类号： TP391． 41 文献标识码： A 文章编号： 2096⁃3998（2025）02⁃0081⁃10

行人重识别（person re⁃identification，ReID）是一种利用计算机视觉来判定图像或视频中是否存在特定行人的技术。 近年来随着监控设施的不断完善，行人重识别被广泛应用于医院、车站、学校、商场等公共场所［1］。 随着深度学习的发展，行人重识别受到研究人员的广泛关注和研究，取得了前所未有的成果，基于深度学习的行人重识别已经替代了早期基于手工设计特征的识别系统，但其仍然面临着诸多挑战，例如：行人姿态多变、光照变化、换衣、遮挡、摄像头分辨率低等。

Sun Yifan 等［2］提出 PCB 模型，通过对特征图水平分割，提取局部区域特征；Park 等［3］在特征图水平分割的基础上，又建立了局部与局部的关系，加强了特征之间的联系；Li Yuanyuan 等［4］提出多尺度上下文感知网络（MSCAN），针对提取网络的变换参数设计了三个新的约束，缓解了行人图像采样错位的问题；Islam 等［5］针对极少有行人会长时间穿着同一件衣服的现象，提出基于长时间换装的行人重识别方法；Zhang Zhizheng 等［6］提出了一个高效的关系感知的全局注意力模块 RGA 来捕获行人图像的全局结构信息以更好地进行特征提取。

此外，在 2017 年，Vaswani 等［7］提出的 Transformer 模型首先应用于自然语言处理，因为其优异的性能和简单的模型结构迅速引起众多学者关注；随后 Dosovitskiy 等［8］ 提出了 ViT（ Vision Transformer） 模型，将 Transformer 引入了 CV 图像领域，为计算机视觉领域带来新的思路和方法；He Shuting 等［9］首次将 Transformer 用于行人重识别和车辆重识别并提出拼图补丁模块（Jigsaw Patch Module，JPM）和侧信息嵌入（Side Information Embeddings，SIE）取得了较好的性能；Zhu Kuan 等［10］ 提出了 AAFormer，结合 ViT的主干网络和额外的局部向量来表征局部信息；Luo Hao 等［11］提出 BNNeck，可将交叉熵损失和三元组损失更好地联合使用于行人重识别。 Transformer 方法在各个视觉任务上表现出广阔的前景，有着逐渐替代卷积神经网络的趋势，但纯 Transformer 网络缺少归纳偏差且计算复杂度高，对硬件有一定要求，训练过程缓慢。

针对以上问题，考虑到现实场景下不同光照会使行人图像出现颜色、纹理等误差，本文提出一种基于暗光增强和细粒度特征提取的行人重识别方法。 首先使用暗光增强模块 IceNet［12］对部分行人图像中的暗光区域增亮，这种图像增强方法不仅能在多种场景下表现优异，而且可以根据使用者需求，自适应生成增强后的行人图像；之后将新型的位置编码：条件位置编码 （ Conditional Positional Encoding，CPE）［13］和拼图补丁（JPM）同时嵌入 Transformer 中，提取更加鲁棒的行人特征；最后受文献［14⁃15］启发，设计出 DAD⁃T（DropKey And Dynamic data pruning in Transformer，DAD⁃T）模块对模型进行轻量化改造，缓解模型性能下降、过拟合的风险，减少复杂度，增强网络的表达能力。

# 1 基本原理

# 1． 1 常规的 ViT 行人重识别

基于卷积神经网络（Convolutional Neural Networks，CNN）的方法难以利用全局范围内的行人信息，往往局限于小的局部区域［16］。 注意力机制关注重要特征同时忽视次要特征，虽然将注意力机制引入CNN 网络，给行人部件分配权重，可以缓解远程依赖关系［17］，但是注意力机制大多嵌入在深层，没有从根本上解决原理问题。 尤其是，行人重识别属于复杂场景下的细粒度任务，只靠单纯的 CNN 方法已经很难有大的突破，需要设计特定的损失函数和修改网络结构。 最近，一些用 ViT 替代 CNN 的模型开始流行，ViT 在图像处理领域取得了优异的性能。

一般情景下，ViT 模型用于行人重识别时的网络框架图如图1 所示，输入行人图像为 $\boldsymbol { x } _ { i } \in \mathbf { R } ^ { H \times W \times C }$ ， $x _ { i }$ 表示一个批量输入的第 $i$ 张行人图像， $H , W$ 和 $C$ 分别表示图像的高度、宽度和通道数，首先将其切分为$N$ 个大小相等的图像块 $\{ x _ { p } ^ { i } \mid i = 1 , 2 , \cdots , N \}$ ，将图像块输入 Embedding 层，线性映射成 $D$ 维向量，即每个图像块对应得到一个 $D$ 维向量。 值得一提的是，这一步操作主要通过卷积实现。

![](images/45bed43ec7d9e16d878d58c98a0d4899923b68a62f2a75d22070a60740f6e3d4.jpg)  
图1　 基于 ViT 的行人重识别网络框架图

此外，ViT 在向量之前添加一个随机初始化可学习的分类标记（［cls］）。 输出的［cls］标记用作全局特征表示 $f _ { i }$ ，在 $f$ 之后使用 BNNeck 联合交叉熵损失和三元组损失。

输入到 Transformer 层的输入序列可以表示为

$$
Z _ { 0 } = \left( x _ { \mathrm { c l s } } , F ( x _ { p } ^ { 1 } ) , F ( x _ { p } ^ { 2 } ) , \cdots , F ( x _ { p } ^ { N } ) \right) + P ,
$$

式中， $Z _ { \ L _ { 0 } }$ 表示输入序列嵌入， $P \in \mathbf { R } ^ { ( N + 1 ) \times D }$ 是位置嵌入， $F$ 是将切割后的图像映射到 $D$ 维的线性投影。

# 1． 2 改进的 ViT 行人重识别

以上基于 ViT 行人重识别模型虽然取得较好的成果，但如何让网络更关注包含行人特征的图像块，抑制现实场景下无用的背景信息，缓解因光照、遮挡等导致的多种负面问题，本文设计了基于暗光增强和改进 ViT 的行人重识别模型，如图2 所示，进一步提高行人重识别的准确性和识别鲁棒性。

![](images/d81edea08cb432f9662d4cdb95a3ca9e660abaae02c3f6ed641e52c3e86ad62e.jpg)  
图 2 网络架构

输入图像首先经过 IceNet 网络进行图像增强，使行人图像更加清晰，色彩更加明亮，之后用滑动窗口，对其进行分块处理，在第一个 Encoder 之后插入动态可生成的条件位置编码（CPE），由此泛化到更长的输入序列，可以和 CNN 的方法一样，拥有平移不变性，增加识别精度。 同时由于行人重识别属于复杂场景下的细粒度任务，行人切片之间联系密切，每个局部片段如果仅考虑连续补丁嵌入的一部分，会造成资源浪费，由此使用拼图补丁模块（JPM），引入额外的扰动，也有助于提高目标 ReID 模型的鲁棒性。 此外将 ViT 的最后一层改为 2 个独立的分支 Global Branch 和 Jigsaw Branch，以此来学习全局特征和局部特征。

# 1． 2． 1 暗光增强模块

光照、色度变化导致行人图像外观差异很大，难以完整地表达行人外观特征，这类图像对比度低，能见度差，严重干扰基于视觉的下游任务。 暗光增强作为一个图像处理问题已经被研究了很长的时间。部分网络模型为了降低不同光照对模型的影响，使用灰色图像代替原始图像，但这类方法损失了图像的色彩信息。 一些传统的低光照图像方法例如直方图均衡化通过灰度变换将一幅图像转换为另一幅均衡直方图，这类方法处理后的图像不仅灰度级减少，局部细节缺失，并且图像对比度变得异常增高。

本文在不改变原始图像结构信息的前提下，为了解决这些问题，引入暗光增强模块 IceNet 对输入图像进行处理，能够有效应对光照变化对模型的影响，提升模型适应视域变化的能力。 该模块是一种基于 CNN 的交互式图像增强算法，可以根据使用者的要求自适应地生成增强图像，也可以自动生成无需交互的图像。 IceNet 非常灵活，框架图如图3 所示，适用于增强来自不同领域的真实低光图像。 有益于加速网格模型的收敛，防止梯度消失，明显优于传统算法。 图中 $\eta \in \left[ 0 , 1 \right]$ 表示曝光水平， $I$ 表示行人图像，S 表示涂鸦图（涂鸦可用于输入图像的局部区域的变暗和变亮）， $Y$ 表示亮度分量， $T$ 表示伽马图，通

过颜色修复生成增强图像 $J _ { \circ }$

![](images/26f28cbbaaac52630c18b0f8f3d1e6b40682a67427b68529a4def79a9c08935a.jpg)  
图3　 暗光增强流程图

增强效果如图4 所示，经过处理的行人图像更加清晰，色彩更加明亮。

![](images/6fb0255485ea48cdbb20f52541ed09f7f715afcea35f4de2845751567ba87589.jpg)  
图4　 暗光增强效果图

# 1． 2． 2 条件位置编码

Transformer 中位置编码已经是被证明有效的，但会降低网络的灵活性，在 Transformer 中，没有对位置编码做特殊的设计，只采用一组可学习的参数来学习位置编码，该方法难以处理高分辨率图像。

Transformer 编码位置表示的方法主要有两种形式：绝对位置编码和相对位置编码。 使用绝对位置编码的 ViT 灵活性和泛化性不佳，它将输入 token 的绝对位置从1 编码到最大序列长度。 也就是说，每个位置都有一个单独的编码向量。 然后将编码向量与输入 token 组合，使得模型能够知道每个 token 的位置信息，绝对位置编码会使得网络模型难以解决图像处理中的平移不变性。 相对位置编码对输入 to⁃ken 之间的相对距离进行编码，从而来学习 token 的相对关系。 相对位置编码会增加时间复杂度，加大模型训练难度。

本文采取一种条件位置编码，让 ViT 的输入更灵活。 受 CNN 卷积的启发，在 ViT 的基础上，融合CNN 平移不变性的优点，能更加适合处理较长的输入序列，提高行人重识别分类精度。 该方法由一个简单的位置编码生成器（Position Encoding Generator，PEG）实现，可以便捷地插入 ViT 模型中，本文在第一个 Encoder 之后插入 PEG，两类位置编码的插入位置对比如图5 所示。

PEG 处理过程如图6 所示。 图中 PEG 的处理过程是将模型输入序列 $X \in \mathbf { R } ^ { B \times N \times C }$ 重塑为二维图像形状 $X ^ { \prime } \in \mathbf { R } ^ { B \times H \times W \times C }$ ，其中 $B$ 表示每批处理的图像数量， $N$ 表示 token 数也就是将行人图像切分成的块数，每个块数的尺寸为 $d \times d , C$ 表示图像的通道数， $H$ 和 $W$ 表示行人图像的高度和宽度。 再通过函数 $F$ 重复应用于局部块上，由此产生需要的条件位置编码， $F$ 函数由内核为 $k ( k \geqslant 3 )$ 和 $( k - 1 ) / 2$ 零填充的

二维卷积有效地实现。

![](images/904d54a53319b31a77dbcd23b2703644f9bcac9ec681d2a437766ce99016824f.jpg)  
图5　 位置编码的插入位置对比图

![](images/6b8d44fc5e549be57702c81df8bc37719f857b0b9e8efe5c4f0193474706dc9c.jpg)  
图 6 PEG 示意图

# 1． 2． 3 拼图补丁模块

考虑到 ViT 直接硬分割会分离出一些高度相关的区域，使用一种拼图补丁模块（JPM），该模块能更加关注行人图像显著特征，在全局范围内探索不同局部块之间的关系，突出重要的块，抑制无用的块，进一步挖掘图片的外观信息，这些信息包含了多粒度特征和更优秀的语义信息。 JPM 模块通过“移位”和“洗牌”操作对嵌入的 patch 进行重新排列，之后将它们组合成不同的部分，每个部分包含整个图像的几个随机补丁嵌入，进入共享的 Transformer 层，以端到端的方式进行训练，能够较好地缓解 ViT 中简单的tokens 化方案所导致的背景干扰和应对数据集中行人图像错位情况的问题，将 ViT 的最后一层改为 2个独立的分支 Global Branch 和 Jigsaw Branch，以此来学习全局特征和局部特征。

移位操作：假设输入到最后一层的隐藏特征表示为 $Z _ { l - 1 } = \big ( z _ { l - 1 } ^ { 0 } , z _ { l - 1 } ^ { 1 } , z _ { l - 1 } ^ { 2 } , \cdots , z _ { l - 1 } ^ { N } \big )$ ，将前 $\mathbf { \nabla } _ { m }$ 个 patch（［cls］标记除外）移到末尾，也就是 ${ ( z _ { l - 1 } ^ { 1 } , z _ { l - 1 } ^ { 2 } , \cdots , z _ { l - 1 } ^ { N } ) }$ 以 $\mathbf { \nabla } _ { m }$ 步移动变为 $( z _ { l - 1 } ^ { m + 1 } , z _ { l - 1 } ^ { m + 2 } , \cdots , z _ { l - 1 } ^ { N } , z _ { l - 1 } ^ { 1 } , z _ { l - 1 } ^ { 2 }$ ，$\cdots , z _ { l - 1 } ^ { m } \rangle$ ）。

洗牌操作：移位的 patch 通过 $k$ 组的 patch 混洗操作进一步混洗，局部特征 $f _ { 1 } ^ { k }$ 可以覆盖不同部位的patch，JPM 通过共享 Transformer 层将其编码为 $k$ 个局部特征 $f _ { 1 } ^ { 1 } , f _ { 1 } ^ { 2 } , \cdots , f _ { 1 } ^ { k }$ ，最终利用 $k$ 个局部特征和全局特征 $f _ { g }$ 训练模型。 联合使用交叉熵损失和加权正则化三元组损失［14］对网络中全局特征 $f _ { g }$ 和 $k$ 个局部特征进行监督训练。 交叉熵损失表示为

$$
{ \cal L } _ { \mathrm { c l s } } \ : = \ : - \ : \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \ : \sum _ { k = 1 } ^ { K } y _ { i , k } \mathrm { l n } ( \boldsymbol { p } _ { i , k } ) \ : _ { \circ }
$$

加权正则化三元组损失继承了三元组损失优化正负样本之间距离的优点，避免了引入范围参数，表示为

$$
L _ { \mathrm { w r t } } ( i ) \ = \ \ln \big ( 1 \ + \ \exp \big ( \sum _ { j } w _ { i j } ^ { p } d _ { i j } ^ { p } \ - \ \sum _ { k } w _ { i k } ^ { n } d _ { i k } ^ { n } \big ) \big ) ,
$$

其中

$$
\begin{array} { r l r } { \displaystyle } & { { } } & { \displaystyle { w _ { i j } ^ { p } } ~ = ~ \frac { \exp \big ( ~ d _ { i j } ^ { p } \big ) } { \sum _ { d _ { i j } ^ { p } \in { \cal P } _ { i } } \exp \big ( ~ d _ { i j } ^ { p } \big ) } , } \\ { \displaystyle } & { { } } & { \displaystyle { w _ { i k } ^ { n } } ~ = ~ \frac { \exp \big ( ~ - ~ d _ { i k } ^ { n } \big ) } { \sum _ { d _ { i k } ^ { n } \in { \cal N } _ { i } } \exp \big ( ~ - ~ d _ { i k } ^ { p } \big ) } \circ } \end{array}
$$

综上所述，本文最终损失函数为

$$
L = L _ { \mathrm { c l s } } f ( g ) + L _ { \mathrm { w r t } } f ( g ) + \frac { 1 } { k } \sum _ { j = 1 } ^ { k } \left( L _ { \mathrm { c l s } } ( f _ { l } ^ { j } ) + L _ { \mathrm { w r t } } ( f _ { l } ^ { j } ) \right) _ { \mathrm { o } }
$$

在处理过程中，将全局特征和局部特征 $( f _ { g } , f _ { l } ^ { 1 } , f _ { l } ^ { 2 } , \cdots , f _ { l } ^ { k } )$ 作为最后的特征表示，更加关注身体局部的细粒度信息特征，并且拥有全局判别能力。

# 1． 2． 4 轻量化模块

ViT 自从被提出以来，因其优秀的性能被广泛用做许多视觉任务默认的 backbone，凭借着 ViT 独特的结构，许多视觉任务的 SoTA 都得到了进一步提升，这得益于 ViT 的注意力模块，该模块存在一个Softmax 操作，Softmax 用于在 token 上产生概率分布，这是 Transformer 非常重要的操作之一，因为 Soft⁃max 操作涉及到内部计算所有输入的指数之和，所以它的计算代价相当昂贵。 Wortsman 等［18］提出将自注意力机制中的 softmax 操作替换为 ReLu 除以序列长度，缩短了模型运行时间；Han Dongchen 等［19］提出一种新的线性注意力模块，在保持模型表达能力的前提下降低了复杂度。

考虑到 ViT 对行人重识别这样的复杂场景鲁棒性不佳且训练的计算量比 CNN 要庞大。 为了提高ViT 的运行效率，缓解性能下降和模型过拟合等问题，本文设计出 DAD⁃T 轻量化模块缓解网络模型的负面影响。 DAD⁃T 模块分为两部分，包括在模型中使用正则化方法 DropKey 和动态剪枝操作。

DropKey 操作目的是通过随机 drop 部分 key 的方式来自适应地调整 ViT 中的注意力权重，使其变得更加平滑，可以当作 Dropout 操作的改进［14］ 。 DropKey 分为 DropKey⁃Cross 和 DropKey⁃Block 两种形式，与 Dropout 的对比如图 7 所示，其中， $Q , K , V$ 分别表示查询、键、值。

![](images/a43b16af63fcef71da4be13991f6fa1e96d670eec7e3a18d49a9d3bba3a8cb30.jpg)  
图7　 对比图

一般的 Dropout 是完全随机的 drop，DropKey⁃Cross 是以一点为中心的十字形连续区域进行 drop，DropKey⁃Block 是以一点为中心的矩形区域进行 drop。 本文选取 DropKey⁃Cross 方法进行实验。

此外，为了进一步加速网络的收敛速度，缓解因为数据增强（填充、翻转、随机擦除等）、标签的处理（label smoothing 和 noise label）等技巧的混合使用而导致的内存占用、参数、运算次数、推理时间和功耗的增加等问题，考虑到当前的模型并不能在复杂的训练范式下以较少的更新次数实现对模型的快速训练，这进一步加剧了模型复杂度，使用动态剪枝采样的方法，维护梯度更新期望［15］

在网络训练过程时，参数、内存占用等问题会导致资源利用率增加，为了不产生冗余计算和多余开销，首先记录样本的损失值（loss）来作为样本分数，初始化默认保留首个 epoch 所有样本，之后对其余epoch 按照剪枝概率 $\boldsymbol { r }$ 来随机对分数小于平均值的样本进行剪枝（保留概率为1 －r）。 为了防止残余的梯度期望值偏差对结果的影响，在最后几个轮次中训练完整的数据来维持性能。

设置剪枝概率的表达式为

$$
P _ { \mathrm { \Lambda } _ { t } } ( z ) = \left\{ { r , \atop 0 } \right. \ { \cal H } _ { t } ( z ) < \overline { { { \cal H } } } _ { t } ,
$$

式中， $H _ { t } ( z )$ 和 $\overline { { H } } _ { t }$ 分别表示样本 $z$ 在上一轮中的分数和上一轮分数的均值。 当 $H _ { t } \left( z \right) < \overline { { H } } _ { t }$ 也就是分数小于均值并留下参与训练的样本，采用了重缩放（rescaling），将其梯度变为 $1 / ( 1 - r )$ 。 这使得整体更新是接近于无偏的。 记原始数据集为 ${ \mathbf { } } D , { \mathbf { } } t$ 时刻剪枝后的数据集为 $\boldsymbol { S } _ { t }$ ，证明推导过程为

$$
\underset { \theta \in \Theta } { \mathrm { a r g m i n } } E _ { z \in D } \big [ L ( z , \theta ) \big ] \ = \ \int _ { z } L ( z , \theta ) \rho ( z ) \mathrm { d } z ,
$$

经过剪枝并重缩放的操作之后，每个样本的采样率变成 $( 1 - P _ { t } \left( z \right) ) \rho ( z )$ ，缩放系数为 $\gamma _ { { { t } } } ( z ) = 1 / ( 1 -$ $\boldsymbol { P } _ { t } \left( z \right)$ ），上式变为

$$
\underset { \delta \in \Theta } { \mathrm { a r g m i n } } \underline { { E } } _ { z \in S _ { t } } \big [ \gamma _ { t } ( z ) L ( z , \theta ) \big ] ~ = ~ \underset { \delta \in \Theta } { \mathrm { a r g m i n } } \frac { 1 } { c _ { t } } \bigg [ \underline { { L } } ( z , \theta ) \rho ( z ) \mathrm { d } z _ { \circ }
$$

剪枝并重缩放后的优化目标和原始的优化目标有相同的解，因为给定时刻的 $1 / c _ { t }$ 是一个常数系数。 其中：

$$
E \Big [ \frac { 1 } { c _ { \iota } } \Big ] = \frac { \vert { D } \vert } { \sum _ { z \in { D } } \big ( 1 - P _ { \iota } ( z ) \big ) } \approx \frac { \vert { D } \vert } { \vert { S _ { \iota } } \vert } \Rightarrow E \big [ \nabla _ { \theta } L ( S _ { \iota } ) \big ] \approx \frac { \vert { D } \vert } { \vert { S _ { \iota } } \vert } E \big [ \nabla _ { \theta } L ( D ) \big ] ,
$$

剪枝后的更新步数变为了原先的 $\frac { \mid S _ { t } \mid } { \mid D \mid }$ ， 步长变为了原先的 $\frac { | D | } { | S _ { t } | }$ 因此概率剪枝加重缩放的策略维护了，  
更新量的总体基本一致［15］。

# 2 实验与分析

# 2．1　 数据集介绍和评估标准

为验证本文方法的有效性，在公开的 3 个行人重识别数据集 DukeMTMC⁃ReID［20］、MSMT17［21］ 和Market⁃1501［22］上进行了验证。 这3 个数据集包含室内和室外场景，行人姿态变化多样且具有明显的光照差异，具体信息见表1。

表1　 数据集具体信息  

<html><body><table><tr><td>数据集</td><td>行人数量</td><td>图像数量</td><td>多镜头</td><td>摄像头数</td><td>标记方式</td></tr><tr><td>DukeMTMC-ReID</td><td>1404</td><td>36 441</td><td>是</td><td>8</td><td>手动</td></tr><tr><td>MSMT17</td><td>4101</td><td>126 441</td><td>是</td><td>15</td><td>自动</td></tr><tr><td>Market-1501</td><td>1 501</td><td>32 668</td><td>是</td><td>6</td><td>混合</td></tr></table></body></html>

实验采用累积匹配特性（cumulative matching characteristic，CMC）中的 Rank $\mathbf { \nabla } \cdot k$ 和平均精度（mean av⁃erage precision，mAP）来验证本文所提方法。 Rank⁃1 作为评价指标，即第1 张图片与查询图像属于同一个。 mAP 表示平均精度均值是将平均精度（AP）求和再取平均。 $A P$ 和 $m A P$ 的计算公式为

$$
A P = \int _ { 0 } ^ { 1 } P ( R ) { \mathrm d } R ,
$$

$$
m A P \ = { \frac { \displaystyle \sum _ { i = 1 } ^ { c } A P _ { i } } { C } } _ { C } { } ~ _ { \circ }
$$

# 2． 2 对比实验

本节提出的方法在3 个主流公开数据集上分别与近年来的主流方法进行了对比（见表2）。 结果显示在这些数据集上 Rank⁃1 值和 $\mathrm { m A P }$ 值都取得了比较理想的结果。

表2　 不同先进方法在数据集上结果对比  
％   

<html><body><table><tr><td rowspan="2">方法</td><td colspan="2">DukeMTMC-ReID</td><td colspan="2">MSMT17</td><td colspan="2">Market-1501</td></tr><tr><td>Rank-1</td><td>mAP</td><td>Rank-1</td><td>mAP</td><td>Rank-1</td><td>mAP</td></tr><tr><td>AGW[23]</td><td>89.0</td><td>79.6</td><td>68.3</td><td>49.3</td><td>95.1</td><td>87.8</td></tr><tr><td>Aligned + + [24]</td><td>85.2</td><td>81.2</td><td>66.3</td><td>40.7</td><td>92.8</td><td>89.4</td></tr><tr><td>SPM[25]</td><td>72.5</td><td>86.5</td><td>丨</td><td>1</td><td>86.5</td><td>72.5</td></tr><tr><td>SCSN[26]</td><td>90.1</td><td>79.0</td><td>83.0</td><td>58.0</td><td>95.7</td><td>88.5</td></tr><tr><td>TransReID[9]</td><td>89.6</td><td>80.6</td><td>83.3</td><td>64.9</td><td>95.0</td><td>88.2</td></tr><tr><td>HAT[27]</td><td>90.4</td><td>81.4</td><td>82.3</td><td>61.2</td><td>95.6</td><td>89.5</td></tr><tr><td>VPAT[28]</td><td>88.8</td><td>78.2</td><td>1</td><td>1</td><td>95.4</td><td>88.0</td></tr><tr><td>CBDBNet[29]</td><td>87.7</td><td>74.3</td><td>丨</td><td></td><td>94.4</td><td>85.0</td></tr><tr><td>LDS[30]</td><td>91.5</td><td>82.5</td><td>86.5</td><td>67.2</td><td>95.8</td><td>90.4</td></tr><tr><td>本文</td><td>92.0</td><td>82.5</td><td>86.7</td><td>69.8</td><td>96.7</td><td>91.3</td></tr></table></body></html>

# 2． 3 消融实验

本节提出的方法在1 个公开数据集上分别进行消融实验。 实验结果见表3，各个模块均能提升识别精度，联合使用效果最好。

表3　 在数据集上的消融实验 ％  

<html><body><table><tr><td rowspan="2">方法</td><td colspan="2">Market-1501</td></tr><tr><td>Rank-1</td><td>mAP</td></tr><tr><td>Baseline</td><td>93.6</td><td>86.0</td></tr><tr><td>+ IceNet</td><td>94.6</td><td>88.6</td></tr><tr><td>+ CPE</td><td>95.0</td><td>89.1</td></tr><tr><td>+ JPM</td><td>94.3</td><td>88.0</td></tr><tr><td>+ DAD-T</td><td>93.9</td><td>86.5</td></tr><tr><td>本文</td><td>96.7</td><td>91.3</td></tr></table></body></html>

此外，本文还对比了在相同训练环境下添加 DAD⁃T 模块前后的模型运行时间，选取 Market⁃1501 数据集为基准。 训练次数设置为120 轮，添加 DAD⁃T 模块前后平均每轮训练时间为185、172 s。 结合表3消融实验表明，该模块能在相对较少的时间内到达更高的识别精度。

# 2． 4 可视化热力图

图8 展示了添加本文提出模块前后热力图可视化图像对比，热力图中暖色表示网络对行人图像某一区域的关注度，颜色越明显，代表关注程度越高。 结果显示本文添加模块可以很好地提升网络性能，能够同时关注行人图像中粗粒度的全局信息和细粒度的局部信息，验证了本模型出色的鲁棒性。

![](images/888b214c96a12c3ebe08714a03a96e5b1faf96bd0ab638e261100c1017929299.jpg)  
图 8 可视化对比

# 3 结论

行人重识别是图像分类的子类任务，近年来随着深度学习的发展，这项技术被广泛应用于公共安全领域。 但由于换衣、光照、遮挡等因素的干扰会导致行人重识别的准确率降低。 同时由于 Transformer网络缺少归纳偏差，计算复杂度高导致训练过程缓慢，本文提出了一种基于暗光增强和细粒度特征提取的行人重识别模型。 首先对行人图像进行图像增强，使行人图像更加清晰，色彩更加明亮，减少色差影响，其次在 Transformer 中嵌入条件位置编码和拼图补丁模块使网络拥有平移不变性，拥有综合提取行人全局特征和局部特征的能力。 最后考虑到因为计算复杂度高导致训练过程缓慢的问题，为了加速网络的收敛速度，添加了轻量化模块，提高了模型的鲁棒性和泛化能力。 实验在3 个行人重识别公开数据集上进行训练和评价，结果显示本文提出的方法有效缓解了暗光场景下行人重识别准确率低的问题。

# ［　 参　 考　 文　 献　 ］

［1］ 罗浩，姜伟，范星，等． 基于深度学习的行人重识别研究进展［J］． 自动化学报，2019，45（11）：2032⁃2049．  
［2］ SUN Yifan，ZHENG Liang，YANG Yi，et al． Beyond part models： Person retrieval with refined part pooling （ and a strongconvolutional baseline）［C］ ／ ／ Proceedings of the European Conference on Computer Vision，2018：480⁃496．  
［3］ PARK H，HAM B． Relation network for person re⁃identification［C］ ／ ／ Proceedings of the AAAI Conference on Artificial In⁃telligence，2020：11839⁃11847．  
［4］ LI Yuanyuan，WANG Xiaofei，ZHU Zhiqin，et al． A novel person re⁃id method based on multi⁃scale feature fusion［ C］ ／ ／39th Chinese Control Conference，2020：7154⁃7159．  
［5］ ISLAM K． Person search：New paradigm of person re⁃identification：A survey and outlook of recent works［ J］． Image andVision Computing，2020，101：103970．  
［6］ ZHANG Zhizheng，LAN Cuiling，ZENG Wenjun，et al． Relation⁃aware global attention for person re⁃identification［ C］ ／ ／Proceedings of the Conference on Computer Vision and Pattern Recognition，2020：3186⁃3195．  
［7］ VASWANI A，SHAZEER N，PARMAR N，et al． Attention is all you need［ Z ／ OL］． （2017⁃06⁃19） ［2024⁃06⁃07］． https： ／ ／arxiv． org ／ abs ／ 1706． 03762v2．  
［8］ DOSOVITSKIY A，BEYER L，KOLESNIKOV A，et al． An image is worth $1 6 \times 1 6$ words：Transformers for image recognitionat scale［Z ／ OL］． （2021⁃06⁃03）［2024⁃06⁃07］． https： ／ ／ arxiv． org ／ abs ／ 2010． 11929v2．  
［9］ HE Shuting，LUO Hao，WANG Pichao，et al． Transreid：Transformer based object re⁃identification［C］ ／ ／ Proceedings of theIEEE ／ CVF International Conference on Computer Vision，2021：15013⁃15022．  
［10］ ZHU Kuan，GUO Haiyun，ZHANG Shiliang，et al． Aaformer：Auto⁃aligned transformer for person re⁃identification［ C］ ／ ／IEEE Transactions on Neural Networks and Learning Systems，2023．  
［11］ LUO Hao，GU Youzhi，LIAO Xingyu，et al． Bag of tricks and a strong baseline for deep person re⁃identification［C］ ／ ／ Pro⁃ceedings of the IEEE ／ CVF Conference on Computer Vision and Pattern Recognition Workshops，2019．  
［12］ KO K，KIM C S． IceNet for interactive contrast enhancement［J］． IEEE Access，2021，9：168342⁃168354．  
［13］ CHU Xiangxiang，TIAN Zhi，ZHANG Bo，et al． Conditional positional encodings for vision transformers［ Z ／ OL］． （2023⁃02⁃13）［2024⁃06⁃07］． https： ／ ／ arxiv． org ／ abs ／ 2102． 10882v3．  
［14］ LI Bonan，HU Yinhan，NIE Xuecheng，et al． Dropkey for vision transformer［ C］ ／ ／ 2023 IEEE ／ CVF Conference on Com⁃puter Vision and Pattern Recognition，2023：22700⁃22709．  
［15］ QIN Ziheng，WANG Kai，ZHENG Zangwei，et al． Infobatch：Lossless training speed up by unbiased dynamic data pruning［Z ／ OL］． （2023⁃10⁃20）［2024⁃06⁃07］． https： ／ ／ arxiv． org ／ abs ／ 2303． 04947v2．  
［16］ LUO Wenjie，LI Yujia，URTASUN R，et al． Understanding the effective receptive field in deep convolutional neural net⁃works［Z ／ OL］． （2017⁃01⁃25）［2024⁃06⁃07］． https： ／ ／ arxiv． org ／ abs ／ 1701． 04128v2．  
［17］ WANG Xiaolong，GIRSHICK R，GUPTA A，et al． Non⁃local neural networks［ C］ ／ ／ IEEE Conference on Computer Visionand Pattern Recognition，2018：7794⁃7803．  
［18］ WORTSMAN M，LEE J，GILMER J，et al． Replacing softmax with relu in vision transformers［ Z ／ OL］． （2023⁃10⁃17）［2024⁃06⁃07］． https： ／ ／ arxiv． org ／ abs ／ 2309． 08586v2．  
［19］ HAN Dongchen，PAN Xuran，HAN Yizeng，et al． Flatten transformer：Vision transformer using focused linear attention［C］ ／ ／IEEE ／ CVF International Conference on Computer Vision，2023：5961⁃5971．  
［20］ ZHENG Zhedong，ZHENG Liang，YANG Yi． Unlabeled samples generated by gan improve the person re⁃identificationbaseline in vitro［C］ ／ ／ IEEE International Conference on Computer Vision，2017：3754⁃3762．  
［21］ WEI Longhui，ZHANG Shiliang， GAO Wen， et al． Person transfer gan to bridge do main gap for person re⁃identifica⁃tion［C］ ／ ／ IEEE Conference on Computer Vision and Pattern Recognition，2018：79⁃88．  
［22］ ZHENG Liang，SHEN Liyue，TIAN Lu，et al． Scalable person re⁃identification：A benchmark ［ C］ ／ ／ IEEE InternationalConference on Computer Vision，2015：1116⁃1124．  
［23］ YE Mang，SHEN Jianbing，LIN Gaojie，et al． Deep learning for person re⁃identification：A survey and outlook［ J］． IEEETransactions on Pattern Analysis and Machine Intelligence，2021，44（6）：2872⁃2893．  
［24］　 LUO Hao，JIANG Wei，ZHANG Xuan，et al． Alignedreid $\mathbf { \psi } + \mathbf { \psi } + \mathbf { \psi }$ ：Dynamically matching local information for person re⁃iden⁃tification［J］． Pattern Recognition，2019，94：53⁃61．  
［25］ ZHANG Shizhou，ZHANG Qi，YANG Yifei，et al． Person re⁃identification in aerial imagery ［ J］． IEEE Transactions onMultimedia，2020，23：281⁃291．  
［26］ CHEN Xuesong，FU Canmiao，ZHAO Yong，et al． Salience⁃guided cascaded suppression network for person re⁃identifica⁃tion［C］ ／ ／ IEEE ／ CVF Conference on Computer Vision and Pattern Recognition，2020：3300⁃3310．  
［27］　 ZHANG Guowen，ZHANG Pingping，QI Jinqing，et al． Hat：Hierarchical aggregation transformers for person re⁃identifica⁃tion［C］ ／ ／ 29th ACM International Conference on Multimedia，2021：516⁃525．  
［28］ LI Yulin，HE Jianfeng，ZHANG Tianzhu，et al． Diverse part discovery：Occluded person re⁃identification with part⁃awaretransformer［C］ ／ ／ IEEE ／ CVF Conference on Computer Vision and Pattern Recognition，2021：2898⁃2907．  
［29］　 TAN Hongchen，LIU Xiuping，BIAN Yuhao，et al． Incomplete descriptor mining with elastic loss for person re⁃identifica⁃tion［J］． IEEE Transactions on Circuits and Systems for Video Technology，2021，32（1）：160⁃171．  
［30］ ZANG Xianghao，LI Gezhong，GAO Wei，et al． Learning to disentangle scenes for person re⁃identification［ J］． Image andVision Computing，2021，116：104330．

［责任编辑：谢 平］

# Person re⁃identification based on dim light enhancement and fine⁃grained feature extraction

YANG Jing，　 GU Lingkang，　 XIA ZhouxiangSchool of Computer and Information， Anhui Polytechnic University， Wuhu 241000， China

Abstract：　 A person re⁃identification model based on dim light enhancement and fine⁃grained feature extraction was designed to address the issue of low accuracy in person re⁃identification caused by factors such as lighting changes and color differences in the real environment． First， the dim light enhancement module is used to enhance the input person image and optimize the image information． Then the image is input into the improved transformer encoder， which introduces conditional positional encoding conditional positional encoding and jigsaw patches module． It prompts the network to pay attention to the fine⁃grained feature information of person images and enhances the ability to extract person features． Finally， the DAD⁃T module was designed to alleviate model performance degradation and reduce complexity． This improves training efficiency and speeds up training without loss of performance． Experiments were conducted on three public person re⁃identification data sets： MSMT17， DukeMTMC⁃ReID and Market⁃1501， and compared with mainstream network models． The results show that the Rank⁃1 and mAP indicators of the model proposed in this article have improved com⁃ pared with the current mainstream models， and it has a high recognition accuracy．

Key words：　 dim light enhancement； fine⁃grained； Vision Transformer； person re⁃identification