# 《海南热带海洋学院学报》网络首发论文

题目： 基于残差图神经网络的伪造人脸图像多尺度检测方法  
作者： 俞弦，邓惠俊  
收稿日期： 2024-11-25  
网络首发日期： 2025-01-21  
引用格式： 俞弦，邓惠俊．基于残差图神经网络的伪造人脸图像多尺度检测方法[J/OL]．海南热带海洋学院学报. https://link.cnki.net/urlid/46.1085.G4.20250121.1524.004

网络首发：在编辑部工作流程中，稿件从录用到出版要经历录用定稿、排版定稿、整期汇编定稿等阶段。录用定稿指内容已经确定，且通过同行评议、主编终审同意刊用的稿件。排版定稿指录用定稿按照期刊特定版式（包括网络呈现版式）排版后的稿件，可暂不确定出版年、卷、期和页码。整期汇编定稿指出版年、卷、期、页码均已确定的印刷或数字出版的整期汇编稿件。录用定稿网络首发稿件内容必须符合《出版管理条例》和《期刊出版管理规定》的有关规定；学术研究成果具有创新性、科学性和先进性，符合编辑部对刊文的录用要求，不存在学术不端行为及其他侵权行为；稿件内容应基本符合国家有关书刊编辑、出版的技术标准，正确使用和统一规范语言文字、符号、数字、外文字母、法定计量单位及地图标注等。为确保录用定稿网络首发的严肃性，录用定稿一经发布，不得修改论文题目、作者、机构名称和学术内容，只可基于编辑规范进行少量文字的修改。

出版确认：纸质期刊编辑部通过与《中国学术期刊（光盘版）》电子杂志社有限公司签约，在《中国学术期刊（网络版）》出版传播平台上创办与纸质期刊内容一致的网络版，以单篇或整期出版形式，在印刷出版之前刊发论文的录用定稿、排版定稿、整期汇编定稿。因为《中国学术期刊（网络版）》是国家新闻出版广电总局批准的网络连续型出版物（ISSN 2096-4188，CN 11-6037/Z），所以签约期刊的网络版上网络首发论文视为正式出版。

# 基于残差图神经网络的伪造人脸图像多尺度检测方法

俞  弦，邓惠俊

（合肥经济学院 大数据学院，合肥 230011）

摘要：伪造人脸图像包含大量浅层特征高频分量，其面部关键点特征处于多尺度维度，在提取过程中梯度快速消失，导致伪造图像检测不准确。为此，提出一种基于残差图神经网络的伪造人脸图像多尺度检测方法。对采集的伪造人脸图像进行校正，并初步估计面部关键点位置。将面部关键点输入残差图神经网络，该网络采用残差块避免梯度消失，同时结合多尺度注意力机制，增强对不同尺度伪造痕迹的敏感度，提取人脸图像关键点的多尺度特征，并构建相应的特征图。融合特征图后，获得面部关键点的复杂特征。利用损失函数计算提取的特征与样本图像特征之间的相似度，从而准确检测图像中的伪造区域。实验结果表明，所设计的检测方法在多个数据集中的 AUC 值平均为 0.96，能够准确检测出人脸图像的伪造区域。

关键词：残差图神经网络；伪造人脸图像；图像检测；多尺度检测；特征融合；关键点特征中图分类号：TP391.41 文献标志码：A

# 0 引言

借助人脸伪造技术能够生成高精度的人脸图像，仅凭肉眼难以判断这些图像的真伪。此类技术被用于制造虚假信息、实施网络欺诈以及侵犯个人隐私等，给个人和社会带来诸多负面影响[1]。因此，开发高效、准确的伪造人脸图像检测技术已成为当务之急。

Wang 等[2]在卷积神经网络的基础上，提取图像特征，并划分出相应的候选区域；引入注意力机制，对候选区域进行分类和位置回归，生成具有一定尺寸的特征图；再对特征图进行深入分析，确定初步位置；利用回归网络对候选位置进行微调；通过计算不同候选区域的权重值，实现对关键特征的提取，由此输出相应的图像检测结果。然而，该方法的卷积神经网络在深层结构中容易出现梯度消失现象，导致网络难以有效捕捉并传递多尺度面部关键点的细微特征。宋清华等[3]构建人脸伪造视频数据集；利用双支流网络对数据集的图像语义信息和高频噪声信息进行分析，分别提取不同类型信息的关键特征；将所提取特征融合构建特征向量，进而构建检测模型，完成对伪造图像的检测。但该方法的双支流网络虽然能够提取不同类型信息的关键特征，但在深层特征融合过程中，由于梯度消失问题导致部分面部关键点特征丢失。宋家骏等[4]基于 U-HRNet 对图像进行分析，筛选出伪造特征，设计损失函数以优化特征的嵌入分布；利用 HiFi-Net 设计的多分支特征提取器，对图像的伪造区域进行像素级分割；针对每个模块生成对应伪造掩码，识别不同伪造类型后输出检测结果。HiFi-Net 设计的多分支特征提取器虽然能够实现图像伪造区域的像素级分割，但在多尺度特征提取方面仍有优化空间。彭舒凡等[5]先对图像进行归一化处理；采用数据增强策略提高图像的质量；在注意力机制的作用下设定激活函数，提取面部关键区域的敏感性；利用 CNN 提取图像细粒度特征，利用多级滑动窗口区分不同特征并做进一步筛选，从而输出检测结果。但该方法在深层特征提取过程中，因梯度消失会造成部分面部关键点特征丢失或失真。

本文提出基于残差图神经网络的伪造人脸图像多尺度检测方法，充分发挥深度学习的优势，通过自动提取图像深层次特征，提升检测精度与泛化性能；同时，引入多尺度特征提取机制，增强模型对图像中不同尺度伪造痕迹的敏感度，进一步提高检测的鲁棒性。

# 1 伪造人脸图像多尺度检测方法设计

# 1.1 伪造人脸图像校正与面部关键点识别

从公开数据集网站、社交媒体平台等多个渠道，收集大量伪造人脸图像，并将获取的人脸图像作为检测样本[6]。在开展检测时，首先需要对图像进行校正处理，以确保图像中的人脸能够完整呈现[7]。其具体校正过程如下：

式（1）中， $X _ { u }$ 为伪造人脸图像的校正结果； $N$ 为该图像中像素的总量； $x _ { n }$ 为原始的伪造人脸图像； $k$   
为图像频谱值； $\beta _ { n }$ 为随机复数。

对图像进行校正处理，能够有效提高图像质量，进而从中提取人脸图像的面部关键点。在此基础上，对人脸面部关键点进行初步估计[8]。其具体的估计过程如下：

式（2）中， $H _ { i } \left( x , y \right)$ 为初步识别到的第 $i$ 个关键点位置； $p _ { k }$ 为面部关键点的热力值； $s _ { i }$ 为人脸关键点所在位置的像素值。

在完成人脸面部关键点位置的初步估计后，通过图像变换，计算相应的关键点位置误差，以此确定面部关键点的位置[9]。具体计算过程如下：

$$
{ { S } _ { i } } = \frac { 1 } { { { T } _ { i } } } { \left[ { { T } _ { i } } \left( { { H } _ { i } } \left( x , y \right) \right) + \Delta { { c } _ { i } } \right] } ,
$$

式（3）中， $S _ { i }$ 为识别到的关键点位置； $T _ { i }$ 为图像中瞳孔的距离； $\Delta c _ { i }$ 表示位置误差。

据此，识别人脸图像多个面部关键点，并生成相应的关键点位置集合，为后续准确检测人脸图像关键点位置奠定基础。

# 1.2 基于残差图神经网络的面部关键点多尺度特征融合

基于识别到的人脸图像面部关键点，运用残差图神经网络，提取出关键点的多尺度特征，并将这些特征进行融合。

残差图神经网络通过引入残差块，有效避免了神经网络在应用时出现的梯度消失等问题，提高了网络的应用性能。此外，借助残差块，能构建更为深层的网络结构，从而提取出关键点的复杂特征[10]。同时，通过设定不同的深层网络参数，得以提取出关键点的多尺度特征[11]。利用残差图神经网络提取人脸图像面部关键点特征的具体过程如图 1 所示。

![](images/6c4e6bbf4830eae31e70ef8c5df9d7a7ceb60ac0ab516d30e515091c31543eca.jpg)  
图1 基于残差图神经网络的人脸图像面部关键点的特征提取过程

如图 1 所示，将包含已识别关键点的人脸图像输入到残差图神经网络。首先，利用引入的残差块对图像进行线性变换[12]。再将变换后的图像输入转置卷积层，利用多卷积层，提取关键点在不同尺度上的深层特征。在此基础上，利用池化层和全连接层，对提取的特征进行深度处理，构建特征图。

在上述过程中，引入的残差块表达式如下：

式（4）中， $Y _ { c }$ 为引入的残差块；F 为图像的变换函数； $x _ { i }$ 为输入的人脸图像； $W _ { i }$ 为该残差块的权重系数； $x _ { c }$ 为残差块的跳跃连接参数。

基于该残差块，利用多卷积层，提取人脸图像关键点的多尺度特征[13]。在提取时，引入多头注意力机制，提取出多尺度特征[14]，并构建相应的特征图。其具体表达式如下：

$$
A _ { h } = s _ { f } \times \left( \frac { q _ { h } \left( k _ { h } \right) } { Y _ { c } \sqrt { c _ { h } } } \right) \times \nu _ { h } ,
$$

$$
F _ { t } = \sum _ { h = 1 } ^ { H } A _ { h } \times \sigma _ { t } \Bigl [ F _ { c } \left( a _ { m } \right) + F _ { c } \left( m _ { p } \right) \Bigr ] \times M _ { c } ,
$$

式（5）中， $A _ { h }$ 为多头注意力机制； $s _ { f }$ 为注意力函数； $q _ { h }$ 为残差块之间的相关性； $k _ { h }$ 为残差块的键向量； $c _ { h }$ 为残差块的多尺度值； $\nu _ { { } _ { h } }$ 为注意力矩阵。式（6）中， $F _ { _ t }$ 为输出的特征图； $H$ 为提取的尺度特征

数量； $\sigma _ { t }$ 为神经网络层参数； $F _ { c }$ 为网络的全连接层； $\boldsymbol { a } _ { { } _ { m } }$ 为特征池化函数； $m _ { p }$ 为最大池化函数； $M _ { c }$ 为尺度因子。

根据特征图中的多尺度特征，进行特征融合。特征融合的结果表示式如下：

$$
R _ { h } = \sum _ { k = 1 } ^ { K } F _ { t } ^ { k } \times r _ { k } \times \frac { 1 } { \sqrt { r _ { h } \times C _ { m } } } ,
$$

式（7）中， $R _ { h }$ 为融合后的关键点特征； $F _ { t } ^ { k }$ 为第 $k$ 个尺度的关键点特征； $r _ { k }$ 为尺度融合参数； $r _ { h }$ 为特征嵌入量； $C _ { m }$ 为激活函数。

# 1.3 伪造人脸图像检测结果输出

基于上述提取的人脸图像关键点融合特征，引入损失函数，计算关键点特征与样本图像关键点特征之间的相似度[15]。具体计算过程如下：

式（8）中， $L _ { s }$ 为特征损失函数； y 为提取特征的熵值。式（9）中， $P _ { s }$ 为提取关键点特征与实际关键点特征的相似度； $R _ { h } ^ { \ j }$ 为公式（7）融合后的关键点特征值； $S _ { r } { } ^ { j }$ 为公式（3）识别的人脸图像的关键点特征； $\boldsymbol { r }$ 为人脸图像中关键点的数量。

若计算得到的 $P _ { s }$ 高于设定的阈值，则该特征对应的关键点为伪造的。通过此方式，能够提取出多个伪造关键点，进而确定相应的人脸图像伪造区域[16]。由此，实现了对伪造人脸图像的精确鉴别，有效保障了人脸信息的安全。

# 2 实验与分析

# 2.1 实验准备

实验主要采用 3 个数据集，即 FaceForensics $^ { + + }$ 数据集、Celeb-DF 数据集以及 FaceSwap Dataset 数据集。这些数据集中包含大量不同性别、年龄的虚拟伪造面部视频，从中随机抽取部分实验图像（图 2）。

![](images/8b00143bace15af7ca3feaa000752a6055c09ff13f7960d3df740e32c4ae17a2.jpg)  
图2 样本图像

图 2 中（a）为真实图像，（b）为通过伪造技术生成的伪造图像，以此作为样本图像展开后续测试。在 Opencv-python、Matplotlib 仿真平台上进行测试时，需对实验数据集进行预处理。首先利用 dlib工具将图像中的人脸定位，然后按照一定比例将其裁剪出来，裁剪尺寸设为 $3 0 0 \ \mathrm { p x } \times 3 0 0 \ \mathrm { p x }$ 。实验的具体参数如表 1 所示。

表1 实验参数  

<html><body><table><tr><td>实验参数</td><td>参数设置</td><td>实验参数 参数设置</td></tr><tr><td>批量大小</td><td>64</td><td>数据集比例 82:9:9</td></tr><tr><td>学习率</td><td>0.005</td><td>尺度因子 0.25</td></tr><tr><td>训练轮数</td><td>100</td><td>尺度数量 16</td></tr></table></body></html>

实验中，运用残差图神经网络提取出样本图像的特征。所应用的残差图神经网络的具体参数如表 2所示。

表2  残差图神经网络参数  

<html><body><table><tr><td>网络结构</td><td>卷积核大小 卷积核数量 步长</td><td>网络结构</td><td>卷积核大小</td><td>卷积核数量</td><td>步长</td></tr><tr><td>残差块1</td><td>3×3 64 2</td><td>转置卷积层</td><td>3×3</td><td>256</td><td>1</td></tr><tr><td>内部卷积层1-1</td><td>3×3 64 2</td><td>卷积层1</td><td>5×5</td><td>256</td><td>2</td></tr><tr><td>内部卷积层1-2</td><td>3×3 128</td><td>卷积层2</td><td>5×5</td><td>512</td><td>1</td></tr><tr><td>残差块2</td><td>3×3 128 2</td><td>池化层</td><td>1</td><td>1</td><td>4</td></tr><tr><td>内部卷积层2-1</td><td>1×1 128 1</td><td>全连接层</td><td>1</td><td>1</td><td>2</td></tr><tr><td>内部卷积层2-2 3x3</td><td>256 2</td><td></td><td></td><td></td><td></td></tr></table></body></html>

如表 2 所示，利用上述设定的网络层参数，提取出相应的图像特征。

基于此，为验证本研究设计方法的效果，特设计对比实验。选用文献[2]、[3]、[4]、[5]方法（分别记为方法 1、2、3、4）为对比方法。

在对比验证时，选用准确率、召回率、AUC 值为评价指标。AUC 值能够衡量方法检测结果的准确性，该值越高，表明该方法结果的准确性越高。AUC 用于衡量不同方法对于真实本文和伪造本文的分类能力，即检测方法的性能，尤其在不同数据集测试中，能直接反映检测方法的泛化能力。AUC 值的具体计算公式如下：

$$
A _ { u } = \frac { \left( F _ { a } - F _ { b } \right) \times \left( F _ { a } + F _ { b } \right) } { 2 } ,
$$

式（10）中， $A _ { u }$ 为方法的 AUC 值； $F _ { a }$ 为检测结果的真正例率； $F _ { b }$ 为检测结果的假正例率。

# 2.2 可视化结果验证

运用本文方法和对比组方法 1\~4 共 5 种方法检测图 2 的伪造区域，将检测到的伪造区域予以标注，并进行可视化呈现，以此直观地展现出本文方法与对比组的检测结果（图 3）。

![](images/14e35a1e46744af189e23de26722fc9629720142dd2bca8954a81e418c5c9cf3.jpg)  
图3 本文方法与对比组方法1\~4 检测结果的可视化展示

如图 3 所示，本文方法通过识别人脸面部的关键点，并对关键点的特征展开对比，以此判定关键点是否存在伪造情况，较为准确地确定了伪造的关键点[图3(a)]。这些关键点连接构成的区域，即为该人脸图像的伪造区域。将此伪造结果与原图像相比，可发现检测结果与伪造区域[图2(c)]高度契合。而对比组的 4 种方法与实际伪造区域相比，仅能检测出部分伪造区域[图 3(b\~e)]，其检测精准度低于本文方法。

# 2.3 AUC 值对比分析

采用本文方法与对照组方法 1\~4 共 5 种方法，在 FaceForensics++、Celeb-DF 以及 FaceSwap Dataset 三个实验数据集上，针对伪造图像进行多次检测，并对 5 种方法的准确率、召回率、AUC 值进行统计与对比。其中，准确率、召回率如表 3 所示，AUC 值结果如图 4 所示。

表3  本文方法与对比组方法1\~4 在不同数据集上的准确率与召回率  

<html><body><table><tr><td rowspan="2">方法</td><td colspan="2">准确率/% <</td><td colspan="3">召回率/%</td></tr><tr><td>FaceForensics++ Celeb-DF</td><td>FaceSwap Dataset</td><td>FaceForensics++</td><td>Celeb-DF</td><td>FaceSwap Dataset</td></tr><tr><td>本文方法</td><td>98.5 97.8</td><td>99.2</td><td>91.7</td><td>90.6</td><td>90.7</td></tr><tr><td>方法1</td><td>97.0 96.5</td><td>98.0</td><td>86.7</td><td>87.5</td><td>88.4</td></tr><tr><td>方法2</td><td>95.1 94.2</td><td>96.0</td><td>86.8</td><td>84.1</td><td>82.8</td></tr><tr><td>方法3</td><td>93.0 92.5</td><td>94.5</td><td>86.4</td><td>86.6</td><td>85.8</td></tr><tr><td>方法4</td><td>92.0 91.1</td><td>93.6</td><td>83.7</td><td>84.5</td><td>83.8</td></tr></table></body></html>

通过对表 3 的分析可知，本文方法在 FaceForensics $^ { + + }$ 、Celeb-DF 及 FaceSwap Dataset 三个数据集上均呈现出良好的应用性能。在 FaceForensics $^ { + + }$ 数据集上，本文方法准确率达到 $9 8 . 5 \%$ ，召回率为 $91 . 7 \%$ ，显著高于对比方法；在 Celeb-DF 上，本文方法准确率达到 $9 7 . 8 \%$ ，召回率为 $9 0 . 6 \%$ ，同样高于对比方法；在 FaceSwap Dataset 上，本文方法准确率高达 $9 9 . 2 \%$ ，召回率为 $9 0 . 7 \%$ ，表现尤为突出。相比之下，对比方法虽各有优势，但在准确率和召回率两项指标上均未能超越本文方法，有力地验证了本文方法的有效性和先进性。

![](images/fb2c6947a1a99ac024548353de5e11e29605eb9755d880b7a205355978d632bd.jpg)  
（a）在 FaceForensics $^ { + + }$ 数据集上的AUC 值

![](images/df99f8ed561b6bcd9fc154a99f50cde39bcd16ef89a9623592be787470425242.jpg)  
（b）在 Celeb-DF 数据集上的 AUC 值

![](images/c3ca6dd843522da239b2d2b3b8da98c8c4682bb1946800cb65294aab3d34c75a.jpg)  
（c）在 FaceSwap Dataset 数据集的 AUC 值  
图4  本文方法与对比组方法1\~4 的AUC 值对比

如图 4 所示，在 3 个数据集上，本文方法的 AUC 值均较高，平均值达到 0.96。对比组 4 种方法检测结果 AUC 值不仅相对较低，且数值较不稳定。例如，方法 3 在 Celeb-DF 数据集上的检测结果 AUC值较低，检测精度较差。

# 3 结束语

本文所提方法通过构建精细的残差图神经网络架构，能够在不同尺度下高效地捕捉图像的细微特征差异，进而实现对伪造人脸图像的有效识别。在实际应用中，该方法首先对输入图像进行多尺度分解，利用残差学习机制强化特征表达能力，随后通过神经网络深度挖掘特征中的判别性信息，从而精准区分真实与伪造人脸。对比实验结果验证了本文所提方法在提升伪造图像的检测精度与效率方面有显著效果。

# 参考文献：

[1] 黄祖超,叶锋,黄添强.基于扩散模型的伪造人脸检测分析[J].福建师范大学学报(自然科学版),2024(2):14.  
[2] WANG S, ZHANG H L, YANG G B, et al. A two-stage fake face image detection algorithm with expanded attention[J].  
Multimedia tools and applications, 2024(18):55709.  
[3] 宋清华,吕东辉,冯国瑞.针对低质量视频的双支流人脸伪造检测方法[J].工业控制计算机,2024(1):109.  
[4] 宋家骏,刘桂雄,黄家曦,等.应用 U-HRNet+SoftTripleLoss 的 HiFi-Net 伪造图像检测技术研究[J].中国测试,2023(9):37.  
[5] 彭舒凡,蔡满春,刘晓文,等.基于图像细粒度特征的深度伪造检测算法[J].信息网络安全,2022(11):77.  
[6] 张时润,彭勃,王伟,等.基于空洞卷积和注意力机制的深度伪造检测[J].现代电子技术,2022(5):42.  
[7] 董丰恺,邹晓强,王佳慧,等.基于帧内–帧间自融合的双流泛化人脸伪造检测方法[J].计算机工程,2024(10):190.  
[8] 冯才博,刘春晓,王昱烨,等.结合图像块比较与残差图估计的人脸伪造检测[J].中国图象图形学报,2024(2):457.  
[9] 宋凯,吴建华.基于单应性矩阵剔除SURF 错误匹配点的复制–移动伪造图像检测[J].沈阳工程学院学报(自然科学版),2021(3):62.  
[10] 苏百兖,杜永生,黄传波.基于极性复指数变换的图像伪造检测算法[J].太赫兹科学与电子信息学报,2021(3):501.  
[11] 杨雨鑫,周欣,熊淑华,等.融合传统特征与神经网络的深度伪造检测算法[J].信息技术与网络安全,2021(2):36.  
[12] 许楷文,周翊超,谷文权,等.基于多尺度特征融合重建学习的深度伪造人脸检测算法[J].信息网络安全,2024(8):1178.  
[13] 王艳,孙钦东,荣东柱,等.伪影间共性机理驱动的多域感知社交网络深度伪造视频检测[J].电子与信息学报,2024(9):3717.  
[14] 卢金花.高分辨率特征增强图像的小目标检测算法[J].海南热带海洋学院学报,2024(5):92.  
[15] 陈俊韬,朱子奇.基于多尺度特征提取与融合的图像复制–粘贴伪造检测[J].计算机应用, 2023(9):2920.  
[16] 杜媛.基于优化因子耦合比值制约规则的图像复制–粘贴篡改检测算法[J].系统仿真技术, 2023(2):151.

（责任编辑：何军民）

# Multi-scale Detection Method for Forged Face Images Based on Residual Graph Neural Network

YU Xian, DENG Huijun

(School of Data Science, Hefei University of Economics, Hefei 230011, China)

Abstract: The forged facial image, its facial key point features of which are in multi-scale dimensions, contains a large number of shallow high-frequency components. During the extraction process, the gradients quickly disappear, which results in inaccurate detection of the fake image. Therefore, a multi-scale detection method for fake face images based on a residual graph neural network was proposed. The collected fake facial images were corrected and the positions of facial key points were preliminarily estimated. The facial key points were input into the residual graph neural network, which uses residual blocks to avoid gradient vanishing. At the same time, the multi-scale attention mechanism was combined to enhance the sensitivity to forged traces at different scales, extract the multi-scale features of the facial image key points, and construct the corresponding feature maps. After fusing the feature maps, complex features of facial key points were obtained. The loss function was utilized to calculate the similarity between the extracted features and the sample image features, so as to accurately detect the counterfeit areas in the image. The results showed that the proposed detection method has an average AUC value of 0.96 in multiple datasets, and can accurately detect the forged areas in facial images.

Keywords: residual graph neural network; fake facial images; image detection; multi-scale detection; feature fusion; key point features