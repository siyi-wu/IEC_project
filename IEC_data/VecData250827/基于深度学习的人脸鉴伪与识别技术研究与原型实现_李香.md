电子科技大学UNIVERSITYOFELECTRONICSCIENCEANDTECHNOLOGYOFCHINA

# 硕士学位论文

MASTERTHESIS

![](images/fdfcc6385bd9cf2751b9e6f434b8bf65eddb46d13d8d10bd0151954139fa9820.jpg)

与原型实现学科专业 软件工程学 号 201721220217作者姓名 李香指导教师 陈波副教授分类号 密级UDC 注1

# 学位论文

基于深度学习的人脸鉴伪与识别技术研究与原型实现

（题名和副题名）

# 李香

（作者姓名）

指导教师 陈波 副教授电子科技大学 成都

（姓名、职称、单位名称）申请学位级别 硕士 学科专业 软件工程提交论文日期2020.4.1 论文答辩日期 2020.4.29学位授予单位和日期 电子科技大学 2020年06月答辩委员会主席评阅人

注1：注明《国际十进分类法UDC》的类号。

# Research and Prototype Implementation of Face Forgery Detection and Recognition Technology Based on Deep Learning

AMaster Thesis Submitted to

University of Electronic Science and Technology of China

Discipline: Software Engineering

Author: Xiang Li

Supervisor: Bo Chen

School: School of Information and Software Engineering

# 摘要

身份验证作为金融服务风险防御的第一道屏障，对金融消费者的信息和资金安全都起着关键性的作用，其中基于人脸生物特征识别的身份验证技术已经在安防和金融支付等领域拥有广泛的应用场景。近年来随着深度学习技术的不断创新和发展，人们利用便捷的开源工具就可以随意地编辑人脸属性，甚至合成逼真的人脸图像，这些伪造的人脸图像通过肉眼已经很难分辨是真实还是伪造的。如果不法分子将人脸的编辑和生成技术应用在基于人脸识别的身份认证服务上将造成严重的安全威胁，这给金融风控领域带来了重大的挑战。因此，本文关于人脸的身份认证问题的研究主要围绕人脸真假鉴伪和人脸识别两个方面展开，具体工作如下：

1.提出了基于跨年龄数据增强和特征压缩的人脸识别方案，主要从数据增强和特征压缩的角度进行改进。在数据增强方面，本文设计了AgeGAN生成对抗网络模拟不同年龄阶段的人脸图像来扩充人脸数据，实现数据增强；在特征压缩方面，本文采用主成分分析法对提取到的人脸特征向量进行压缩。最终在VGG16 和ResNet18两种特征提取网络上经过实验验证了该方案能够在降低预测阶段时间消耗的同时有效提高人脸识别模型的准确率。

2.提出了基于神经网络特征热图的人脸真假鉴别方案。本文在MesoNet基线模型的基础上进行了三点改进：第一，人脸图像数据在输入到VGG 网络后，从VGG网络的卷积层获取合适的特征热图，经放缩到原始样本的维度后，和原始样本合并作为模型的输入；第二，在模型中引入空洞卷积、非对称卷积和全局平均池化层等方式进一步降低网络的参数量；第三，结合数据集的特点，采用FocalLoss作为模型的损失函数。最后我们在公开数据集和自制的人脸鉴别数据集上进行了实验,数据集中包括了由DeepFake、StyleGAN和FusVAE 等模型生成的人脸数据，实验最终验证了该方案能有效提高模型的准确率和性能。

3.基于PythonFlask 轻量级框架对提出的人脸真假鉴别和人脸识别算法进行了功能设计和原型实现。该Web系统主要提供身份认证和信息录入等功能模块，对人脸识别和真假鉴别算法进行了展示和应用。

关键词：深度学习，身份认证，人脸识别，人脸鉴伪

# ABSTRACT

As the first barrier of financial services risk defense, authentication plays a key role in the consumer information and capital security. Among them,authentication technology based on face biometric has been widely used in security and payment fields.In recent years, with the innovation and development of deep learning technology, people can use convenient open source tools to edit face attributes at will, or even synthesize realistic face images, which are difficult to distinguish whether the image is real or fake by eyes. If criminals apply the face editing and generation technology in authentication services, it will cause serious security threats,which brings a great challenge to the field of financial risk control. Therefore, the research on face identification in this thesis mainly focuses on the two aspects of face forgery detection and face recognition. The specific work is as follows:

1.A face recognition scheme based on cross-age data enhancement and feature compression is proposed， which is mainly improved from the perspective of data enhancement and feature compression. In terms of data enhancement, this thesis designed AgeGAN to generate face images of different age stages in the adversity-network simulation to expand face data and realize data enhancement. In the aspect of feature compression, this thesis uses principal component analysis to compress the extracted face features vectors. Finally, experiments on two feature extraction networks, VGG16 and ResNet18，proved that this scheme could effectively improve the accuracy of face recognition model while reducing the time consumption in the prediction stage.

2.A face true or false discrimination scheme based on neural network characteristic heat map is proposed. In this thesis, three improvements are made on the basis of the MesoNet baseline model: first, after the face data is input into the VGG network, the appropriate characteristic heat map is obtained from the convolution layer of the VGG network. After the face data is shrunk to the dimension of the original sample, it is combined with the original sample as the input of the model. Secondly, the empty convolution, asymmetric convolution and global average pooling layer are introduced into the model to reduce the number of parameters of the network. Third, combining with the characteristics of the data set, Focal loss is used as the loss function of the model. Finally, we conducted experiments on public data sets and self-made face discrimination data sets, including face data generated by DeepFake, StyleGAN and Fus VAE models, and finally verified that this scheme can effectively enhance the accuracy and performance of the network model.

3.Based on the Python Flask lightweight framework, the functional design and prototype implementation of the proposed real and false face recognition algorithms are implemented. The Web system mainly provides functional modules such as identity authentication and information entry， and displays and applies face recognition and authentication algorithms.

Keywords: deep learning， identity authentication， face recognition， face forgery detection

# 目录

第一章绪论  
1.1 研究工作的背景与意义  
1.2 国内外研究现状  
1.2.1人脸编辑和生成研究现状 2  
1.2.2人脸真伪鉴别研究现状， 3  
1.2.3人脸识别研究现状 4  
1.3论文的主要研究内容 .5  
1.4 论文的结构安排 .6  
第二章 基础理论和相关技术 8  
2.1卷积神经网络 .8  
2.1.1 卷积层. .8  
2.1.2 池化层. .10  
2.1.3 全连接层 .10  
2.1.4 损失函数 11  
2.2 卷积神经网络模型 13  
2.2.1 VGG.. 13  
2.2.2 ResNet . .14  
2.3 图像生成技术 .. 15  
2.3.1 基于流的生成模型 .15  
2.3.2生成对抗网络 ..15  
2.3.3 变分自动编码器 . 17  
2.4 模型评价指标. .. 18  
2.5 本章小结. ..19  
第三章 基于跨年龄数据增强和特征压缩的人脸识别 .. 20  
3.1引言 ..20  
3.2改进点介绍 .20  
3.2.1人脸识别基本流程 .20  
3.2.2图像数据增强 .21  
3.2.3主成分分析法. 22

3.3基于跨年龄数据增强和特征压缩的特征提取算法

# 目录

3.3.1 跨年龄人脸数据增强 .25  
3.3.2人脸特征提取算法 ...27  
3.4 实验与结果分析 ...30  
3.4.1数据集及预处理 .30  
3.4.2 模型训练细节 .32  
3.4.3 实验环境. ....33  
3.4.4实验结果及分析 ..33  
3.5 本章小结 .35  
第四章基于特征热图的人脸鉴别技术研究 ...3.6  
4.1 引言 ..36  
4.2 改进点介绍.. .36  
4.2.1 特征热图 .36  
4.2.2 模型轻量化 ...39  
4.2.3人脸图像生成 .44  
4.3 基于特征热图的人脸鉴别模型架构 ..48  
4.3.1基线模型 ..48  
4.3.2 基于特征热图的人脸鉴别模型 .. 49  
4.4 实验与结果分析 .50  
4.4.1 数据集及预处理. ..50  
4.4.2实验结果及分析. .52  
4.4.3 BadCase 分析.. .54  
4.5 本章小结 ..55  
第五章基于人脸的身份认证原型实现 ..56  
5.1 需求分析. .56  
5.2 平台设计 .56  
5.3．系统实现与展示.. .59  
5.3.1开发环境 .59  
5.3.2主要功能实现与展示， .60  
5.4系统测试及结果分析. ....63  
5.5 本章小结 .63  
第六章总结与展望 .64  
6.1全文总结 ..64  
6.2后续工作展望 .65

# 目录

致谢 66   
参考文献 .67   
攻读硕士学位期间取得的成果 72

# 第一章绪论

# 1.1研究工作的背景与意义

近年来随着互联网技术的高速发展和广泛应用，各种互联网金融模式层出不穷，包括第三方支付、大数据金融、P2P网贷等新型金融模式。这些新的业务模式虽然带来了巨大的经济收益和便捷的金融服务，但也引发了大量的金融欺诈事件，给金融风控带来了新的挑战。金融欺诈是指在金融领域中出现的骗取钱财或金融机构信用，扰乱金融管理秩序的行为，具有频次高、欺诈手段更新快、风险传染性强等特征，涉事者通过虚构事实或者隐瞒事实真相等手段牟利[，随着电信和网络金融诈骗日益复杂多样，金融欺诈对人民财产安全和网络金融生态构成严重威胁，有知名征信公司统计数据显示，美国源于网络犯罪产生的经济损失在GDP中的占比为 $0 . 6 4 \%$ ，中国以 $0 . 6 3 \%$ 的占比仅次于美国，并且互联网欺诈风险进入全球排名前三[2]。所以对于金融行业来说，利用技术手段做好有效的欺诈检测与防范工作就显得格外重要。

不管针对交易欺诈还是申请欺诈，反欺诈技术都遵循从身份评估、交易评估到信用评估的流程进行处理。其中身份评估作为金融服务风险防御的第一道屏障，对金融消费者的信息和资金安全都起着关键性的作用。传统身份认证技术主要分为三种：客户本人知悉的静态密码、客户本人特有并且不可重复利用的数字证书或电子签名以及通过安全渠道产生的动态密码，这些身份标识物不仅携带不方便、难记忆，还容易被伪造从而遭受假冒攻击。生物识别技术是指利用人体特有的生理特征或行为进行身份鉴定的一种技术，并且近年来随着人工智能、大数据、互联网等科技水平的不断进步和发展，该项技术在从基础理论到实际应用领域都取得了不断突破和创新，成为新一代人工智能的重要研究领域，引起了全球范围内许多国家的广泛关注。据估计，截止到2021年，身份识别技术与应用市场规模将超过300 亿美元，与2015年的137亿美元相比，增长幅度高达 $1 1 8 \% ^ { [ 3 ] }$ 。当前指纹识别、步态分析、声纹识别、虹膜识别、人脸识别、笔迹鉴定等是比较成熟和流行的身份识别技术，每一种生物特征识别技术自身都存在一定的优缺点，有各自适合的应用场景，例如：指纹考勤打卡机、虹膜识别门禁系统、移动端人脸支付等，这些都为人民群众的生活带来了极大的便利。其中人脸识别通过提取一些面部特征来确定人的身份信息，在安全性、扩展性、便捷性和持续性等方面优势明显。

一个人脸识别系统通常主要由人脸图像数据采集及检测、人脸图像预处理、人脸图像特征提取和特征匹配等四大部分组成，例如当在一些银行或互联网金融平台注册账户时，会按照一定的要求采集一张客户的照片或根据用户提供的个人身份信息从权威部门获取存档照片构建人脸特征库。之后用户在办理相关网上业务时，只需要进行数据采集，得到一张用户的实时照片，然后与数据库中保存的标准照片进行比对，如果比对结果相似度较高，则判断为同一个人，身份验证成功。随着深度学习在图像处理领域研究的不断深入，人脸识别技术取得了突破性的进展，不仅算法实现了 $90 \%$ 以上的准确率，而且在安防和支付等领域拥有广泛的应用场景[4]。但是，识别的结果往往会受到姿态、光照、面部表情、年龄等多方因素的影响，特别是随着年龄的增长人的外貌特征在形状和纹理上会发生一些显著的变化，从而给人脸识别技术带来巨大的挑战。此外，随着人们对生成对抗网络和卷积神经网络相关模型不断发展创新，以及GPU硬件性能的提高，人脸图像生成和人脸属性编辑的效果都取得了突出成就，通过肉眼已经很难对人脸图像判定真伪。如果这些假数据被应用于现实生活的方方面面，不仅对金融领域造成严重的威胁，对整个人类社会的安全问题都是一个巨大的挑战。因此，本文基于深度学习相关技术对人脸真伪鉴别和识别算法开展了研究，并基于该算法设计一个身份验证的展示和应用方案。

# 1.2国内外研究现状

# 1.2.1人脸编辑和生成研究现状

近年来，随着信息技术的飞速发展，每天有近20亿张图像被上传到全球各类网站上，大约 $40 \% - 5 0 \%$ 的图像由于各种各样的原因进行了人为的计算机编辑5过程，有因为一些合理原因而进行的编辑（例如：杂志封面的修图），同时还有不法分子由于恶性意图进行的图像修改（如：假新闻活动）。由于人脸图像在社交活动和经济生活中所扮演的重要角色，近二十年来关于人脸编辑和生成的研究呈急速增长的趋势，根据所采用的技术方案的差异性，我们大致将其划分为传统的人脸变形方法和近年来广泛流行的深度学习方法。

传统的人脸变形方案除了经典质点弹簧网格模型，就是基于2D/3D人脸图像变形方法，如Luan Tran 在 2018 年提出的非线性3D人脸变形模型[和 2019 年提出的自然场景下的3D人脸形变方案7，这些方案对2D和3D人脸图像进行矩阵变换来达到人脸形变的目标。此外还有基于人脸姿态捕捉的建模方法，Thies et al8]基于消费级深度摄像头追踪并构建了从原始图像到目标图像的3D模型，首次实现了实时人脸表情的迁移。

随着深度学习技术的飞速发展，基于深度学习的人脸编辑和生成技术得到了广泛的关注。首先是人脸属性的编辑（面部表情，面部特征），如Korshunoval等人通过卷积神经网络实现了从输入人脸到目标人脸的全自动实时交换，并保持原来的姿势、面部表情和光照条件；Lample等人[提出了一种借助改变属性值生成图像变化的新思路，编码-解码器结构通过训练将图像的显著信息与潜在空间的属性值（年龄、性别、眼镜、眼睛和嘴巴张开幅度等）直接分离来进行图像的重构；Chen 等人[1将文本编码器和图像解码器在一个网络结构中同时进行训练，实现了细粒度文本到人脸图像的生成。除了对人脸面部属性进行编辑外，人们还对计算机生成高质量的图像进行了研究，这些生成的人脸图像几乎可以做到以假乱真。目前在深度学习领域主流的生成模型就是生成对抗网络（GAN）[12]以及其衍生出的变种，这些网络架构不仅能够生成高质量的图片，在计算机视觉中还得到了其它广泛的应用，如：图像修复、图像标注、图像风格转换等。原始的GAN借助零和博弈的思想让网络中的生成器和判别器不断得到优化，以实现真实图像和生成图像的分布相似度不断增加，但是却出现了生成样本多样性不足和训练不稳定的问题。LSGAN[13]为了使模型在训练的过程中更多关注真实度不高的样本，原始的损失函数被最小二乘损失函数代替，从而使原始GAN训练不稳定和生成样本多样性不足的问题得到了改善。当使用GAN进行图像生成时，我们根据网络中生成器和判别器的数量，将相关的生成方法总结为直接方法、分层方法和迭代方法等三大类。

（1）直接法：网络模型中生成器和判别器的数量都为1，并且两者的结构是直接的，没有任何分支。Radford[14]等人提出的DCGAN网络架构首次将卷积神经网络应用在GAN的架构中，由于生成网络变得稳定，实现了生成图像质量的提高。

（2）分层法：按照某种标准将图像分成两部分，在网络中设计两个生成器和鉴别器，其中两个生成器可以是并联或串联的关系，并将两个生成器生成的不同图像部分结合起来一起作为最后的生成图像。SS-GAN[15]先由一个 Structure-GAN 生成图像表面结构，然后再使用Style-GAN 对图片的样式进行补充，最后得到目标生成图像。

（3）迭代法：网络中包含了多个结构相同或相似的生成器，迭代生成从粗到细的图像。StackGan $+ + ^ { [ 1 6 ] }$ 将多个生成器和判别器排列成树状结构，实现图像从低分辨率到高分辨率的转变，最终得到高精度的图像。

# 1.2.2人脸真伪鉴别研究现状

鉴于人脸图像在社交活动和经济生活中所扮演的重要角色，关于人脸图像篡改和人脸生成方案层出不穷，这些经过篡改或生成的人脸图像被不法分子应用在身份识别和认证服务上，给相关的领域造成了严重的安全漏洞。因此，关于人脸图像真伪鉴别的研究也得到了广泛的关注。

在研究初期，人脸真伪鉴别原理主要是基于图像格式和相关元数据的特性进行数字图像取证算法和相关工具的研究[17-20]。如Hashmi 等人[17]为了解决图像复制移动伪造问题，提出了基于离散小波变换的检测算法，其思想是：首先对待检测图像进行离散小波变换分解为LL、LH、HL和HH四部分，然后为了提取关键特征并找到其对应的描述符向量，对LL部分使用尺度不变特征变换，最后基于各个描述符向量之间的相似性判断给定图像的真伪。此外广泛流行的还有基于频域分析的人脸真伪鉴别方法[21-23]，在使用JPEG技术压缩图像时会保留压缩历史记录，通过频域分析可以检测出人脸图像上的操作区域。如：Yang等人[21提出了一种因子直方图的统计量用于估计位图JPEG压缩历史记录。但是这种基于频域分析的方法具有一定的局限性，处理具有复杂和平滑边缘的图像效果会大打折扣，为了解决这个问题，研究人员基于JPEGGhost[24技术进行了改进。JPEGGhost的前提是假设如果图像被修改，伪造部分通常被认为是从其他质量不同的图像区域进行了复制操作，因此JPEGGhost会搜索在同一张图像上的不同区域是否存在不同的图像质量，然后检测不同区域是否存在图像质量的差异。当然，这种方法缺乏一定的健壮性，当图像伪造区域与原始图像的图像质量水平基本一致时，这种方法就失效了。

近年来，鉴于卷积神经网络在分类和图像分割领域的优异表现，越来越多的研究者开始尝试将卷积神经网络模型应用于人脸真伪鉴别问题。如Nguyen使用VGG网络的部分结构来加深Capsule网络的特征提取层，最终对人脸图像进行二分类检测[25]。Nataraj[26]首次将共生矩阵和卷积神经网络相结合来检测 GAN 所伪造的图像。此外，Bharati 等人[27提出了一种基于子类的监督稀疏自编码方法，用于对样本进行专门的稀疏编码，从而实现对不同种族和性别的人脸编辑检测。在损失函数的改进方面， $\mathrm { H s u } ^ { [ 2 8 ] }$ 采用对比损失的方法，通过两个级联的CNN网络捕获不同GAN生成的假图像的联合鉴别特征，同时将特征输入到分类器中得到图像真假的鉴别结果。上述方法往往会针对某一种或几种图像生成或篡改技术进行分类，往往不具有普遍性，并且图像的伪造技术也随着鉴别技术的提升而提升，这也是该领域所面临的最大问题。

# 1.2.3人脸识别研究现状

自上世纪70 年代人脸识别算法诞生以来[29-30]，人脸验证、识别和聚类等相关问题的研究就逐渐成为计算机视觉和生物识别领域热点。传统的人脸识别技术主要强调特征的人工设计和相关机器学习技术的选择，其中人工设计的特征可以是边和纹理的描述量，机器学习技术可以采用支持向量机、线性判别分析和主成分分析。但是人脸识别往往会受到姿态、光照、面部表情、年龄等多方因素的影响，传统的人工方式只能针对每种变化情况进行特征的设计，因此就会使算法缺乏一定的泛化性。GBHuang等人首次将深度学习算法应用到人脸识别领域，并且在公开数据集上取得 $87 \%$ 的识别准确率[31]。随后出现了许多表现优秀的识别模型，准确率也得到大幅度的提升。

但是随着年龄的增长，人的外貌特征在形状和纹理上会发生一些显著的变化，对模型的识别效果产生一定的影响。为了解决这一问题，有研究者提出了利用相关模型对人脸面部老化过程中发生的变化进行模拟，生成一张目标年龄阶段的图片用于人脸识别[32]。Shu 等人[33]提出的老化字典模型通过特定老化模式来线性组合不同年龄阶段的特征表示，但是这种方式会导致生成的老化人脸出现分影。Wang等人通过LSTM34网络模型学习一种面部老化的连续变化过程，受到数据量的约束，不具有很强的可靠性[35]。

在人脸识别系统中通常包括人脸检测、人脸对齐、人脸表征、人脸匹配等模块。人脸检测主要算法有早期基于模板匹配技术的 AdaBoost 框架、Viola 等人[36使用Haar-like 特征和级联AdaBoost分类器构造的人脸检测器；Zhang 等人[37采用候选框加分类器的思想设计了三个级联的多任务神经网络模型用于人脸区域和关键点的检测，三个级联网络分别是P-NET（快速生成候选窗口）、R-NET（选择高精度候选窗口过滤）、O-NET（边界框和人脸关键点生成）。随着深度学习特别是卷积神经网络的快速发展，人脸表征作为人脸验证流程中的最重要模块取得了较为突出的研究成果，基于神经网络的方法可以从大量训练的数据中自动学习稳健的人脸表征，而不用针对不同的情况设计特定的人脸表征。Facebook的DeepFace[38作为基于CNN的人脸识别最早的模型之一，其在LFW 基准数据集上实现了 $9 7 . 3 5 \%$ 的准确度，其贡献在于人脸对齐环节引入了3D人脸矫正模型、利用9层DCNN深度卷积网络在包含440万张人脸的数据集上进行人脸表征学习。香港大学团队提出了DeepID系列模型，其中DeepID1[39采用四层卷积网络架构，将不同的人脸区域放入CNN中进行提取特征，最后将不同的块所输出的特征连接起来形成高级人脸特征表示，而DeepID2[40]在DeepID1基础上加入了识别和验证两种监督信号，其作用分别为增加类间距离和减少类内距离，从而得到更加有效的人脸特征表示。在人脸识别流程中的最后一个环节，我们需要将提取的不同人脸特征进行相似性比较，这种度量的方式包括计算不同人脸特征的联合贝叶斯概率值、余弦距离和欧氏距离等，往往针对不同的方式，我们会设置不同的阈值判定是否属于同一张人脸。

# 1.3论文的主要研究内容

本论文基于四川新网银行股份有限公司新一代人工智能专项智能金融风险管控项目（项目编号：2018GZDZX0042），属于智能金融风险控制业务支撑关键技术研究子模块。目的是基于人脸生物特征对真假鉴别和人脸识别两大任务进行研究，从而构建一个基于人脸的身份验证方案用于支撑金融机构的风险决策。论文的主要研究内容如下：

（1）对本课题的研究意义和现状进行分析，同时对本课题中所涉及的一些基础理论知识、网络模型结构和两大任务使用到的评价指标进行了梳理和总结。

（2）在原有的人脸识别任务流程基础上，本文设计了AgeGAN生成对抗网络模型模拟不同年龄阶段的人脸图像进行数据增强，并在模型的预测阶段使用主成分分析法对特征向量进行降维。这两点改进带来了如下的影响：第一，通过AgeGAN 模型模拟不同年龄阶段的人脸图像来实现数据增强，这种数据增强方式成倍的扩充了样本数量，有效地缓解了模型的过拟合问题，并在一定程度上可以削弱年龄因素对模型准确率的影响；第二，采用主成分分析法对提取到的特征向量进行压缩，主成分分析法可以将高维空间的数据投影到低维空间中，并且使数据间的方差尽量大，这样可以保持数据特征值在相对关系不变的情况下有效降低特征维度。最后通过一系列的对比实验验证上述方案的有效性。

（3）提出了一种基于神经网络特征热图的人脸真假鉴别方案。本文在基线模型MesoNet网络的基础上提出三点改进：第一，人脸数据在输入到VGG网络后，从VGG网络的卷积层获取合适的特征热图来辅助模型训练；第二，通过在模型中引入空洞卷积，非对称卷积和全局平均池化层来降低模型的参数量实现网络结构的轻量化；第三，使用FocalLoss作为模型的损失函数进行反向传播来优化网络参数。最终本文通过实验验证了本方案的优越性。

（4）基于PythonFlask轻量级框架设计并实现了一个基于人脸的身份认证系统。系统主要包含信息录入和身份认证功能模块，对人脸识别和真假鉴别任务进行了展示和应用。

# 1.4论文的结构安排

本论文对人脸图像的真伪鉴别和人脸识别相关研究内容的论述一共由六个章节组成，每个章节安排如下：

第一章主要介绍了使用深度学习相关技术设计一个基于人脸数据的金融反欺诈身份验证方案的研究背景和重要现实意义，然后针对人脸编辑和生成、人脸真伪鉴别以及人脸识别三方面内容介绍了国内外的研究现状。最后梳理了论文的主要研究内容和对每个章节的具体安排。

第二章阐述了本课题所涉及的一些基础理论知识、网络模型结构和图像生成技术。首先从卷积层、池化层和全连接层三个方面对卷积神经网络展开了叙述，同时还介绍了本文中使用到的网络模型。最后对目前主流的图像生成技术以及模型评价指标进行了介绍。

第三章主要解决人脸识别问题，本文在原有人脸识别算法流程的基础上进行了改进。通过生成对抗网络模型实现人脸跨年龄数据增强，以及主成分分析法进行特征压缩。本章首先对该方案中所涉及到的人脸识别基本流程、三元组损失函数、主成分分析法和图像增强等相关理论知识进行了概述。接着介绍了特征提取模块所使用的网络模型和主成分分析法的作用，同时设计了AgeGAN生成模型来模拟不同年龄阶段人脸图像。最后介绍了实验所使用数据集、预处理方式、训练过程和实验环境，并对实验结果进行了分析。

第四章提出了一种基于神经网络特征热图辅助分类的人脸真假鉴别方案。首先介绍了该方案基于MesoNet模型的三点改进的相关理论知识，包括特征热图、FocalLoss损失函数以及模型的轻量化设计等。接着阐述了基于特征热图的人脸真假鉴别模型架构和算法流程；最后介绍了实验的相关设置和对实验结果的分析。

第五章基于PythonFlask 轻量级框架设计与实现了一个基于人脸的身份认证系统，该系统是对前两章提出的人脸真假鉴别和人脸识别算法的实际应用。

第六章对全文的内容进行了总结，并进一步展望了人脸真假鉴别领域的研究思路。

# 第二章 基础理论和相关技术

# 2.1卷积神经网络

上世纪60年代Hubel和Wiesel等人对猫初级视皮层中的神经元进行了研究，并首次公开提出了感受野和双目视觉等相关概念，在此基础上，到80年代科学家Fukushima提出了层级化的多层神经认知模型[41]。1998年YannLeCun等人基于梯度学习提出了卷积神经网络架构LeNet- $\cdot 5 ^ { [ 4 2 ] }$ ，并且以低于 $1 \%$ 的错误率在手写数字字符识别任务上得到成功的应用，为未来卷积神经网络的发展打下了坚实的基础。卷积神经网（CNN）作为一种特殊的人工神经网络，其基础结构也是由具有学习权重和偏置能力的神经元构成[43]。随着CNN研究取得的巨大成效，不仅被广泛用于处理图像的分类、检测、分割等计算机视觉问题，还在文本分类中得到了应用。图 2-1是CNN常见的模型结构，主要包括卷积层、池化层、全连接层和 Softmax四个部分，输入图像经过多层交替卷积层和池化层处理后提取得到特征图像，该特征图像在全连接层进行局部关联和数据压缩操作后输出，最后再使用Softmax 回归甚至支持向量机模型实现图像分类。下面将对这些组成部件进行具体的介绍。

![](images/ac92e3df6313344d92256d283f88c455a2771b7e83446db9911a2bab3461efa3.jpg)  
图2-1卷积神经网络结构

# 2.1.1 卷积层

卷积层作为CNN中的基本操作，其目的在于提取图像的特征。在进行卷积操作时要设置好卷积核大小、卷积核每次移动的距离、填充值和卷积核的深度等参数值的大小。从函数映射的角度，卷积计算的过程就是将卷积核中的参数与图像中对应位置的像素进行点积运算实现线性变化映射成新值，因此针对多层卷积可以看作逐层映射使整体上构成一个复杂的函数，训练的过程是函数的拟合过程，学习每个局部映射所需的权重。图2-2展示了三通道彩色图片的卷积计算过程。

![](images/853eb632dba4753f82b8864eb9699aa7a24e00e23aa634991558017fe6e7d1d6.jpg)  
图2-2图像卷积计算

在图2-2中，第一列为含有三个通道的输入数据，第二列和第三列为两个神经元对应三个通道的卷积核权重系数，卷积核每次从上到下、从左到右移动的步长为2，输入数据周围有一层填充值，第四列对应的是两个神经元卷积操作后得到的值。图中第一列的三个窗口中的数据和FilterW0的计算过程为：

第一个窗口： $0 \mathbf { x } 0 + 0 \mathbf { x } 0 + 0 \mathbf { x } ( - 1 ) + 0 \mathbf { x } 1 + 1 \mathbf { x } 0 + 2 \mathbf { x } 0 + 0 \mathbf { x } 1 + 1 \mathbf { x } 0 + 0 \mathbf { x } 1 = 0 ;$ 第二个窗口： $0 \mathbf { x } 0 + 0 \mathbf { x } 1 + 0 \mathbf { x } ( - 1 ) + 0 \mathbf { x } ( - 1 ) + 0 \mathbf { x } 1 + 2 \mathbf { x } ( - 1 ) + 0 \mathbf { x } 0 + 2 \mathbf { x } ( - 1 ) + 0 \mathbf { x } 2 = - 4$ ：第三个窗口： $0 \mathbf { x } 0 + 0 \mathbf { x } 1 + 0 \mathbf { x } ( - 1 ) + 0 \mathbf { x } 1 + 0 \mathbf { x } ( - 1 ) + 1 \mathbf { x } 0 + 0 \mathbf { x } 0 + 2 \mathbf { x } 0 + 0 \mathbf { x } ( - 1 ) = 0 ;$

最后，将以上三步操作的结果加上偏置量的值一起求和就是第四列中第一个输出矩阵中的第一个值： $0 + \ ( - 4 ) \ + 0 + 1 = - 3$ 。

如果卷积层的输入张量为 $x ^ { l }$ ，大小满足 $H ^ { l } \times W ^ { l } \times D ^ { l }$ ，该层对应的卷积核表示为 $f ^ { l }$ ，代表着 $D ^ { l }$ 个大小为 $H \times W$ 的卷积核，最终将一次卷积操作所涉及到的所有$H \times W \times D ^ { l }$ 元素求和作为本次卷积的结果。当采用 $f ^ { ' }$ 卷积核的数量为 $\mathbf { D }$ 时，在同一个位置的卷积输出维度为 $1 \times 1 \times 1 \times \mathrm { D }$ ，卷积操作的表达式为（2-1）。

$$
\begin{array} { r } { y _ { i } ^ { l + 1 } , j ^ { l + 1 } , d = \underset { i = 0 } { \overset { H } { \sum } } \underset { j = 0 } { \overset { W } { \sum } } \underset { d ^ { l } = 0 } { \overset { D ^ { l } } { \sum } } f _ { i , j , d ^ { l } } \times \pmb { x } _ { i ^ { l + 1 } + i , j ^ { l + 1 } + j , d ^ { l } } ^ { l } } \end{array}
$$

局部感受野和权值共享是卷积神经网络的两大核心思想，不仅有助于减少网络参数量，还能促使网络结构在位移、尺度、缩放等方面具有一定程度上的稳定性。所谓局部感受野是指卷积层的每个神经元仅和前一层的节点进行局部连接，提取局部的特征，然后在高层综合不同神经元学习到的局部信息得到全局信息；由于在CNN 中不同的卷积核具有不同的权重参数值，权值共享的实质是位于同一层中的任何神经元都可通过相同的卷积核在输入图像的不同位置获取相同的特征信息。

# 2.1.2 池化层

池化层作为CNN中的一个重要概念，其实质是一种降采样操作形式，该操作的引入源于生物视觉系统对于输入对象的降维和抽象方式。池化操作后所得结果中的一个元素和输入信息中的某个子区域相对应，这就相当于从空间范围上实现了维度约减，网络模型也可以获得范围更广的特征。同时下一层输入数据大小的减少，会使得该层参数数量和计算量减少，从而在一定程度上控制了过拟合现象的发生。实际应用中我们可以选择多种非线性池化函数，最常见的是最大池化和平均值池化，两者都将输入的数据划分为若干个子区域，最大池化是输出每个子区域的最大值，平均值池化是将每个子区域对应数值求取平均值后作为输出。在进行池化操作时，由于没有需要学习的参数，我们只需要选择合适池化函数、对核的大小和移动步长进行设置。图2-3是最大池化的过程示意图，步长为2，相当于进行池化操作的图像数据每隔2个元素划分一块子区域，然后选择4个数中的最大值作为该区域的输出，最终实现减少 $7 5 \%$ 的数据量。

![](images/66813c5587a7232e2635cb4cd28a679af43603ac574fcfb48717088cb582e91d.jpg)  
图2-3最大池化过程示意图

# 2.1.3全连接层

如果卷积层、池化层等操作是实现从原始数据到隐层特征空间的映射，那么全连接层的目的则是将前几层学习到的特征表示映射到样本标记空间。在卷积神经网络中，全连接层相当于实现“分类器”的功能，其作用是将连续多层卷积和池化操作处理后提取的特征进行整合，映射成一个固定长度且包含高层含义图像特征的向量，以此来完成图像分类的任务。全连接层通常出现在CNN网络的最后几层，该层的每一个神经元会与上一层中的所有神经元建立联系，进行局部特征的加权求和，实现一个空间到另一个空间的线性变换。图2-4是一个全连接层的示意图，在经过前两步的卷积和池化操作后，得到了20个 $1 2 \times 1 2$ 的特征图，经过一个全连接层后输出 $1 \times 1 0 0$ 的特征向量，其实质就是对于输入全连接层的20个 $1 2 \times 1 2$ 的特征图，每个特征图都使用 $1 2 \times 1 2$ 的卷积核进行处理后变成1个数，然后进行加权求和，由于采用全连接的方式，在计算的过程中总共需要 $1 0 0 \times 2 0$ 个 $1 2 \times 1 2$ 大小的卷积核，这样全连接层的参数个数为 $1 0 0 \times 2 0 \times 1 2 \times 1 2$ 。所以，在实际使用过程中，全连接层的功能可以通过使用卷积操作实现：当前层是全连接的全连接层，通常使用 $1 \times 1$ 的卷积核进行卷积操作；而当前层是卷积层的全连接层，则选定卷积核为 $h \times w$ 进行全局卷积（其中h和w分别代表前层卷积输出图片的高和宽）。

![](images/7a4d1455086eb1ad156edb0d5ae0060338083e21a1651ed27fdff4e9713d84ba.jpg)  
图2-4全连接层示意图

# 2.1.4损失函数

损失函数，也可称作代价函数，是一种将随机事件映射为表示与该事件相关的“风险”或“损失”的非负实数的函数[44]。在深度学习应用中，我们通常会针对目标任务选择一个半正定的函数作为神经网络的损失函数，用于衡量网络输出和实际输出之间的误差。并以此作为学习准则，在训练的过程中通过使用梯度下降等优化算法实现神经网络参数权重值的更新以实现损失函数最小化。大量的研究结果表明，损失函数的选择会对模型的性能产生一定的影响，并与网络结构的鲁棒性呈现负相关，即如果损失函数的值越小，就意味着模型的鲁棒性越强。

从学习任务的类型角度出发，我们从广义上通常可以将损失函数分为回归损失和分类损失两大类。所谓的回归任务主要涉及连续值的预测问题，输出为连续型变量，是一种定量输出。例如：根据输入的房屋价格、数量和大小等数据，对该房屋的价格做出预测。该类问题中常见的损失函数为均方误差、平均绝对值误差等。

（1）均方误差（MSE）：又称为L2损失，是回归任务中最常使用的损失函数，计算方法是求取模型预测值和目标值之间差值平方的均值，表达式（2-2）是对应的数学公式。MSE是一种光滑函数，能够用梯度下降法进行优化，具有较好的收敛效果。但是对于预测值和真实值差距较大的异常点，会赋予较大的权重从而加大惩罚力度，这时如果该异常点不属于考虑范围，就会让损失函数的指导方向出现一定的偏差。

$$
M S E = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } { \left( y _ { i } - \hat { y } _ { i } \right) ^ { 2 } }
$$

（2）平均绝对值误差（MAE）：又称为L1损失，计算方式为求取模型预测值和目标值之间绝对差之和的平均值，表达式（2-3）是对应的数学公式。这种损失函数由于不使用平方，对异常值具有较好的鲁棒性，但是可能出现更新梯度始终相同的严重问题，即使损失值很小，函数的梯度也会很大，不利于模型的学习收敛，通常使用变化学习率来解决这个缺陷。

$$
M A E = \frac { 1 } { n } \sum _ { i = 1 } ^ { n } \left| y _ { i } - \hat { y } _ { i } \right|
$$

而分类任务则是指根据输入数据从类别值有限的数据集中判断所属的类别，分为二分类和多分类问题。此类任务的输出为离散型变量，用于指定所属类别，是一种定性输出。在分类问题中常见的损失函数为负对数似然损失、交叉熵损失以及指数损失等，其中交叉熵损失是最常使用的损失函数。1948年香农提出的“信息熵”概念解决了信息的量化度量问题，信息的不确定性越强，用来表达所需的数据量也就越多。如果使用概率分布 $P$ 来表示事件的不确定性，信息量的计算公式为$I ( x ) = - \log p ( x )$ ，信息熵则代表着所有信息量的期望，（2-4）是相关数学表达式，其中 $\mathfrak { n }$ 代表事件的所有可能性。

$$
H ( x ) = - \sum _ { i = 1 } ^ { n } p ( x _ { i } ) \log ( p ( x _ { i } ) )
$$

在机器学习中，如果同一个随机变量X拥有 $p ( \mathbf { x } )$ 和 $q ( \mathbf { x } )$ 两个相互独立的概率

分布，分别描述模型的真实分布和预测分布，我们通常会使用相对熵（KL散度）来对两个概率分布之间的差异性进行衡量，数学表达式如（2-5）所示。KL散度值越小， $p ( \mathbf { x } )$ 和 $q ( \mathbf { x } )$ 两个分布则越接近。

$$
D _ { K L } ( p \| q ) { = } \sum _ { i = 1 } ^ { n } p ( x _ { i } ) \mathrm { l o g } { \left( { \frac { p ( x _ { i } ) } { q ( x _ { i } ) } } \right) }
$$

对公式（2-5）进行进一步的推导得到公式（2-6），式中前后两部分分别代表信息熵和交叉熵。以此可以得出相对熵 $\dot { \bf \Phi } = \dot { \bf \Phi }$ 交叉熵-信息熵的结论。信息熵由于是对消除真实分布p不确定性所需信息量的度量，该值往往是最小而且固定的。因此，为了通过优化减小相对熵从而使相关模型在大量训练数据上学习到的预测分布和真实的数据分布更加接近，通常为了简化计算会直接优化交叉熵来达到同样的目标。并且在分类问题中，交叉熵常与 softmax 函数结合使用，softmax 对输出结果进行处理后使多个分类的预测值和为1，然后再通过交叉计算损失。

$$
\begin{array} { l } { \displaystyle { D _ { \scriptscriptstyle { K L } } ( p \| q ) = \sum _ { i = 1 } ^ { n } p ( x _ { i } ) \mathrm { l o g } \Bigg [ \frac { p ( x _ { i } ) } { q ( x _ { i } ) } \Bigg ] } } \\ { \displaystyle { \quad \quad = \sum _ { i = 1 } ^ { n } p ( x _ { i } ) \mathrm { l o g } \big ( p ( x _ { i } ) \big ) - \sum _ { i = 1 } ^ { n } p ( x _ { i } ) \mathrm { l o g } \big ( q ( x _ { i } ) \big ) } } \\ { \displaystyle { \quad \quad = - H \big ( p ( x ) \big ) + \left[ - \sum _ { i = 1 } ^ { n } p ( x _ { i } ) \mathrm { l o g } \big ( q ( x _ { i } ) \big ) \right] } } \end{array}
$$

# 2.2卷积神经网络模型

# 2.2.1VGG

牛津大学Visual Geometry Group 组提出的VGG 模型[45]主要使用了连续的多个 $3 \times 3$ 的小卷积核替换了Alex-Net 中 $1 1 \times 1 1$ 和 $7 \times 7$ 的较大卷积核，这样的改进一方面能够在保证相同感受野的前提下，通过提升网络深度，进行更多的非线性映射，提升网络的学习能力；另一方面减少了网络的参数量，比如使用3个 $3 \times 3$ 大小的卷积核替换一个 $7 \times 7$ 的卷积核的处理效果，如果使用 $C _ { i n }$ 和 $C _ { o u t }$ 来分别表示卷积层输入和输出对应的通道数，3个 $3 \times 3$ 卷积核堆积的参数量为$3 \times \left( 3 \times 3 \times C _ { o u t } \times C _ { i n } \right)$ ，而 $7 \times 7$ 卷积核对应的参数量为 $7 \times 7 \times C _ { o u t } \times C _ { i n }$ 。VGG在 2014年的ImageNet竞赛的分类任务上获得了亚军，证明了网络深度的增加可以对网络的最终性能产生一定的影响。VGG16和VGG19是VGG网络的两种主要结构，唯一的区别在于网络的深度不同，当向VGG16网络中输入维度为 $2 2 4 \times 2 2 4 \times 3$ 的彩色图像时，图2-5展示了不同层的处理过程。

eu 92223004 590 8235239 333233 3553333 9255335 335333 ￥ 3325332 > 3325332 332332 → 332332 3325332 3325232 9604 9604 000I AuoO CuoO UoO AuoO AUOO AUoO CUo AUo AUoO AUoO UoO AUoO A D

# 2.2.2 ResNet

传统的神经网络随着网络深度的加深，网络的训练和优化工作会变得更加的困难，并且准确率在达到一定的饱和点后也会急剧退化。针对这种现象，微软研究院的四名华人研究员提出了深度为152层的ResNet[4残差神经网络，该网络结构在2015年的ImageNet竞赛中凭借 $3 . 5 7 \%$ 的错误率取得了第一名。其主要思想为：以一定比例保留前面网络层的输出，将其绕道传送到后面的层中直接作为输出，从而保护信息的完整性，并且整个网络的学习目标仅仅是输入与输出之前的差异。残差学习的基本模块如图2-6所示。

在实际的结构设计中，我们有时会希望通过一层或多层去模拟一个恒等的映射函数，但是这样的做法本身往往存在一定的难度。在图中的残差模块中，将输入输出分别表示为X和 $H ( X )$ ，那么该图表示的函数关系为 $H ( X ) = F { \bigl ( } X { \bigr ) } + X$ ，同时网络的学习目标转化为 $F \left( X \right) = H \left( X \right) - X$ 。

![](images/9738514f410c3ec7a6d3bf313abedb2af93a45f46395d8a812784bc7bbbe33f6.jpg)  
图2-5VGG16网络结构图  
图2-6残差学习基本模块图

# 2.3图像生成技术

随着深度神经网络的迅速发展，图像生成领域也取得了一些突破性的进展。基于流的生成模型Glow[47]、生成对抗网络GAN 和变分自动编码器VAE[48]是目前比较主流的生成网络模型，下面将对这些模型展开详细说明。

# 2.3.1基于流的生成模型

Glow也是一种可逆生成模型，这种网络结构可以在使用相对较少的数据情况下，快速合成高清逼真的图像；同时还可以利用该模型学习到的潜在表征对样本数据的属性特征进行操作。图2-7展示了Glow的网络结构图，其中图2-7（a）是对图 2-7（b）中“step of flow”的具体展开。Glow 模型的核心思想是要找到一个可逆双射来完成输入和潜在空间之间的相互转换，这个映射函数则采用分层变换的设计思想，一个完整的流由K个单步流组成，在对完整流经过split处理后进行多尺度循环，经过 $L - 1$ 次循环后得到目标潜变量 $Z { = } \left( Z ^ { 1 } , \ Z ^ { 2 } , { \cdots } , \ Z ^ { L } \right)$ 。在一个单步流中，“Actnorm”是第一个处理层，其主要作用是对输入的处理数据进行预处理操作，通过使用每个通道对应的标度和偏差参数执行激活的仿射变换，和批量标准化具有一定的相似性。接着进入 $1 \times 1$ 可逆卷积层，通过矩阵的简化计算降低整体的计算量。最后进入实现可逆转换的关键层“AffineCouplingLayers”，该层主要由零初始化、拆分和连接、排列三个操作构成。

![](images/6929f4cd81d672e3202a8233a91910da553716be17e90e727f2561ed136da74a.jpg)  
图2-7Glow模型结构图 (a)单步流结构图；(b)多尺度循环结构图

# 2.3.2生成对抗网络

2014年IanGoodfellow等人提出的生成对抗网络模型GANs引起了广泛的关注，并迅速成为了人工智能领域的一个研究热点。GAN不但有广泛的应用场景，可以适用于图像、文本和音频等各种数据类型，而且在计算机视觉领域取得了突出的效果，如：图像修复、高分辨率逼真图像生成、风格迁移和图片翻译等。GAN借鉴了博弈论中的零和游戏思想，整个网络结构主要由一个生成器和一个判别器构成。其中生成器采用随机样本数据作为输入，生成新样本数据，经过不断的训练，学习真实样本数据的分布特征，最后能够生成和真实样本相似度较高的数据；而判别器的作用则是对输入数据的真假进行判断。两个模型采用对抗的方式进行不断训练，生成器最大化生成逼真样本数据的能力，判别器则将做出错误判断的可能性降到最小，最终拟合数据样本的内在分布并且能够生成新的样本数据。GAN的网络架构如图2-8所示。

![](images/986eb7dc17446dc8bd348cb1c75d4fa84748d2907fd5671618d199afe5d95453.jpg)  
图2-8GAN网络架构图

在图中，生成器G接收一个随机噪声数据 $z$ （服从先验分布 $P _ { z } \left( z \right)$ ），经过处理将生成样本数据 $G ( z )$ 输出；真实数据 $\mathbf { x }$ （服从真实数据分布 $P _ { d a t a } ( x )$ ）或者生成的假样本 $G ( z )$ 输入到判别器网络D进行真假判断，真实数据输出1，反之输出0。生成器G尽可能生成以假乱真的数据对判别器D进行欺骗，而判别器 $\mathbf { D }$ 则尽量提高真假判断的能力，这样就形成了一个极大极小的动态博弈训练过程。表达式（2-7)是GAN的目标函数：

$$
\operatorname* { m i n } _ { G } \operatorname* { m a x } _ { D } V ( D , G ) = E _ { x \sim P _ { d a c } x } [ \log D ( x ) ] + E _ { z \sim P _ { z } z } [ \log \left( 1 - D ( G ( z ) ) \right) ]
$$

GAN的训练包括两个阶段：第一阶段，将生成器 $\mathbf { G }$ 固定，对判别器D进行优化得到 $\tilde { D }$ ；第二阶段，将D 设定为最优判别器 $\tilde { D }$ ，对G 进行优化得到最优模型。第一阶段，固定G，优化D：判别器的输入可以是真实样本 $\mathbf { x }$ 或者生成假数据 $G ( z )$ 。当输入为 $\mathbf { x }$ 时，希望预测值尽可能接近1，输入为真实数据的概率 $D ( x )$ 越大越好;如果输入是 $G ( z )$ ，则希望输出结果尽量为0， $D \left( G ( z ) \right)$ 越小越好，也就是 $1 - D \big ( G ( z ) \big )$ 越大越好，所以判别器 $\mathbf { D }$ 的优化是一个极大化过程。第二阶段，固定D，优化G：输入为随机噪声数据 $z$ ，此时希望D 对假样本数据 $G ( z )$ 的预测标签为1，也就是$D \big ( G ( z ) \big )$ 尽可能趋近1， $\left. 1 - D \big ( G ( z ) \big ) \right.$ 趋近0，所以生成器G的优化是一个极小化过程。

# 2.3.3 变分自动编码器

VAE是结合了深度学习和统计学习观点的另外一种生成模型，其基于自动编码器，在隐藏层中加入了一个隐藏向量Z以实现自动生成数据。现在我们提出一些原则：解码器 $P _ { \theta } \big ( \boldsymbol { X } | Z \big )$ 用于从潜在变量 $Z$ 重构原始数据 X；通常被假定为正态分布的后验分布 $P _ { \theta } { \big ( } Z | X { \big ) }$ 从原始数据 X 学习到的压缩 $Z$ 的表示形式；编码器$q _ { \varphi } \mathopen { } \mathclose \bgroup \left( Z \mid X \aftergroup \egroup \right)$ 用于近似后验分布 $P _ { \theta } \mathbf { ( } Z \mid X )$ 。主要思想如下：首先向编码器中随机输入一个原始数据X 得到相应的分布 $q _ { \varphi } \mathopen { } \mathclose \bgroup \left( Z \mid X \aftergroup \egroup \right)$ ，同时对 $q _ { \varphi } \mathopen { } \mathclose \bgroup \left( Z \mid X \aftergroup \egroup \right)$ 进行随机采样得到向量Z，通过KL散度来衡量 $q _ { \varphi } \mathopen { } \mathclose \bgroup \left( Z \mid X \aftergroup \egroup \right)$ 和 $P _ { \theta } \big ( Z | X \big )$ 两个分布之间的相似性；然后将Z输入到解码器中生成数据，使用重构误差来代表生成数据和原始数据之间的差异性。图2-9是具体的模型示意图。

![](images/fb8ae39d2b2f8a0ff66b50eb90151860b6b47ce1c2a7d53b8b1536b8d55f6bd3.jpg)  
图2-9VAE网络架构图

对于样本数量为 $\mathfrak { n }$ 的数据集 $X$ ，其中每一个样本 $x ^ { i }$ 均服从同一分布。边际似然是单个数据点的边际似然之和，（2-8）展示了单个数据点的边际似然计算表达式，式子中右边第一项为KL散度，第二项为变分下界。为了达到优化边界似然的目的，对公式中的变分下界进行改变得到（2-9)，式子中的右边两项构成了SGVB,分别代表正则化和重构损失，实现了与反向传播算法相结合的变分编码器的优化。

$$
\log P _ { \theta } \big ( x ^ { i } \big ) { = } D _ { k l } \big ( z ^ { i } \mid x ^ { i } \big ) \| P _ { \theta } \big ( z ^ { i } \mid x ^ { i } \big ) { + } L \big ( \theta , \varphi ; x ^ { i } \big )
$$

$$
\begin{array} { c } { { \log P _ { \theta } \Big ( x ^ { ( i ) } \Big ) \geq L \Big ( \theta , \varphi ; x ^ { ( i ) } \Big ) { = } - D \kappa \Big ( q \varphi \Big ( Z | x ^ { ( i ) } \Big ) | | P _ { \theta } \Big ( Z \Big ) \Big ) } } \\ { { { } } } \\ { { + E q \varphi ( z ^ { ( i ) } \mid x ^ { ( i ) } ) \Big [ \log P _ { \theta } \Big ( x ^ { ( i ) } \mid z ^ { ( i ) } \Big ) \Big ] } } \end{array}
$$

# 2.4模型评价指标

在完成模型的搭建工作后，我们还需要选用适合的评价指标来评估模型性能表现，并根据相关评价指标的结果对模型的结构和相关参数等做出一定的调整，最终通过训练得到相对成功的模型。同时，对于一个神经网络我们应该采用不同的度量指标从不同的方面去判断其性能的好坏。下面对本文中选用的网络性能评价指标进行简要介绍：

（1）准确率（Accuracy）：被正确预测的正样本和负样本数在所有样本中所占的比例，反映了分类器对整个样本的预测能力。计算公式见（2-10）：

$$
A c c u r a c y = { \frac { T P + T N } { T P + F P + T N + F N } }
$$

（2）精确率（Precision）：通俗地讲，就是在所有预测为正样本的结果中，模型预测正确所占的比重。计算公式见（2-11）：

$$
P r e s i c i o n { = } \frac { T P } { T P + F P }
$$

（3）召回率（Recall）：通俗地讲，就是被正确预测的正样本数在所有真实正样本数中所占的比例。计算公式见（2-12）：

$$
{ \mathrm { R e } } c a l l = { \frac { T P } { T P + F N } }
$$

（4）F1值：F1值同时受到Precision和Recall的影响，取值区间为[O,1]，1代表了模型的输出最好，0则代表了最差的模型输出。计算公式见（2-13）：

$$
F 1 = \frac { 2 T P } { 2 T P + F P + F N }
$$

（5）ROC曲线：通常用于对一个分类器的性能评价。横轴和纵轴分别使用“假正例率”和“真正例率”进行表示，如果面积越大则分类的性能越好。

在上述几个评价指标的表达式中出现了TP、TN、FP和FN四个特殊符号，它

们分别代表了不同的含义。我们借助二分类问题中常见的混淆矩阵（如表2-1）进行说明。TP，实际为正预测也为正；TN，实际为负预测也为负；FP，实际为负但预测为正；FN，实际为正但预测为负。

表2-1二分类结果混淆矩阵  

<html><body><table><tr><td rowspan="4">真 实 类 别</td><td colspan="4">预测类别</td></tr><tr><td></td><td>1（正）</td><td>0（负）</td><td>合计</td></tr><tr><td>1(正)</td><td>TP</td><td>FN</td><td>实际为正TP+FN</td></tr><tr><td>0（负）</td><td>FP</td><td>TN</td><td>实际为负FP+TN</td></tr><tr><td>合计</td><td>预测为正TP+FP</td><td>预测为负TN+FN</td><td>TP+FP+FN+TN</td></tr></table></body></html>

# 2.5本章小结

本章主要介绍了课题所涉及的一些基础理论知识。首先从卷积层、池化层和全连接层三个方面对卷积神经网络展开了介绍，并介绍了回归和分类任务中常见的损失函数。接着阐述了论文中所使用到的VGGNet和ResNet网络结构。然后叙述了目前比较主流的生成网络模型：基于流的生成模型Glow、生成对抗网络GAN 和变分自动编码器VAE。最后对本文中选用的网络性能评价指标进行了介绍。

# 第三章 基于跨年龄数据增强和特征压缩的人脸识别

# 3.1 引言

随着深度学习技术的迅猛发展，人脸识别算法在众多开源数据集上已经实现了较高准确率，并且在安防和支付等领域拥有广泛的应用场景。但是，该项技术仍然面临着诸多的挑战。例如：识别的结果往往会受到人体姿态、光照强度、面部表情等诸多因素的影响，特别是随着年龄的不断增长，人的外貌特征在形状和纹理上都会发生一些显著的变化，这些给人脸识别带来了新的挑战。此外，利用深度学习技术建立的人脸识别系统依赖大量数据的训练和学习，这样会导致数据的体量和维度呈指数级的增长，过高的数据维度对算法的计算效率和分类性能都会造成影响，因此有必要在模型的部署阶段对特征向量进行压缩来实现特征降维，通过这种方式降低数据处理的复杂度，提高处理效率。

人脸识别的基本流程包括人脸检测、数据预处理、特征提取和特征匹配等模块，本文通过数据增强和特征压缩的方式对人脸识别技术进行了研究。在数据增强方面，通过我们设计的AgeGAN生成对抗网络生成不同年龄阶段的人脸图像来实现数据增强，这种数据增强方式能够成倍的扩充人脸样本，缓解模型的过拟合问题。在特征压缩方面，我们采用主成分分析法对模型提取到的人脸特征向量进行压缩，主成分分析法可以将高维空间的数据投影到低维空间中，并且使数据间的方差尽量大，这样可以使数据的特征值在保持相对位置关系不变的情况下降低特征维度。最后，我们分别在VGG16和ResNet18两种特征提取网络上验证了数据增强和特征压缩方法的有效性，我们不仅能够有效提高人脸识别模型的准确率，还能降低预测阶段的时间消耗。

# 3.2改进点介绍

# 3.2.1人脸识别基本流程

人脸识别的目的在于通过人脸的面部特征信息实现身份检测和验证，一个基本的人脸识别系统通常包括人脸检测、数据预处理、特征提取和特征匹配四个主要模块。为了提升人脸识别模型的性能表现，本文在此基础上添加了数据增强和特征压缩模块，具体的人脸识别流程如图3-1所示。下面对所涉及的模块进行介绍。

（1）人脸检测模块。人脸检测在整个识别流程中占有重要的地位，其主要任务是判断输入的待处理图像中是否存在人脸，如果存在则返回对应的位置、姿态和大小等信息。如果一张图片中同时出现多张人脸，则将检测到的面积最大的人脸区域作为与标签对应的目标区域。

（2）数据增强模块：本文除了采用一些常见的数据增强方式，如水平镜像、颜色变换、高斯模糊和添加高斯噪声等，还采用生成对抗网络模型来模拟生成不同年龄阶段的人脸图像。以此来扩充数据集中样本的数量和多样性，避免由于过拟合影响网络的效果。

（3）数据预处理模块。这里的数据预处理主要包括人脸对齐。通过人脸对齐可以校正眼睛、嘴巴、鼻子和眉毛等面部关键特征点的位置，有利于模型对人脸关键特征的提取。

（4）特征提取和压缩模块。模块的主要功能是通过深度学习模型对人脸图像提取特征向量，然后对所获取的特征向量通过主成分分析法实现特征精简，这样可以保证算法准确率的同时，减少特征匹配阶段的时间消耗。

（5）特征匹配模块。将人脸图像经过特征提取得到的向量表示进行相似性度量。常见的相似性度量方式有相关距离、余弦距离和欧氏距离等，针对不同的方式会基于模型在验证集上的表现来设置不同的阈值，从而判定是否属于同一人脸。

![](images/b0bab854bff2f28aa23993fc363b1fa85fec88157e1bd08ae7408f480e1a9945.jpg)  
图3-1人脸识别的具体流程图

# 3.2.2图像数据增强

在卷积神经网络的训练过程中，数据集中样本的数量和多样性会在一定程度上对模型的性能表现产生一定的影响。而图像增强的目的则在于通过某种变换算法加强整体或局部特征从而产生新的样本数据。

常见的数据增强方式主要采用水平镜像、颜色变换、高斯模糊和添加高斯噪声等图像变换算法来扩充样本数量。水平镜像，利用图像的垂直对称性将图像的左右两部分进行镜像对换。颜色变换，是指对人脸图像的饱和度、色调和曝光度进行随机的调整生成不同光照和颜色状态下的图片。高斯模糊，我们可以将所谓的“模糊”理解为每一个像素的取值为周边像素的平均值；在具体实现时，由于图像具有一定的连续性，我们采用加权平均的方式，距离越近赋予的权重值越大。添加高斯噪声，高斯噪声是指满足高斯正态分布的误差，由于我们的神经网络可能学习到大量无用的高频特征，这时我们在样本数据中适量添加零均值的高斯噪声能增强模型的学习能力。图3-2展示了常见增强方式的处理效果。

![](images/2d2e9006bc96980c72de392ddeb792a186a63bec8e94c7651aefb7ec4c59a928.jpg)  
图3-2基于图像变换的数据增强效果图

生成对抗网络GAN及其相关变体模型通过从数据集中提取高维度特征来合成图像数据，这种增强方式能够避免生成的图像和原始数据集中的样本存在大量的相似特征，出现信息冗余从而影响网络的训练过程。本文中采用生成对抗网络来模拟不同年龄阶段的人脸图像，不仅可以实现数据集中样本数量的扩充，还可以让网络模型在训练的过程中学习到更多关于人脸图像的年龄特征信息，从而在一定程度上削弱年龄因素对人脸识别准确率的影响。

# 3.2.3主成分分析法

在对实际问题进行研究时，通常需要对具有很多特征变量的数据进行观测和分析，如果对每个变量进行独立的分析，不但不能充分利用数据中提供的信息，还会产生巨大的工作量。但是往往变量之间可能会存在一定的相关性，我们可以把这几个变量综合为一个变量，最终实现使用少量的综合指标来代表所有变量中存在的信息，即降维思想。对于本文的人脸识别任务，我们将使用主成分分析法PCA对特征提取模块处理后所得到的人脸表征向量进行降维，消除一些冗余的背景信息，保留关键特征点的分布信息作为最终分类的依据。

主成分分析法PCA的主要思想是在原有样本的n维特征空间内通过正交变换后得到 $\mathbf { k }$ 维的线性空间，即主成分。图3-3是PCA将二维数据映射至一维空间的示意图，数据集中的数据样本都具有两个维度，假定每个维度服从正态分布，将数据在标准二维坐标系X-Y中表示，就会形成近似椭圆的点阵。在椭圆中有两条相互垂直的长轴PC1和短轴PC2，将数据投射到PC2上，很多点会聚集产生重合，数据表示的很多信息不能得到充分利用；相反，如果投射到PC1上，数据比较分散，呈现出较大的变化；在PC2退化为一个点得极端情况下，可以使用PC1所代表的基向量解释数据中的所有变化。推广到具有多维变量的数据，数据集的观测形状类似于一个高维椭球，我们需要从高维椭球的所有轴中找出能代表数据大部分信息的最长k个轴作为新变量（主成分），这些变量之间满足相互垂直。最终我们可以得到相同特征之间的方差值越大越好，不同特征之间的协方差越小越好。

![](images/68312cfd58f2a5b6e93ee5449dbd69297793392ce36298c9a6279a8cfc052ecc.jpg)  
图3-3PCA二维数据降维示意图[49]

实际计算中，对于样本数量为 $\mathfrak { m }$ ，每个样本具有 $\mathfrak { n }$ 维特征的数据集，使用矩阵$X \in R ^ { n \times m }$ 表示，其中每列 $x _ { i } \in R ^ { n \times 1 }$ 代表一个样本，每一行则为一个特征，具体表示如（3-1）所示。对样本数据 $\mathrm { \Delta } \mathrm { X } \mathrm { \Omega }$ 去中心化处理得到均值 $\overline { { \boldsymbol X } }$ 。

$$
X = { \bigl ( } x _ { 1 } , x _ { 2 } , \cdots , x _ { m } { \bigr ) } = { \left( \begin{array} { l l l l } { x _ { 1 1 } } & { x _ { 2 1 } } & { \cdots } & { x _ { m 1 } } \\ { x _ { 1 2 } } & { x _ { 2 2 } } & { \cdots } & { x _ { m 2 } } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { x _ { 1 n } } & { x _ { 2 n } } & { \cdots } & { x _ { m n } } \end{array} \right) }
$$

如果我们要实现降维，将每个样本的 $\mathfrak { n }$ 维特征压缩至 $\mathbf { k }$ 维代表样本数据的主要信息，需要借助一个大小为 $k \times n$ 的矩阵 $\mathbf { A }$ 。这样 $A x _ { i } \in R ^ { k \times 1 }$ ，便实现将 $x _ { i }$ 列向量的维度降至为 $\mathbf { k }$ 。将降维后的结果表示为矩阵 $Y \in R ^ { k \times m }$ ，具体表示为（3-2）所示。为了方便，将其简化表示为（3-3）。

$$
Y = A { \big ( } X - { \overline { { X } } } { \big ) } = { \left( \begin{array} { l l l l } { a _ { 1 } { \big ( } x _ { 1 } - { \overline { { X } } } { \big ) } } & { a _ { 1 } { \big ( } x _ { 2 } - { \overline { { X } } } { \big ) } } & { \cdots } & { a _ { 1 } { \big ( } x _ { m } - { \overline { { X } } } { \big ) } { \big ) } } \\ { a _ { 2 } { \big ( } x _ { 1 } - { \overline { { X } } } { \big ) } } & { a _ { 2 } { \big ( } x _ { 2 } - { \overline { { X } } } { \big ) } } & { \cdots } & { a _ { 2 } { \big ( } x _ { m } - { \overline { { X } } } { \big ) } } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { a _ { k } { \big ( } x _ { 1 } - { \overline { { X } } } { \big ) } } & { a _ { k } { \big ( } x _ { 2 } - { \overline { { X } } } { \big ) } } & { \cdots } & { a _ { k } { \big ( } x _ { m } - { \overline { { X } } } { \big ) } } \end{array} \right) }
$$

$$
Y = { \left( \begin{array} { l } { Y _ { 1 } } \\ { Y _ { 2 } } \\ { \dots } \\ { Y _ { k } } \end{array} \right) } = { \left( \begin{array} { l l l l } { y _ { 1 1 } } & { y _ { 1 2 } } & { \dots } & { y _ { \mathrm { 1 m } } } \\ { y _ { 2 1 } } & { y _ { 2 2 } } & { \dots } & { y _ { 2 \mathrm { m } } } \\ { \vdots } & { \vdots } & { \ddots } & { \vdots } \\ { y _ { \mathrm { k 1 } } } & { y _ { \mathrm { k 2 } } } & { \dots } & { y _ { \mathrm { k m } } } \end{array} \right) }
$$

现在我们已经得到了降维后的矩阵表示Y，我们需要进行一系列的优化工作，使找到的矩阵A能够使经过变化后的样本特征内部方差最大化，特征之间的协方差最小化，首先对矩阵Y的第一个分量进行优化，优化目标是实现样本特征内部的方差最大化，公式（3-4）是 $Y _ { 1 }$ 的具体优化目标函数：

$$
\operatorname* { m a x } \sum _ { i = 1 } ^ { m } { { { \left( { { y } _ { 1 i } } - { \overline { { Y _ { 1 } } } } \right) } ^ { 2 } } }
$$

当我们将 $\overline { { Y } } _ { 1 }$ 使用（3-5）表示，优化目标就变成了（3-6）所表示的公式。

$$
\overline { { Y _ { 1 } } } = \frac { 1 } { m } \sum _ { i = 1 } ^ { m } a _ { 1 } \big ( x _ { i } - \overline { { X } } \big )
$$

$$
= { \frac { a _ { 1 } } { m } } \left( \sum _ { i = 1 } ^ { m } x _ { i } - \sum _ { i = 1 } ^ { m } { \overline { { x } } } \right) = 0
$$

$$
\begin{array} { l } { { \displaystyle \operatorname* { m a x } \sum _ { i = 1 } ^ { m } \bigl ( y _ { 1 i } - \overline { { Y } } \bigr ) ^ { 2 } = \operatorname* { m a x } \sum _ { i = 1 } ^ { m } { y _ { 1 i } } ^ { 2 } = \sum _ { i = 1 } ^ { m } \bigl ( a _ { 1 } \bigl ( x _ { i } - \overline { { X } } \bigr ) \bigr ) ^ { 2 } } } \\ { ~ = a _ { 1 } \underset { i = 1 } { \overset { m } { \sum } } \bigl [ \bigl ( x _ { i } - \overline { { X } } \bigr ) \bigl ( x _ { i } - \overline { { X } } \bigr ) ^ { T } \bigr ] a _ { 1 } ^ { T } } \\ { ~ = a _ { 1 } \sum a _ { 1 } ^ { T } } \end{array}
$$

在公式（3-6）中， $\textstyle \sum$ 代表协方差矩阵，在后续的优化工作中，将使用拉格朗日乘数法对（3-7）展示的式子进行求解。

$$
\operatorname { E } ( a _ { 1 } ) { = } a _ { 1 } \sum a _ { 1 } ^ { T } - \lambda _ { 1 } { \big ( } a _ { 1 } a _ { 1 } ^ { T } - 1 { \big ) }
$$

公式（3-7）对 $a _ { 1 }$ 求取一阶导数，并将此导数置为零，如（3-8）所示，可以得到 $\textstyle \sum$ 协方差的特征向量为 $a _ { 1 } ^ { T }$ ，特征值为 $\lambda _ { 1 }$ ，将其带入（3-6）的目标优化函数，可以得出当 $\lambda _ { \mathrm { 1 } }$ 取最大特征根就可以实现最优化， $a _ { \mathrm { { \scriptscriptstyle 1 } } }$ 为第一特征根对应的特征向量，并且 $\lambda _ { \mathrm { { _ { 1 } } } }$ 是第一个主成分的方差。

$$
\frac { \hat { c } E } { \hat { c } a _ { 1 } } { = } 2 \Bigl ( \sum a _ { 1 } ^ { T } - \lambda _ { 1 } a _ { 1 } ^ { T } \Bigr ) { = } 0
$$

$$
\sum a _ { 1 } ^ { T } = \lambda _ { 1 } a _ { 1 } ^ { T }
$$

接下来我们将继续对矩阵Y 的第二个分量进行优化，得到最合适的 $a _ { 2 }$ ，和 $Y _ { 1 }$ 不同， $Y _ { _ 2 }$ 的优化目标除了样本特征内部的方差最大化，还要 $a _ { 1 }$ 和 $a _ { 2 }$ 满足正交关系，将 $Y _ { _ 2 }$ 的优化目标定义为（3-9）。

$$
\operatorname* { m a x } a _ { 2 } \sum a _ { 2 } ^ { T }
$$

$$
\boldsymbol { a } _ { 2 } \boldsymbol { a } _ { 2 } ^ { T } = \left\| \boldsymbol { a } _ { 2 } \right\| = 1
$$

$$
a _ { 2 } a _ { 1 } ^ { T } = a _ { 1 } a _ { 2 } ^ { T } = 0
$$

同样使用拉格朗日乘数法进行求解，得到（3-10）的表达式：

$$
\operatorname { E } { \big ( } a _ { 2 } { \big ) } = a _ { 2 } \sum a _ { 2 } ^ { T } - \lambda _ { 2 } { \big ( } a _ { 2 } a _ { 2 } ^ { T } - 1 { \big ) } - \beta a _ { 1 } a _ { 2 } ^ { T }
$$

$$
\frac {  { \hat { \sigma } } \mathrm { E } } {  { \hat { \sigma } } a _ { 2 } } { = } \left( 2 \sum a _ { 2 } ^ { T } - 2 \lambda _ { 2 } a _ { 2 } ^ { T } - \beta a _ { 1 } ^ { T } \right) ^ { T } = 0
$$

由于 $\sum$ 是对称阵，从式子（3-10）我们很容易得出 $\beta = 0$ ，所以可以将 $Y _ { _ 2 }$ 的优化目标进一步表示为（3-11），此处的优化目标当 $\lambda _ { 2 }$ 取第二大特征根时实现， $a _ { 2 }$ 则为第二特征根对应的特征向量。按照此种方式，依次求出其它的最优向量。

$$
\operatorname* { m a x } a _ { 2 } \sum a _ { 2 } ^ { T }
$$

$$
a _ { 2 } a _ { 2 } ^ { T } = 1
$$

$$
\sum a _ { 2 } ^ { T } = \lambda _ { 2 } a _ { 2 } ^ { T }
$$

一般地， $\mathbf { k }$ 值的选择是根据主成分的方差在所有样本数据总方差中的占比来决定。由前面的计算推导过程可知，主成分的方差就是X对应的协方差矩阵的特征值， $\lambda _ { t }$ 是第t个主成分的方差值。前 $\mathrm { ~  ~ \mathfrak ~ { ~ t ~ } ~ }$ 个主成分的累计方差贡献率为∑'/∑",，当这个贡献率达到 85%以上，就可以认为这k个主成分能够基本代表样本中原始 $\mathfrak { n }$ 维特征所表示的相关信息。

# 3.3基于跨年龄数据增强和特征压缩的特征提取算法

# 3.3.1跨年龄人脸数据增强

本节基于生成对抗网络来进行跨年龄数据增强，模拟生成一个人在不同年龄阶段的人脸图像。通过这种方式不仅可以使人脸样本数量成倍增加，避免由于过拟合而影响模型的效果；还能够使网络模型学习到不同年龄下的人脸特征，从而削弱年龄因素对人脸识别效果的影响。下面对跨年龄人脸模拟的实现过程进行说明。

在现实生活中往往不同的人在老化速度上会存在一定的差异，并且也无法收集有准确年龄标签的人脸数据集，因此本模型的目标是生成目标人脸在某一特定年龄范围的人脸图片。总而言之，跨年龄人脸模拟的根本逻辑是通过神经网络模型来学习不同年龄段所具备的人脸特征。

图3-4是我们所提出的生成对抗网络结构图，在这里简称为AgeGAN。整个流程主要包括生成器网络和判别器网络两部分。首先给生成器输入包含年龄信息的隐含编码C 和高斯噪声Z，通过生成器网络G生成人脸样本 $\mathbf { G } ( Z )$ ，然后从假数据集G(Z)和真实数据X中随机采样,输入给判别器网络D,由判别器网络进行判断。判别器网络不仅承担判断人脸图像真伪的功能，还要对人脸样本的年龄进行分类，此外，图3-5为生成器网络G的结构图，判别器网络D和生成器网络互为镜像，唯一区别在于判别器网络有两个输出层分别承担图像真假判别和年龄分类的功能。

![](images/f8a2e42030fdae8f6c01f4a0d6fa94431fbc29a675b7be642c1cae988223152a.jpg)  
图3-4AgeGAN网络结构图

![](images/40b835f551fc5d6b472e195a6094c7faf000cc23adb9f8bf46be44eac23062fa.jpg)  
图3-5生成器网络结构图

关于网络的损失函数也分为生成器和判别器两个部分。首先是生成器网络的目标是欺骗判别器网络，也就是最小化损失 $L _ { G }$ ，具体公式如（3-12）所示：

$$
L _ { G } = E _ { Z \sim p _ { z } } \left[ \log \left( 1 - \mathrm { D } \big ( \mathrm { G } ( \mathbf { \Sigma } ) \big ) \right) \right] + C E ( c ^ { \prime } - c )
$$

而判别器网络的目标是分辨输入人脸样本的真伪以及所在的年龄区间，所以判别器网络的损失分为对抗损失和分类损失，具体的公式如（3-13）所示：

$$
{ \cal L } _ { D } = - E _ { X \sim P _ { d a t a } } [ l o g D ( x ) ] - E _ { Z \sim p _ { z } } \left[ l o g \left( 1 - D \big ( G ( z ) \big ) \right) \right] + C E ( c ^ { \prime } - c )
$$

在数据增强模型的训练过程中，采用 $\mathrm { C A C D ^ { [ 5 0 ] } }$ 跨年龄数据集，该数据集包含了年龄在16至62岁范围内的2000人的总共16万张人脸图片，每张图片都进行了年龄标注。我们将数据集的 $90 \%$ 作为训练集，剩余为测试集，并按照年龄分为五组11-20、21-30、31-40、41-50和 $5 0 +$ 。同时，基于1080TiGPU的硬件环境和TensorFlow 深度学习框架，设置学习率为0.001，batch size为32。图3-6展示了网络输出的不同年龄段的人脸模拟图像。从实验效果图中可以看出，随着年龄的增加，人脸的面部出现了一些纹理上的变化，能够较好的模拟人脸的老化过程；同时生成的人脸图像自然逼真，没有出现伪影或重影。

![](images/98b5474bc11fd00eb77eb5cc31c37ce87e31e7fd9626be5f0889bcc21590b425.jpg)  
图3-6不同年龄段的人脸模拟效果图

# 3.3.2人脸特征提取算法

人脸特征提取是人脸识别流程中的核心环节，传统的人脸编码方法是基于肉眼可观测的特征，如眼间距，口鼻的相对位置等手动特征提取的方式。但卷积神经网络在无需人工干预的情况下，将人脸图像映射到高维空间中，然后提取人脸图像的高维特征信息，这些提取到的人脸特征能够有效地应用于人脸编码任务中。

![](images/7ea0e5ec9b90e7a633994a6dbaf029696e77b106b5339c4c5ea31821b7bc6bb4.jpg)  
图3-7人脸特征提取算法流程图(a)预训练--交叉熵损失；(b）训练--三元组损失

人脸编码算法的具体流程如图3-7（a）和图3-7（b）所示，首先是预训练过程，预训练步骤是为了使神经网络权重的初始分布更加合理，从而让训练过程更加高效。模型预训练的基本流程是将带有标签的人脸图像在经过人脸对齐等预处理操作后输入神经网络，经过逐层前向传播，最后通过Softmax层对人脸图像进行分类，预测结果和原始人脸标签之间的交叉损失作为代价函数来进行反向传播实现卷积网络中权重参数的更新。

经过预训练过程后，我们采用三元组损失对网络进行训练，使网络能够有效的减小类内距离，扩大类间距离，从而提取更加有效的人脸特征信息。首先人脸图片要成对的输入给卷积网络，一个训练样本包括两张同一个人的不同人脸图像和一张不同人的人脸图像，同时输入给网络，经过三次前向传播，我们可以在全连接层（FClayer）得到三张人脸图像的特征编码，在经过主成分分析法特征降维后提取到人脸图像的主要特征后使用欧式距离对特征编码进行相似性度量，最后采用三元组损失进行反向传播来进一步优化网络。

![](images/0c820dd191ec8e0d18b00172d75a911166f88844e2770916a7f9adc73b68602c.jpg)  
图3-8人脸预测阶段流程图

预测阶段的思路是非常简洁的，如图3-8所示。将待预测的人脸图像输入到网络中得到对应的特征编码，经过主成分分析法进行降维后，将所得到的特征向量逐个与人脸数据库中所有的特征向量逐个进行相似性度量，当该值小于某个阈值时就找到该特征向量所对应的标签信息。

我们分别采用VGG16和ResNet18网络作为人脸特征提取网络来验证跨年龄数据增强和特征压缩的有效性。我们以VGG16网络结构为例介绍训练的过程，如图3-9所示，输入图片的维度为 $2 2 4 \times 2 2 4 \times 3$ ，卷积核都为 $3 \times 3$ 卷积，最大池化层步长为2，为了加快网络训练过程，在每个卷积操作后加上批标准化层来标准化数据分布。网络在预训练过程中，最后一层全连接层使用2000个网络节点，对应人脸数据集中的2000个人，同时采用SoftMax层进行激活，损失函数使用交叉熵损失函数；在训练阶段，我们将最后的全连接层的网络节点数改为1024个，对应我们所提取的特征编码的长度，使用三元组损失函数来对网络进行进一步的训练。

![](images/b7ce90868fd88195b8f4274102c9f9652e4c7604d7d30f09aa0ce7b63f973170.jpg)  
图3-9VGG16网络结构图

在对网络进行训练优化时，为了要能够更好实现类内距离的缩小和类间距离的扩大，本文将采用三元组损失函数作为特征提取网络训练阶段的误差函数。三元组损失函数TripletLoss 是谷歌团队基于欧式距离提出，其中所谓三元组（A，P,N）的构成为：将从训练集中随机选取的样本作为锚点，称为Anchor；把和Anchor拥有相同标签和不同标签的两个随机选取样本分别称为正样本Positive和负样本Negative。图3-10是三元组损失示意图，通过不断学习同类样本A和P更加靠近，不同类型的样本A和N之间距离变大。

![](images/4c91bafb504ebb533fbfab13df62f34761dc4ebc850d4bee53e48b07b543243e.jpg)  
图3-10三元组损失示意图

将三元组中的A、P、 $\mathrm { ~  ~ N ~ }$ 三个样本分别输入到网络模型中得到特征表达，并分别记为 $g \big ( x _ { i } ^ { a } \big )$ 、 $g \Big ( x _ { i } ^ { p } \Big )$ 和 $g \left( x _ { i } ^ { n } \right)$ ，三元组损失函数的公式为（3-14）所示。

$$
\begin{array} { r } { L o s s _ { T r i p l e t } = \sum _ { i } ^ { N } \biggl [ \left\| g \left( x _ { i } ^ { a } \right) - g \left( x _ { i } ^ { p } \right) \right\| _ { 2 } ^ { 2 } - \left\| g \left( x _ { i } ^ { a } \right) - g \left( x _ { i } ^ { n } \right) \right\| _ { 2 } ^ { 2 } + \alpha \biggr ] _ { + } } \end{array}
$$

在公式中，\*为求取两个向量之间的欧式距离， $\alpha$ 代表（A，P）和（A，N）两对样本距离的最小间隔，并从中得出结论：

（1）当 $d \big ( A , N \big ) < d \big ( A , P \big ) + \alpha$ 时，三元组损失公式[]内的值大于0，产生损失。  
(2）当 $d \bigl ( A , N \bigr ) > = d \bigl ( A , P \bigr ) + \alpha$ 时，损失为0。

# 3.4实验与结果分析

# 3.4.1数据集及预处理

本章节所涉及到的实验数据集如表3-1所示。由于硬件资源有限，在训练人脸特征提取网络时，我们从CASIA-WebFacel51数据集中随机选择共 2000人相关的人脸图像数据作为模型的训练数据集；在模型训练完成后，还使用了人脸识别领域最重要的LFW[52]数据集中的人脸样本对网络模型的性能进行测试。

表3-1实验相关数据集  

<html><body><table><tr><td>过程</td><td>数据集名称</td><td>描述</td></tr><tr><td>AgeGAN训练集</td><td>CACD跨年龄数据集</td><td>该数据集包含了年龄在16至62岁范围 内的 2000人的总共16万张人脸样本。</td></tr><tr><td>人脸识别训练集</td><td>CASIA-WebFace数据集</td><td>选择其中 2000人的人脸数据集，共包括 20万张人脸样本。</td></tr><tr><td>人脸识别测试集</td><td>LFW数据集</td><td>该数据集中共包含5749人的13000多张 人脸样本。</td></tr></table></body></html>

在对特征提取网络进行训练前，我们需要对数据进行人脸检测和对齐处理并将图像裁剪成网络的输入大小 $2 2 4 \times 2 2 4 \times 3$ 。下面将进行详细的说明和结果展示。

# （1）人脸检测

有效的人脸检测是完成人脸相关研究任务的核心内容，本文中选择使用Python dlib 库中正向人脸检测器 face_detector 进行人脸检测，返回具有面部关键点标注和检测框的人脸图像。表3-2展示了具体的算法流程，图3-11是部分人脸检测结果示意图。

# 表3-2dlib_opencv人脸检测算法

输入：人脸数据集 ${ \bf D } =$ {A.jpg/png/jpeg..., B.jpg/png/jpeg, B.jpg/png/jpeg,...};dlib 可识别人脸68个关键点并标注的文件“shape_predictor_68_face_landmarks.dat”；输出：面部检测 $^ +$ 面部关键点标注的人脸图像

过程：

1．初始化dlib人脸检测器dlib.get_frontal_face_detectorO，并基于输入的人脸68个关键点标注文件创建面部标志预测器dlib.shape_predictor(".dat文件")

2.重复下一步骤：

3. 对人脸数据集D中每张图片  
4. 读取图片信息，预处理，进行图像的维度重构和灰度化  
5. detector(灰度图像,1)进行人脸检测，将存放所有人脸框位置信息的rects返回  
6. 对rects对象中存放的人脸框信息进行循环遍历  
7. 人脸检测器predictor确定人脸框的面部关键点坐标，并转为NumPy 阵列  
8. 将dlib 矩形人脸框转为OpenCV样式的边界框，cv2.rectangle 绘制边界框  
9. 通过NumPy阵列循环找出面部关键点坐标（x，y），调用cv2.circle 绘制  
10. 直到停止条件

11．直到停止条件（1）人脸对齐

![](images/4b88d963be3d92eb01a719ee09228fe6dd3a0fe8363069eea5938bc6c4e0d2e0.jpg)  
图3-11部分人脸检测结果图

在人脸检测完成后，我们得到了人脸图像的面部关键点标注和检测框。人脸对齐则利用了其中的左眼、右眼、鼻尖和嘴角的五个关键点进行了仿射变换。在具体实现时借助了第三方库 imutils的FaceAligner 类完成了对人脸的基于仿射变换的对齐处理，并在FaceAligner类的初始化时，指定图像的目标大小以满足相关网络结构的输入尺寸要求。图3-12展示了部分对齐处理前后的图像。

![](images/5df5903da66a16e88dc06fe7d6c4f47875b90083a782eb1b3b6a368069d88235.jpg)  
图3-12人脸对齐效果图

# 3.4.2模型训练细节

深度网络的训练是在不断迭代的过程中对模型的参数进行调整来实现网络损失值的降低，通过这种方式得到最优化的网络模型。本章实验中的人脸特征提取网络结构基于TensorFlow 框架搭建，训练过程分为预训练和训练两部分，预训练过程我们将人脸识别问题定义为分类问题，对数据集中的数千张人脸进行分类，通过这种方式将学习到的权重信息迁移到训练过程，这样在正式的训练过程中只需要对模型进行微调，通过这种方式大幅提高训练效率。我们对训练数据集根据人物信息按照9：1的比例随机划分为训练部分和验证部分。此外，在训练环节我们采用人脸老化的数据增强策略。在预训练阶段，设置学习率为0.01，批大小为128；而在训练阶段，批大小设置为64，采用Adam优化器表现最优，设置初始学习率为0.0001，三元组损失函数边界值 $\alpha$ 为0.2。训练流程如图3-13所示，具体描述如下：

![](images/1003c2dffd4169348c77db293a95bd706638d8b3fd1ec99c611b0af89d905969.jpg)  
图3-13网络训练流程图

（1）第一步，我们需要对网络模型进行初始化，并对学习率、批大小、迭代次数等超参数的值进行设置。（2）第二步，按照批大小值将已经预处理的训练数据输入到模型中，网络经过前向传播计算输出预测值。（3）第三步，利用它预测值和实际值进行损失计算，并按照反向传播执行参数更新。（4）第四步，重复第二步和第三步的内容进行循环迭代训练。在训练的过程中使用验证集来测试模型的性能，并且根据验证结果、训练和验证损失函数对当前的训练情况进行分析，对网络的超参数等相关设置做出一定的调整。最后当达到模型的训练目标或训练迭代次数，整个训练过程结束。

# 3.4.3 实验环境

本章中提出的数据增强方案需要人脸识别算法来验证模型的有效性，网络需要从大量的人脸图像数据中学习，从而提高模型分类的准确性。由于图像处理的本质为一系列的矩阵运算，这就需要我们的实验环境能够提供充足的存储和计算资源。对于实验的硬件环境，采用了搭载GPU1080Ti显卡的计算机主机，具体的硬件配置如表3-3所示。而软件环境方面，使用64位的Ubuntu16.04作为操作系统，将 Python 作为主要的编程语言，使助Dlib、Opencv-Python 和Matplotlib 等第三方库完成相关的图像处理任务，并且基于TensorFlow主流深度学习框架实现人脸真假鉴别模型。在具体开发工作中使用Pycharm2019.1.164位作为编辑器。

表3-3实验硬件环境配置  

<html><body><table><tr><td>硬件名</td><td>参数信息</td></tr><tr><td>CPU</td><td>Intel CoreI7-6500U</td></tr><tr><td>GPU</td><td>NVIDIAGeForceGTXTITANX</td></tr><tr><td>内存</td><td>64GB</td></tr><tr><td>磁盘容量</td><td>1TB</td></tr></table></body></html>

# 3.4.4实验结果及分析

在本节中我们通过实验数据验证跨年龄数据增强和特征压缩方案对于人脸识别算法的有效性，同时为了验证方案的普适性，我们分别以VGG16和ResNet18 网络架构作为特征提取网络进行实验。

（1）我们使用主成分分析法（PCA）对VGG16作为特征提取网络的人脸识别模型进行特征压缩实验，在保留不同的特征向量维度的情况下测试人脸识别模型的准确度。模型在提取人脸样本的特征编码后，为进一步提高分类效率，通过使用主成分分析法提取特征编码中的主要信息。实验结果如表3-4所示，图3-14 是对应的变化折线图。

从实验数据和对应的曲线变化情况，我们可以看到特征向量在512维的情况下，可以小幅提高分类效果，但进一步降低会严重影响模型的分类效果，经过思考我们认为PCA可以通过线性映射将提取到的高维空间的数据映射到低维空间中，并使映射到低维空间上的数据方差尽可能大，这种方式可以使原有的特征点在相对位置不变的情况下有效降低维度，从而提高模型的性能表现。此外值得注意的是，我们将网络模型的最后特征维度直接设置为512维进行训练，实验准确率仅为$90 . 7 7 \%$ ，实验效果不如特征压缩后的实验效果，这验证了特征压缩方案的有效性。

表3-4不同信息量下模型的性能表现  

<html><body><table><tr><td>特征维度</td><td>1024</td><td>768</td><td>512</td><td>256</td><td>128</td></tr><tr><td>准确率</td><td>92.13%</td><td>92.68%</td><td>93.17%</td><td>85.43%</td><td>76.94%</td></tr></table></body></html>

![](images/0e23495803dea1a582c3904444c92fc6141699c214b73193967684fc7a8f0964.jpg)  
图3-14不同信息量下模型准确率折线图

（2）基于VGG16和ResNet18人脸特征提取网络来验证AgeGAN数据增强的有效性。实验结果如表3-5。从实验数据中可以看出AgeGAN可以使人脸模型提高2个百分点，基于主成分分析法的特征压缩也可以有效提高人脸识别准确率。此外，为了量化特征向量间相似性度量的时间消耗，我们分别计算了512维和1024维特征向量在计算欧式距离时的时间消耗，计算一次欧式距离时间消耗分别是0.0005秒和0.002秒，这也和直观感受相符合，证明了特征压缩的有效性。图3-15为模型训练过程中在验证集上的表现。

表3-5人脸识别方案准确率列表  

<html><body><table><tr><td>Model</td><td>Accuracy</td></tr><tr><td>VGG16</td><td>89.66%</td></tr><tr><td>VGG16+AgeGAN</td><td>92.95%</td></tr><tr><td>VGG16+AgeGAN+PCA</td><td>93.17%</td></tr><tr><td>ResNet18</td><td>91.22%</td></tr><tr><td>ResNetl8+AgeGAN</td><td>92.66%</td></tr><tr><td>ResNet18+AgeGAN+PCA</td><td>93.42%</td></tr></table></body></html>

![](images/73579ba648a4eee9c9aaf2f56e69c1b11b0102add68cb6c377eb2c6b0e8a7046.jpg)  
图3-15验证损失曲线图

# 3.5本章小结

本章节提出的人脸识别方案主要包括两方面的内容：第一，基于生成对抗网络模拟不同年龄阶段的人脸图像来实现数据增强；第二，采用主成分分析法进行特征压缩。在本章节我们首先对方案所涉及到的三元组损失函数、主成分分析法和特征相似性度量的相关理论知识进行了介绍。然后我们设计并实现了AgeGAN跨年龄数据增强模型，通过该模型生成不同年龄阶段的人脸图像来扩充训练样本的多样性。人脸特征提取作为人脸识别流程中的核心环节，我们在采用VGG16 和ResNet18网络作为模型主体结构的基础上修改了模型的全连接层，为了加速提取人脸特征，我们将网络训练过程分为预训练和训练两部分，并在预测阶段使用主成分分析法来进一步提高模型的性能表现。最后，通过实验验证了基于AgeGAN 跨年龄数据增强和基于主成分分析法特征压缩方案的有效性。

# 第四章 基于特征热图的人脸鉴别技术研究

本章节提出了一种基于特征热图的人脸真假鉴别方案。首先本章概述了该方案的出发点和主要贡献点。然后，从网络结构和可视化方面对特征热图进行了深入的分析，并对网络的损失函数、模型轻量化设计和生成假人脸数据的具体实现进行了介绍。接着，阐述了基于特征热图的人脸鉴别模型架构和算法流程。最后是对实验相关设置的介绍和实验结果的分析。

# 4.1 引言

本章我们提出了基于特征热图辅助分类的人脸真假鉴别方案，该方法基于MesoNet[53]模型所提出的统计学观点：真假人脸图片的特征热图在眼睛区域存在肉眼可见的较强差异性。基于该观点，我们基于VGG网络来提取特征热图辅助网络模型训练，该方案有如下贡献：（1）实现多种人脸图片生成方案来扩充假脸数据集的多样性，提高模型对各种假脸数据的泛化性和鲁棒性；（2)基于训练好的VGG网络来提取合适的特征热图参与模型训练；（3）对分类模型进一步优化，降低参数量。（4）鉴于数据集正负样本存在的不均衡问题和在鉴别难易程度上的不同，我们改进了网络的损失函数。最终该方案能够在降低参数量的同时提高模型的准确率和性能，其表现优于基准模型MesoNet。

# 4.2改进点介绍

# 4.2.1特征热图

卷积神经网络是针对图像处理任务而设计的一种拥有自主学习能力的特征提取器，具有强大的图片处理能力，对于分类、检测和识别等不同的任务都取得了较好的研究成果，能够实现较高的准确率。但人们对其工作机制至今仍然不甚了解，CNN用较为简单的操作就能够完成大量复杂的运算，对于无数的学者来说一直是一个黑箱子，并且由于所提取特征较为抽象，人们很难理解其中的原理[54]。因此，一些研究者对网络提取到的特征信息开始可视化的研究，并进一步对CNN的内部结构实行可视化分析。2014年 Zeiler等人[5在论文《Visualizing and UnderstandingConvolutionalNetworks》用可视化方式揭示了网络模型中每一层学习到的东西、提取到的特征，从而让人们对卷积神经网络的内部有了更多更深的理解和认识。近年来，通过对卷积神经网络的广泛研究，主要有以下两种可视化方式：

（1）前向计算可视化：通过前向计算直接对CNN深度网络中每层对应的卷积核和输出的特征图进行可视化，并通过其数值变化来推断卷积核的作用。特征图的大小会随着深度的加深而变得更加稀疏。有以下两种方式：其一是卷积核输出可视化，可视化卷积操作后输出的结果，即输入图片的特征热图，帮助理解卷积核的作用。其二是卷积核的可视化，对卷积核本身可视化，需要定义一个损失函数，该损失函数将用于最大化某个指定滤波器的激活值。以该函数为优化目标或使用随机梯度下降来调整输入图像的值，以便最大化该激活值，实现对卷积核学习的解释。

（2）反向计算可视化：将深层网络提取的特征图通过反向传播的方式映射到原图的像素空间，从而明白所提取的特征图从原图中获得了哪些特征信息，具体有以下方法：其一是类激活图可视化，核心思想是获取到最后一个卷积层的卷积输出以及目标类别神经元相对于每一个通道的梯度，使用这个梯度对卷积核的每一个通道进行加权处理，最后对通道求均值并且对重要度进行归一化处理。其二是反卷积可视化，与第一种方法类似，主要使用目标卷积层的输出作为输入，利用反卷积、反池化、反激活等方法得到反向操作的结果，用于验证显示各层提取到的特征图。

本文研究特征热图的出发点是基于基线模型MesoNet在文章的结尾处所提出的观点：真实人脸图像的特征热图在眼睛区域与采用Deepfake方法生成的人脸图像的特征热图存在明显的差异性。如图4-1所示，左图为多张Deepfake 假脸样本特征热图的均值，右图为多张真实人脸图片特征热图的均值，容易看出，真实人脸图片的眼部区域有较强的激活，而Deepfake方案生成假脸图像的特征热图在背景区域较为突出。基于该发现，我们尝试基于特征热图来辅助模型训练，但在原文中没有描述准确的特征热图可视化策略，所以我们尝试了使用上文中所提到的卷积核输出的特征图来验证该观点。而VGG模型表现优异的特征提取能力已经得到了广泛的认可，为了选取合适的特征热图提取方式，我们采用了训练好的VGG分类网络，下面将对此方法进行详细介绍。

![](images/7a15c450c873ef66ae1b40b17c641f9d4eb20a8cd1199b61034b1ebdcd1db60d.jpg)  
图4-1真假人脸样本特征热图[53]

卷积输出可视化是对卷积操作后所得到的特征图进行可视化的一种相对较容易的方案，通常第一层所提取到的特征能够和输入图片有较好的对应关系，但是到了卷积神经网络的更深层，所提取到的特征也更抽象。特征热图生成方式如图4-2所示，Input为输入图像，过滤器为卷积神经网络所学习到的参数，将多个过滤器的输出层相叠加，可视化后也就得到了特征热图。

![](images/c83a47d6e822a22bc6ce7c8152e4e8061a97d6ee9fbe94274bc6444e94089996.jpg)  
图4-2特征热图生成方法

图4-3展示的是从VGG网络的不同卷积层所得到特征热图，图片从左到右，从上到下分别对应卷积层逐渐加深的过程，通过观察与分析人脸所对应的特征图像，可以得出以下结论：

（1）特征热图的分辨率随着网络的加深，分辨率逐渐变小；（2）浅层网络提取的是纹理和细节特征，能较好的和输入图像对应；（3）深层网络提取的更多为轮廓和形状等特征，尤其是眼睛区域，同时图像表达也更为抽象。层数越深，所提取的特征越具有代表性；

![](images/a331308cbd075ebb2a59cea6f602b7f62083626d69351f13ea128d588f7d82b6.jpg)  
图4-3VGG网络特征热图

为验证真实图像和Deepfake生成图像所对应热图的差异性，这里随机选择五张真实人脸和生成人脸图像，通过VGG 网络的“conv51”卷积输出并激活后得到的特征热图，该特征热图的维度为 $1 6 \times 1 6 \times 5 1 2$ ，为进行可视化操作，将512张特征热图相加，维度变成 $1 6 \times 1 6 \times 1$ 。如图4-4所示，左边两列为真实图像及其所提取到的特征热图，右边两列为伪造的人脸图像所提取的特征热图，通过肉眼观察，我们容易得出结论，相对于Deepfake方案所生成的假脸图像，大多数真实的人脸图像眼睛和嘴巴的区域在深层的卷积网络计算中更易被激活，同时这也和眼睛嘴巴等区域包含特征信息更为丰富这一直观的感受相符合，而假脸图片在眼睛区域的生成更容易模糊，在卷积神经网络网络的高维特征热图中，相对不易被激活。

![](images/8717ee8ed99942bb6028297065b83ead701b491dc62ae597d402ed0296a240cf.jpg)  
图4-4真假人脸特征热图对比

# 4.2.2模型轻量化

近几年来，卷积神经网络在计算机视觉任务上取得了显著的成果。相关算法的精确度虽然变得越来越高，但卷积神经网络模型也呈现出越来越复杂的发展趋势，模型的深度、尺寸和参数量也因此而成倍地增长，对服务器的性能和GPU等硬件提出了巨大的挑战，同时也对算法应用在存储和计算资源不足的移动和嵌入式设备上进行了限制，在表4-1展示了几种经典神经网络模型深度、大小和参数量的相关数据。所以，为了提高网络模型的效率，使其拥有更加广泛的实际应用，我们需要采用轻量化的模型设计思想，通过设计更加高效的卷积方式，实现降低参数量的同时，不影响模型的准确率和性能。

表4-1经典CNN 模型深度、大小和参数量对比  

<html><body><table><tr><td>网络模型</td><td>深度</td><td>参数量</td></tr><tr><td>AlexNet</td><td>8</td><td>60M</td></tr><tr><td>VGG-16</td><td>16</td><td>138M</td></tr><tr><td>GoogleNet</td><td>22</td><td>6.8M</td></tr><tr><td>ResNet</td><td>110</td><td>1.7M</td></tr></table></body></html>

为了削减CNN冗余参数实现模型小型化，不少学者已经致力于此问题的研究，并且到目前已经出现了几种具有代表性的轻量级模型：SqueezeNet[56、Xception、MobileNet[57]和 ShuffleNet[58]。

![](images/0464d2f09f8905590c4241f5d9863aeee1165187575850e4eeaa856548492eb2.jpg)  
图4-5Fire模块的具体操作示意图

（1）SqueezeNet：为了在保证精度的同时实现最少参数，该模型能够达到将原始AlexNet压缩至原来的510倍左右（ ${ < } 0 . 5 \mathrm { M B }$ ）的效果。SqueezeNet的设计者主要基于以下三条策略进行实现：策略1，将网络结构中 $3 \times 3$ 的卷积核替换为1$\times 1$ 的卷积核，由于1个 $1 \times 1$ 卷积核在参数数量上是 $3 \times 3$ 卷积核的九分之一，当卷积核数量相同时，这样便可将模型尺寸压缩9倍；策略2，限制 $3 \times 3$ 卷积核的输入通道数，如果一个卷积层采用的卷积核大小为 $3 \times 3$ ，那么该层所有卷积参数的数量总和为 $P = N * C * 3 * 3$ ，其中参数N和C分别为卷积核数量（输出通道数）、输入通道数，所以可通过控制输入通道数可以达到减少参数量的目的；策略3，减少并后置下采样操作，以便保留更多的特征信息从而提高模型的准确率。此外，还基于inception 的思想提出了Fire 模块，该模块由 Squeeze 层和Expand 层两大部分组成，前者由 $\mathbf { S } _ { 1 }$ 个 $1 \times 1$ 大小的卷积核构成，后者则是将 $\mathbf { e } _ { 1 }$ 个 $1 \times 1$ 的卷积核和 $\mathbf { e } _ { 3 }$ 个 $3 \times 3$ 的卷积核处理后得到的feature map 进行了串联，并使三个参数的关系满足$\mathrm { S } _ { 1 } { < } ~ \mathrm { e } _ { 1 } { + } \mathrm { e } _ { 3 }$ 。图 4-5是Fire 模块的具体操作示意图，输入该模块的特征图大小为$\mathrm { H } ^ { \ast } \mathrm { W } ^ { \ast } \mathrm { M }$ ，经过Squeeze层处理后特征图的分辨率保持不变，而数量压缩到 $\mathbf { S } _ { 1 }$ 个，然后把 $\mathrm { H ^ { * } W ^ { * } S _ { l } }$ 特征图输入到Expand层执行卷积操作的结果进行串联作为Fire 模块的输出。

（2）MobileNet：MobileNet由Google 团队提出，用于解决网络模型在移动端计算资源和速度受限制的问题。MobileNet的设计主要基于以下两条策略。策略1,使用分离卷积核代替传统卷积方式以达到减少参数数量和计算量的目的。对于传统的卷积方式，如果对上一层输出大小为 $D _ { f } \times D _ { f } \times M$ 的特征图采用 N 个大小为$D _ { k } \times D _ { k }$ 的卷积核进行处理，涉及的计算量为 $\mathrm { D } _ { \mathbf { k } } { } ^ { * } \mathrm { D } _ { \mathbf { k } } { } ^ { * } \mathrm { D } _ { \mathrm { f } } { } ^ { * } \mathrm { D } _ { \mathrm { f } } { } ^ { * } \mathrm { M } ^ { * } \mathrm { N }$ 。而深度可分卷积则将一个传统的标准卷积划分为深度卷积和点卷积，首先对于M个通道的输入数据 $D _ { f } \times D _ { f } \times M$ 进行深度卷积，每个大小为 $D _ { k } \times D _ { k }$ 的卷积核只负责对输入数据的一个特征通道执行卷积操作，总共M个卷积核，合理设置填充下得到的输出结果为$D _ { f } \times D _ { f } \times M$ ，此深度卷积产生的计算量为 $\mathrm { D _ { k } { ^ \ast } D _ { k } { ^ \ast } D _ { f } { ^ \ast } D _ { f } { ^ \ast } M }$ ；然后使用N个 $1 \times 1$ 的卷积核对深度卷积的结果执行逐点卷积，即将深度卷积的多通道输出进行结合，这一步产生的计算量为 $\mathbf { M } ^ { * } \mathbf { D } _ { \mathrm { f } } { } ^ { * } \mathbf { D } _ { \mathrm { f } } { } ^ { * } \mathbf { N }$ 。从理论上卷积分解在计算效率方面的提升比例为表达式（4-1）所示。策略2，通过宽度调节和分辨率两个超参数来维持网络计算速度和准确度之间的平衡。

$$
\frac { D _ { k } * D _ { k } * D _ { f } * D _ { f } * M + M * D _ { f } * D _ { f } * N } { D _ { k } * D _ { k } * M * D _ { f } * D _ { f } * N } { = } \frac { 1 } { N } { + } \frac { 1 } { D _ { k } ^ { 2 } }
$$

（3）ShuffleNet：该模型借鉴ResNet和MobileNet的核心设计思想，在原始ResNet残差模块上基于分组卷积和通道重排两点改进策略形成了ShuffleNet网络的主要组成单元。所谓的分组卷积实质是一种通道稀疏连接方式，对输入层的所有通道分组，每组分别使用不同的卷积核进行卷积处理，达到降低卷积的计算量的目的。但是这样的做法会使模型的不同组之间缺乏信息流通从而对网络的特征表达能力和识别精度产生一定的影响，因此引入通道重排机制将分组卷积后所得到的特征图进行重组或均匀打乱以便信息能够在不同组之间流转。图4-6(a)和4-6(b)是具体的结构图。

![](images/c7a1665e3f6ea722d889b690da54bb1dca9e52901294fd8d932181b34caf2035.jpg)  
图4-6 ShuffleNet结构图 (a)具有分组逐点卷积和通道重排；(b)stride ${ \boldsymbol { \mathbf { \mathit { \sigma } } } } _ { : = 2 }$ 用在 ShuffleNet

鉴于对以上几种具有代表性的轻量级模型的了解，在本论文的工作中，我们主要通过在模型结构中引入空洞卷积和非对称卷积，同时采用全局平均池化层替换网络最后的全连接层来实现网络的轻量化。达到在减少网络模型参数量的同时，提高模型的准确率的目的。

（1）空洞卷积：空洞卷积就是在普通卷积核里注入空洞，传统神经网络中通常采用卷积层加池化层来降低图像的尺度并且提升感受野，但是这样的做法会造成内部数据结构丢失和空间层级化信息的丢失。所以为了在提升感受野的同时不丢失相关信息,Fisher等人[59]第一次将空洞卷积用于处理语义分割问题。图4-7(a)和图4-7（b）分别是 $3 \times 3$ 普通卷积和空洞卷积的示意图。

![](images/a2cd1dce65de3da4926c888c66f32567017554895793d3850a458ee0aac84320.jpg)  
图4-7卷积示意图 (a) $3 \times 3$ 普通卷积； (b) $3 \times 3$ 空洞卷积示意图

在空洞卷积中有一个特殊的参数量“dilation rate”，其含义是卷积核kernel 的间隔数量，通常“dilationrate”设置不同的值，就会得到不同的感受野。图4-7（a）为标准的 $3 \times 3$ 卷积核，其感受野同样为 $3 \times 3$ ；图4-7（b）中黑点代表非零权重，在非零权重之间使用零值进行填充，也就是只有图中9个黑色的圆点会参与卷积操作。这样能在不增加参数量的同时实现 $7 \times 7$ 卷积核的感受野。

![](images/e5f0649836dce8171af517bbae935bb459eb754465163243a850d95ef4733f33.jpg)  
图4-8非对称卷积示意图

（2）非对称卷积：为了实现卷积神经网络的压缩和加速，即网络不但可以提取有效特征信息，还能提高计算速度和降低网络的参数量。Szegedy 等人[60]在Inception网络架构中提出了非对称卷积网络，使用 $\mathsf { d } \times 1$ 和 $1 \times \mathsf { d }$ 卷积核逼近 $\mathsf { d } \times \mathsf { d }$ 正方形卷积核的卷积效果，并且由于乘法运算规模由 $\mathsf { d } \times \mathsf { d }$ 减少到了 $2 \times \mathsf { d }$ ，网络的运算量降低，d越大运算量降低的效果越明显。其背后的理论知识为：如果二维卷积核满足秩为1的条件，则运算可以使用一些一维卷积进行等价的替换，但是实际应用下的深度网络学习到的核由于具有分布的特征值，导致内在秩大于1。针对这种问题，Denton 等人[61使用 SVD分解方法寻找低秩近似，然后通过对上层进行精细化微调实现性能恢复；Jaderberg等人[62通过最小化重构误差，成功实现了对于水平核和垂直核的学习。图4-8展示使用 $3 \times 1$ 和 $1 \times 3$ 的两层卷积核达到 $3 \times 3$ 感受野的示意图，降低了 $33 \%$ 的参数量。

（3）全局平均池化层：在卷积神经中，我们通常会在网络的最后设计一层甚至多层全连接将连续多层卷积和池化操作处理后得到的二维图像特征转化为一个一维向量，这样由于参数量的增多会导致模型变得十分臃肿。为了解决这一问题，有学者提出使用全局平均池化层替换全连接层来实现降维的作用。全局平均池化层的主要思想是将网络卷积层最后输出的每一层特征图平均化成一个值，例如：网络卷积输出的三维特征图 $h \times w \times d$ 经过局平均池化后，每一层 $h \times w$ 被平均化为一个值。全局平均池化层代替全连接层不仅可以使特征图和最终分类之间的转换变得更加自然，还能降低空间参数让模型更具健壮性。

# 4.2.3人脸图像生成

深度学习的本质是设计合适的网络结构对数据和标签之间存在的复杂非线性关系实现拟合，在这期间模型需要从数据中进行不断的学习更新，获得区分度较强的特征表示。在人脸真假鉴别任务中，对于网络模型的训练需要两类数据，一种是真实的人脸数据，往往是使用目前该领域比较通用的公开数据集。另外一种是使用一定技术手段生成的假人脸数据图像，下面将对DeepFake[63]、生成模型StyleGAN[64]和FusVAE[65]相关技术原理和实现过程进行详细说明。

# （1）DeepFake

DeepFake是由“deep machinelearning”和“fake photo”组合而成，翻译过来则是深度模仿（基于深度学习的人脸替换）。2017年秋，DeepFake技术首次出现在一段用于制作成人换脸内容的脚本中，该技术的目的是将目标视频序列中的人脸使用源视频或图像集合中观察到的人脸进行替换。传统的换脸方法是通过特征点匹配来分析用于互换人脸图像中的相似信息，如将一张人脸图像中的眼睛、鼻子等特征信息匹配到另一张人脸图像上；这种方式虽然不需要大量数据集训练网络用于提取相关特征信息，但是一些表情等细节信息却无法体现。随着深度学习的发展，使用深度神经网络提取图像数据的高层特征表示变得更加容易。DeepFake 技术的核心思想是对两个编码器进行并行训练和权值共享从而学习两张用于人脸互换图像的共同信息，同时设置两个解码器用于重建不同人脸的恒定特征形状和细节信息，图4-9展示了实现人脸交换的流程。在图中，训练阶段将不同个体A和B的对齐人脸图像数据集分别输入到两个并行的Encoder 编码器中学习共同的隐含信息，然后再将特征信息FeaturesA和FeaturesB输入到对应的解码器Decoder A和DecoderB中重新还原各自的输入图像，同时采用L1范数损失函数以及通过Adam优化算法进行梯度下降。在生成阶段则是将个体A的图像数据输入到Encoder中学习到特征信息FeaturesA，然后输入到DecoderB达到人脸交换目的。

本论文中准备的DeepFake数据集使用了FaceForensics $\mathrm { \Omega _ { \mathrm { 5 + } } } \mathrm { [ } 6 6 \mathrm { ] }$ 的DeepFake数据集，该数据集包含了1000个伪造视频和与之相关的1000个真实视频。所有的视频都是用H.264编解码器压缩的，最低分辨率为 $8 5 4 \times 4 8 0$ 像素。本文的DeepFake数据集分割为训练集、验证集和测试集。从1000个视频中选择了300个伪造视频和真实视频，并针对每个视频随机提取15张图片，最后训练集中，真图4500 张，假图4500 张；同时从1000个视频中选择了伪造视频和真实视频各150个，并针对每个视频随机提取20张图片，最后测试集中，真图3000张，假图3000张；训练集图片数量的 $10 \%$ 作为验证集。

![](images/fe56bec96cb70b1c1030ddef61b6f2602c70b035ae397bbfddcd2d25d7f109a2.jpg)  
图4-9DeepFake效果示意图

（2）StyleGAN

StyleGAN是对ProGAN[67]的进一步改进。ProGAN采用渐进式增长的训练模式，初始时浅层的网络结构用于生成 $4 \times 4$ 的低分辨率图片，随着训练的不断进行，网络的层数逐渐动态的增加，从而不断学习更高分辨率图片的生成，最后生成1024$\times 1 0 2 4$ 超高分辨率图片。但是，该网络结构在逐级直接生成图像的过程中没有添加任何额外的控制，从而无法知悉每一级中学到了什么样的特征，导致决定生成图像的某些特定特征的能力不足。因此StyleGAN在ProGAN模型生成器的基础上添加多个附加模块来对图像的不同视觉特征实现更加细微和精确的控制，图4-10 展示了StyleGAN的生成器模型结构。

![](images/7bb2e8589d9603d11b0154e73b143d53dfb3493339e9cd9f2f40d9417abdf51e.jpg)  
图4-10StyleGAN生成器结构示意图[64]

在具体实验过程中，选用包含7000O张Flicker人脸的高清数据集FFHQ作为训练和测试数据。然后基于1080TiGPU的硬件环境和 TensorFlow深度学习框架，并对学习率、batchsize的大小等参数进行设置。最后，运行模型共生成10000 张$1 0 2 4 \times 1 0 2 4$ 的高清人脸图像。图4-11是StyleGAN生成的部分假人脸效果图。

![](images/2e3c9e0cf521a5b05626855b8da7835f71154317a83cd4d7232a415eebcb078b.jpg)  
图4-11StyleGAN生成人脸图

（3）FusVAE

在论文《End-to-end FusVAEForFace Image Fusion》中，我提出了一种基于变分编码器VAE对两张人脸图像的信息进行融合生成一张新人脸图像的网络架构FusVAE，并且通过设置融合偏倚参数能够对新生成目标图像的头发颜色、发型和面部表情等细节进行控制。该方法生成的图像不仅自然完整，而且融合过程简单快捷。FusVAE 的整个网络结构由编码器Vencoder、解码器Vdecoder和VGG三个子网络构成。图4-12是结构示意图，表4-2是三个子网络对应的配置情况。

![](images/bfcfa41945356b70f858be932774eb27e7063789624c26fc0530c2c97e2ddc55.jpg)  
图4-12FusVAE网络结构图

表 4-2Vencoder、Vdecoder和VGG 网络配置  

<html><body><table><tr><td>Vencoder</td><td>Vdecoder</td><td>VGG</td></tr><tr><td>Conv(32,4,2),BN</td><td>FC(4096),Reshape(4x4x256),BN</td><td>Conv(64,3,1) Maxpool(2,2)</td></tr><tr><td>Conv(64,4,2),BN</td><td>UpSampling(8x8x256),Conv(128,3,1),BN</td><td>Conv(128,3,1) Maxpool(2,2)</td></tr><tr><td>Conv(128,4,2),BN</td><td>UpSampling(16x16x128),Conv(64,3,1),BN</td><td>Conv(256,3,1) Maxpool(2,2)</td></tr><tr><td>Conv(256,4,2),BN</td><td>UpSampling(32x32x64),Conv(32,3,1),BN</td><td>---</td></tr><tr><td>Flatten(4096)</td><td>UpSampling(64x64x32),Conv(3,3,1),BN</td><td>!</td></tr><tr><td>FC(100),sigmoid</td><td></td><td></td></tr></table></body></html>

该网络结构的基本思想是将两个大小为 $6 4 \times 6 4 \times 3$ 的原始图像 $X ^ { a }$ 和 $X ^ { b }$ 输入到编码器Vencoder 中得到两个服从正态分布的 $q _ { \varphi } \mathopen { } \mathclose \bgroup \left( Z \mid X ^ { a } \aftergroup \egroup \right)$ 和 $q _ { \varphi } \big ( Z | X ^ { b } \big )$ ，然后对两个分布进行随机采样得到100 维特征向量 $\hat { Z } _ { a }$ 和 $\hat { Z } _ { b }$ ，进一步处理后得到100 维特征向量 $\hat { Z }$ ， $\hat { Z } = W _ { 1 } \times \hat { Z } _ { a } + W _ { 2 } \times \hat { Z } _ { b }$ 。其中融合偏倚参数 $W _ { 1 }$ 和 $\textstyle { W _ { 2 } }$ 是 100 维的列向量，每个维度的取值为0或1，分别代表放弃或保留对应的图像特征。接着将 $\hat { Z }$ 输入到解码器Vdecoder中，得到融合后生成的新图像 $X ^ { a b }$ 。整个体系结构的优化目标包括两部分。一是重建损失，用于测量最终的融合效果；二是KL散度，其目的是将Vencoder的输出与标准正态分布 $\mathrm { { N } } ( 0 , 1 )$ 对齐。重构误差：将原始图像 $X ^ { a }$ 、 $X ^ { b }$ 和融合输出图像 $X$ 分别输入到VGG网络中，最后提取前三层的输出结果；针对每一层，对 $X ^ { a }$ 和 $X ^ { b }$ 的输出结果进行加权求和并将求和的结果与 $X$ 的输出计算均方误差，最后将三层求得的均方误差相加得到网络最终的重构误差；假设 $L _ { a } ^ { i } \setminus L _ { b } ^ { i }$ 和 $L ^ { i }$ 分别代表 $X ^ { a }$ 、$X ^ { b }$ 和 $X$ 第 $i$ 层对应的输出，偏倚参数 $\boldsymbol { W } _ { 1 } ^ { ' }$ 和 $\boldsymbol { W } _ { 2 } ^ { ' }$ 是取值范围为[0,1]的实数，两者之和为1。重构误差的表达式为（4-2），其中 $P _ { i } = W _ { 1 } ^ { ' } \times L _ { a } ^ { i } + W _ { 2 } ^ { ' } \times L _ { b } ^ { i }$ 。

$$
\mathrm { L O S S } _ { \mathrm { r e s } } { = } \sum _ { i = 1 } ^ { 3 } m e a n \sim s q u a r e \left( P _ { i } , { \cal L } ^ { ( i ) } \right)
$$

KL 散度：我们希望尽量减小重构误差，但是由于100 维的特征向量 $\hat { Z }$ 是通过重新采样过的，在进行重构的过程中容易受到噪声的影响，因而加大重构的难度。所以为了重构的效果，我们会尽量减小这个噪声强度（正态分布方差），但是为了防止噪声为零，保证模型具有生成能力，我们让正态分布向标准正态分布看齐，此时需要再重构误差的基础上加上一个 $\mathrm { K L \_ l o s s : K L ( N ( u , o 2 ) | | N ( 0 , 1 ) ) } \mathrm { _ { c } }$

在具体实验过程中，选用CelebA作为数据集，模型FusVAE的实现基于tensorflow 框架。在训练阶段选择优化器Adam，同时将批量大小和学习率设置为

64和0.005。通过调节两对偏倚参数的值，最后共保存了5000张FusVAE生成的人脸融合图像。图4-13是部分人脸融合效果图。

![](images/f13be7f52279ab127076772eafe02249378927efa5cb3082292a5a825b8b906d.jpg)  
图4-13FusVAE人脸融合效果图

# 4.3基于特征热图的人脸鉴别模型架构

# 4.3.1基线模型

本文采用Afchar等人提出的MesoNet网络作为基线模型。该网络架构开始于四个连续的卷积块操作，每个卷积操作由卷积层、BatchNormalization 和池化层构成，然后是两层全连接层 Fully-connected16 和 Fully-connected1，最后再通过Sigmoid 激活函数得到最终分类的结果，前两个卷积操作使用了Inception 模块，该模块的思想是将几个具有不同卷积核形状大小的卷积层的输出进行叠加，从而增加模型的函数复杂度。图4-14（a）和图4-14（b）分别是MesoNet模型和Inception模块的结构示意图。

![](images/91c7233c00cd13594d5ae7a17dac207509a55ed1b41d082d05da355e63709557.jpg)  
图4-14 基线模型结构图 (a)MesoNet 结构图；(b)Inception 模块结构图

# 4.3.2基于特征热图的人脸鉴别模型

鉴于前文所提到的现象，真假人脸数据集样本的特征热图存在肉眼可见的差异性，本文提出了一种基于特征热图辅助训练的人脸鉴别框架，特征热图来源于在ImageNet数据集中训练的具有较好特征提取能力的VGG 分类网络在前向传播过程中卷积层的输出结果，我们将提取到的特征热图放缩至输入图片的维度，其激活区域能够和原始图像形成映射关系，有效指导模型的训练，从而提高模型准确率。本文采用的人脸鉴别模型的框架结构如图4-15所示。

![](images/55fa898c5fe016e8764d2c6e93219aa84b7e8a391daa0b7c1e415ac50bdd46b2.jpg)  
图4-15人脸鉴别模型架构图

我们改进的出发点是：在提高网络模型准确率的同时去减少参数量。图中Inception层也采用了图4-14（b）中所展示的结构。在这里主要有以下三点改进：

（1）引入特征热图：基于VGG分类网络，使用在ImageNet数据集中所训练好的权重来提取特征热图，经放缩到原始图片的维度后，与原始图片合并后作为网络输入。

（2）精简网络模型：引入非对称卷积，将网络中 $n \times n$ 卷积核替换为 $n { \times } 1$ 和$1 \times n$ 卷积核；同时替换基线模型中的全连接层为全局池化层，降低模型训练参数量。

（3）改进损失函数：鉴于我们所采用的数据集存在正负样本不均衡和样本鉴别难易程度的不同，我们在这里引入了目标检测领域常用的FocalLoss[68损失，在解决正负样本不均衡问题的同时，也进一步平衡难易程度不同的样本对模型的影响。该损失函数的核心思想就是在交叉熵的基础上添加权重，降低大量简单负样本在训练过程中所占用的权重，即对于困难样本的挖掘。（4-3）是Focalloss 的定义表达式，在式中 $\boldsymbol { p } _ { t }$ 代表不同分类类别的概率，参数γ和 $\alpha _ { \mathrm { { t } } }$ 都是不参与训练的固定值，γ是大于0的数， $\alpha _ { \mathrm { { t } } }$ 是取值区间为[0,1]的小数。并且从表达式中可以得出以下结论:第一，无论对于前景类还是背景类， $\boldsymbol { p } _ { t }$ 的值越大，权 $\left( 1 - p _ { \mathrm { t } } \right) ^ { \gamma }$ 重值就越小，也就是说可以通过调节权重对简单样本进行约束；第二， $\alpha _ { t }$ 调节正负样本的比例， $\alpha _ { t }$ 和 $1 - \alpha _ { t }$ 分别对应前景类别和背景类别，由于 $\gamma$ 和 $\alpha _ { \mathrm { { t } } }$ 的最优值具有相关性，我们在评估准确度时需要将两个参数组合取来共同进行调节。

$$
F L ( P _ { t } ) { = } { - } \alpha _ { t } ( 1 { - } p _ { t } )  \} \log ( p _ { t } )
$$

不同于基线模型在卷积操作激活函数之后加入BatchNormalization层，改进模型在每个卷积操作后，激活函数前都加入了BatchNormalization层。其中Inception模块所采用的卷积核的数量如表4-3所示，Inception模块采用不同大小的卷积核在提取不同感受野的图像特征信息的同时，显著降低模型的参数量。其中该方案主要尝试解决人脸鉴别方案非实时的应用场景，关于从分类网络中对特征热图的提取环节可以通过工程化的方案多线程并行执行，对时间开销没有影响。

表4-3Inception层参数表  

<html><body><table><tr><td>Inception</td><td>A</td><td>B</td><td>C</td><td>D</td></tr><tr><td>Layer 1</td><td>1</td><td>4</td><td>4</td><td>1</td></tr><tr><td>Layer 2</td><td>1</td><td>4</td><td>4</td><td>2</td></tr></table></body></html>

# 4.4实验与结果分析

# 4.4.1数据集及预处理

为了验证基于特征热图的人脸真假图像鉴别方案的有效性，我们这里采用了由MesoNet论文所提供的公开数据集进行实验，并且为了进一步验证模型的泛化性，我们根据前面所提到的多种人脸图像生成方案构造了一种更加全面的数据集，其中的假脸样本包括Deepfake，StyleGAN以及FusVAE所生成的人脸图像。同时对数据集采用3.4.1小节中使用的人脸检测和对齐方案将图像裁剪成 $2 5 6 \times 2 5 6$ 。

实验所使用的公开数据集和自定义数据集数量如表4-4，4-5所示，其中训练集包括了训练集和验证集，其比例为9:1，并且在实验过程中，随机抽选满足相应比例的验证集的同时，会保持验证集中真假人脸图片的比例和训练集保持一致，以尽可能避免一些潜在因素的干扰。

表4-4公用数据集  

<html><body><table><tr><td>数据集</td><td>假脸图像</td><td>真脸图像</td></tr><tr><td>训练集</td><td>5111</td><td>7250</td></tr><tr><td>测试集</td><td>2889</td><td>4259</td></tr></table></body></html>

表4-5自定义数据集  

<html><body><table><tr><td>数据集</td><td>假脸图像</td><td>真脸图像</td></tr><tr><td>训练集</td><td>14000</td><td>26000</td></tr><tr><td>测试集</td><td>2000</td><td>5000</td></tr></table></body></html>

网络的训练过程主要采用批量学习和动态学习率的方式，批量学习可以不受训练图像样本顺序的干扰，在反向传播过程中精确的估计梯度向量，并可以采用并行化的方式成倍降低训练时长，使网络快速收敛。经实验网络采用Adam优化器表现最优，在初始学习率设置为0.001，每经过2000 次迭代后，将学习率乘以0.1,直到学习率降至0.00001后不再进一步下降。

在模型训练前，人脸图像的预处理工作是机器学习中必不可少的阶段。进行一系列预处理操作的作用在于能够让图像特征的分布更加合理，使网络快速有效的学习到图像特征，从而提高模型的训练效果。在人脸鉴别任务中，我们需要对人脸数据进行统一的规范化处理，包括统一图像的维度和背景颜色。对于图像处理任务，图像标准化Z-score 或归一化是常见的预处理中操作，其中Z-score 方法是求出样本图像 $x$ 各个通道的均值 $\mu$ 以及标准差 $\sigma$ ，通过（4-4）展示的公式运算后，图像样本会变成均值为0，方差为1的数据分布 $x ^ { * }$ 。

$$
x ^ { * } = \frac { x - \mu } { \sigma }
$$

而对于图像归一化处理通常采用最大最小值归一化方式，其主要思想是将原始的图像样本像素值从0-255放缩到0-1的范围，其数学公式可用（4-5）来表示，其中 $x _ { \mathrm { m a x } }$ 和 $x _ { \operatorname* { m i n } }$ 分别为样本数据的最大值和最小值。

$$
x ^ { * } = \frac { x - x _ { \operatorname* { m i n } } } { x _ { \operatorname* { m a x } } - x _ { \operatorname* { m i n } } }
$$

事实上，由于本文所提出的方案需要通过已训练好的VGG网络来提取人脸样本所对应的特征热图，而该分类网络采用Z-score图像标准化的方式进行预处理，

所以在这里我们的人脸鉴别数据集也同样采用这种方式，并且经过实验，Z-score图像标准化的效果要优于归一化的方案。

# 4.4.2实验结果及分析

为了评估从VGG模型的不同卷积层对实验效果的影响，以及进一步验证本章提出的基于特征热图的人脸鉴别方案在不同数据集上的效果。主要进行以下几组实验:

（1）实验一：从训练完成的VGG 模型中提取多层特征热图对比实验效果。基于上文中所提到的公开数据集，从VGG模型的不同卷积层获取特征热图，经放缩到原始图片的维度后，与原始图片合并作为人脸鉴别模型的输入。如图4-16所示，这里我们提取了VGG部分前向网络层的特征热图来进行对比实验。

![](images/6782cc9d0d263af5a0adb7834f56b6902e898bf0fbd27bdf70ad0c291ccb39ec.jpg)  
图4-16VGG网络不同卷积输出层的特征热图

表4-6不同特征热图下的实验结果  

<html><body><table><tr><td>模型</td><td>Accuracy</td><td>F1</td></tr><tr><td>MesoNet</td><td>0.914</td><td>0.908</td></tr><tr><td>Conv2_1Module</td><td>0.908</td><td>0.902</td></tr><tr><td>Conv3_1Module</td><td>0.951</td><td>0.947</td></tr><tr><td>Conv4_1Module</td><td>0.935</td><td>0.942</td></tr><tr><td>Conv5_1Module</td><td>0.921</td><td>0.915</td></tr></table></body></html>

实验结果如表4-6所示，我们可以看出VGG网络中Conv3_1层所提取到的特征热图表现效果较好，易见由VGG网络中Conv3_1层所提取的特征热图来进行辅助分类可以得到最佳效果；同时，我们在使用VGG 网络的Conv2_1、Conv3_1、Conv4_1或Conv5_1来提取特征热图的实验结果大多优于MesoNet方案，这也表明基于特征热图的方案是有效的。

（2）实验二：通过实验对比验证改进点非对称卷积的作用和损失函数的效果。相比于基线模型，我们的人脸真假鉴别方案提出了非对称卷积和损失函数，为了验证两点改进的有效性，在实验中，我们进行了严格的控制变量，其中特征热图均采用VGG网络中Conv31的特征热图，损失函数采用原始交叉熵损失。

实验结果如表4-7所示，第一行为正常卷积模型（如 $3 \times 3 , 5 \times 5$ 的卷积形式）和Focalloss损失的实验结果，第二行表示模型采用非对称卷积和交叉熵的损失的实验结果，第三行为模型采用非对称卷积核Focalloss 损失的实验结果。我们可以发现非对称卷积方案在有效降低训练参数量的同时并不会影响模型的准确率。同时实验结果表明基于FocalLoss 损失函数的改进是有切实效果的，改进模型在Accuracy和F1两个指标上分别提高了 $1 . 5 \%$ 和 $2 . 2 \%$ 。

表4-7对比实验结果  

<html><body><table><tr><td>Model</td><td>Accuracy</td><td>F1</td></tr><tr><td>Normal-Conv + Focal loss</td><td>0.952</td><td>0.945</td></tr><tr><td>Asymmetric-Conv +Cross Entropy</td><td>0.936</td><td>0.925</td></tr><tr><td>Asymmetric-Conv +Focal loss</td><td>0.951</td><td>0.947</td></tr></table></body></html>

（3）实验三：对比最终网络模型在不同数据集中的表现。在该实验中，我们列举了基线模型 MesoNet，CGFace[69]和改进模型的 Accuracy 和F1 两个评价指标对应的实验结果。其中我们设计的网络模型统一采用实验一中所得到的效果最优的特征热图，即VGG 网络的Conv3_1卷积输出层的特征热图，损失函数均采用FocalLoss损失函数。

表4-8不同数据集实验结果  

<html><body><table><tr><td rowspan="2">数据集</td><td colspan="2">CGFace</td><td colspan="2">MesoNet</td><td colspan="2">Ours</td></tr><tr><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td><td>Accuracy</td><td>F1</td></tr><tr><td>公开数据集</td><td>0.919</td><td>0.925</td><td>0.914</td><td>0.908</td><td>0.951</td><td>0.947</td></tr><tr><td>自制数据集</td><td>0.889</td><td>0.911</td><td>0.892</td><td>0.883</td><td>0.932</td><td>0.957</td></tr></table></body></html>

根据表4-8所展示的实验结果，对于公开数据集，我们提出的模型相比基线模型 MesoNet在Accuracy 和F1两种评价指标上分别提升了 $3 . 7 \%$ 和 $3 . 9 \%$ 。而在自制的数据集中包含了Deepfake、StyleGAN 和 FusVAE 多种技术生成的人脸图像，Accuracy和F1值分别提升了 $4 \%$ 和 $4 . 2 \%$ ，进一步证明了我们提出的基于特征热图的人脸鉴别方案对于多种人脸生成方案都具有良好的泛化性和鲁棒性。

ROC曲线图在正负样本发生变化时，形状保持基本不变，该评估指标能够比较客观的衡量模型性能。我们基于改进模型在公用数据集下的训练权重，绘制了ROC 曲线图，如图4-17所示，横纵坐标分别代表假阳性率FPR和真阳性率TPR。

![](images/e19e59a9277958066d1dc8b977500d6340c5850987ad7d2e3cd5d1dc18221fc6.jpg)  
图4-17ROC曲线图

# 4.4.3BadCase分析

在本小节中，我们对实验结果进一步分析来深入了解模型的表现。我们自制数据集中包括Deepfake，StyleGAN以及FusVAE所生成的伪造人脸图像，其中测试集共包含真实的人脸图像 5000 张，假脸数据 2000 张。在假脸数据中Deepfake,StyleGAN以及FusVAE所占的比例别为5:3:2。

表4-9实验数据  

<html><body><table><tr><td>特性 数据</td><td>Deepfake</td><td>StyleGAN</td><td>FusVAE</td><td>真实人脸</td></tr><tr><td>数量</td><td>1000</td><td>600</td><td>400</td><td>5000</td></tr><tr><td>准确率</td><td>0.940</td><td>0.897</td><td>0.958</td><td>0.933</td></tr></table></body></html>

改进模型在不同数据集的表现效果如表4-9所示，由数据可知，模型对于FusVAE的假脸数据识别率最高，这是由于FusVAE模型融合出的人脸图像普遍存在背景较模糊的情况，相对于其他生成模型，更容易识别；关于 SyleGAN的识别率相对较低，在这里分析是由于 StyleGAN 模型生成的人脸图像像素值相对较高，生成人脸整体较为逼真。

![](images/216955ae0a7608156310fd31d34e1aec018e80958c487a5d1ea499711265c65b.jpg)  
图4-18 部分badcase样本

在图 4-18中我们列举了测试集中预测错误的真实人脸图像和假脸图像，其中第一行为真实人脸数据集，第二行为生成的人脸图像数据。通过对badcase样本的分析我们总结出模型对于以下几种情况的人脸数据表现较差。

（1）光线较为昏暗，人脸辨识度较低的样本。  
（2）眼部区域呈闭眼或眯眼状态的样本。  
（3）头部倾斜较为严重，只能观察到侧脸的样本。

上述三种情况，其中第一种和第三种涉及到数据集质量的原因，不可避免。第二种样本经分析认为其表明了眼睛区域对于人脸图像鉴别的重要性，所以改进模型对于闭眼状态下的人脸真假鉴别问题表现较差。

# 4.5本章小结

本章节受到模型MesoNet中所提出的启发性观点，我们提出了基于神经网络特征热图辅助分类的人脸鉴别方案。该方案中我们设计的网络模型在MesoNet的基础上进行了三点改进：引入特征热图，从VGG网络的卷积输出来获取特征热图，经放缩到原始图片的维度后，与原始图片合并作为真假人脸鉴别网络输入；通过在模型中引入空洞卷积，非对称卷积以及全局平均池化层等方式来降低模型的参数量；针对训练样本的实际情况，采用FocalLoss损失函数来优化模型。最后我们通过实验我们证明了改进模型能够在降低参数量的同时，提高模型的准确率，并且在不同数据集上都具有较好的表现。

# 第五章基于人脸的身份认证原型实现

本章采用PythonFlask 轻量级框架设计与实现了一个基于人脸的身份认证展示平台，该平台是对前两章提出的人脸真假鉴别和人脸识别算法的一个实际应用。下面将对具体的设计与实现方案进行介绍。

# 5.1需求分析

基于人脸的身份认证功能展示平台是对第三章的人脸识别算法和第四章的人脸真假鉴别算法进行的工程化实现。图5-1是对应的系统用例图，并且该平台应该满足如下需求：

（1）信息录入：本文所采用的人脸识别算法需要使用特征提取网络获得每张人脸图像的一个特征编码，并将录入人脸图像的特征编码信息和对应的人物信息进行存储，从而实现人脸识别功能。此外，还需要提供对人的身份信息进行修改和删除的功能。

（2）人脸识别：系统对拍摄或上传的人脸图像先进行人脸图像的真假鉴定，如果人脸图像鉴定的结果为真，则将该人脸图像输入到特征提取网络中，得到对应的特征编码，最后和数据库中的特征编码进行比对，从而实现人脸识别功能。

（3）人脸拍摄。用户可以在平台上调用本地摄像设备来实现拍照的需求。

（4）人脸上传。用户可以直接在平台中上传本地图像文件。

![](images/d303bcd1c8e5dbae4aa6632f7b5622ffe354075201e52133d3e06e15da4ed3ed.jpg)  
图5-1系统用例图

# 5.2平台设计

本平台由图像采集与显示模块、信息录入模块、身份认证模块和数据存储模块四部分构成，图5-2展示了平台的总体设计架构图。该平台提供信息录入和身份认证两大主要功能。如果选择身份认证，首先需要图像采集与显示模块通过摄像头或者图像上传的方式获取人脸图像，接着将人脸图像数据送入到身份认证模块进行预处理、真假鉴别和人脸识别等一系列处理，将得到的人脸特征向量与数据存储模块中存放的人脸信息进行逐一匹配，最后将结果返回到图像采集与显示模块进行输出。若选择信息录入功能，我们同样需要经过图像采集与显示模块获取人脸图像，并输入身份信息，然后将人脸图像送至身份认证模块判断数据库中是否存在该人脸信息，如果不存在，则将人脸信息和相关的身份信息存放到数据库中并将信息录入的结果返回到图像采集与显示模块进行输出。下面对主要功能模块进行阐述。

![](images/e27d614f7361c0908af30b714484e35b1257d158d323e4acfe6f7555d027f189.jpg)  
图5-2系统的总体设计架构图

（1）图像采集与显示模块：该模块提供了通过摄像头和图片上传的方式得到人脸图像，如果选择摄像头的方式，我们需要从摄像头获取的视频流中取出其中的一帧进行保存。并使用Pythondlib库中已经训练完成的正向人脸检测器facedetector完成人脸检测任务以及借助第三方库imutils的FaceAligner类完成对人脸的基于仿射变换的对齐处理。同时该模块还对信息录入模块和身份认证模块返回的处理结果进行输出。

（2）身份认证模块：该模块是平台中的核心部分。对输入该模块的人脸图像进行归一化等预处理操作，然后平台加载第四章中已经训练好的鉴别模型进行真假判断。如果鉴别结果为“真”，则继续加载第三章中已经训练好的人脸识别模型提取特征向量，并将此向量与数据存储模块中存放的人脸信息通过欧式距离进行逐一匹配，最后将认证结果返回到图像采集与显示模块，若认证成功，界面显示用户的姓名 $+$ 认证成功，否则提示“unkown $^ +$ 认证失败”，此时可以在页面上选择进行信息录入；反之鉴别结果为“假”，则将结果返回到图像采集与显示模块并在系统界面提示“图片审核不通过”。图5-3(a)展示了该模块的工作流程图。

![](images/026f166e0b2ccbae13018770a4299593fc43695a03118970359e0aad652e9e20.jpg)  
图5-3流程图(a)身份认证模块流程图；(b)信息录入模块流程图

（3）信息录入模块：对于未注册的用户在使用该平台进行身份认证之前应该先录入信息，录入的信息主要有人脸图像、姓名、电话号码、联系地址等。将采集的人脸图像经过检测和对齐处理后输入到身份认证模块中得到对应的特征向量，然后在数据库中进行查询，根据结果判断是否重复录入。如果重复则将结果返回到图像采集与显示模块并在界面提示“信息已存在”，相反则将新录入的信息保存到数据库中并在页面提示“录入成功”。图5-3(b)是信息录入模块流程图。同时，在该模块中还提供对录入信息的删除和部分录入信息（如：电话号码、联系地址等）的修改功能。

（4）数据存储模块：该模块中主要存放用户录入的身份信息和人脸图像两类数据。用户身份信息主要包括姓名、身份证号、电话号码和联系地址等。而人脸图像却分为了两种不同的数据：一种是人脸图像数据本身，当用户通过界面发起查看身份信息的请求时，我们通常将该类数据作为检索结果进行返回；另一类人脸数据是经过人脸识别模型提取到的特征向量，这种数据不对用户进行公开，用于不同人脸的特征向量通过欧式距离进行相似性的度量，得出人脸识别的结果。所以，通过分析功能需求，为该系统的数据库设计了三张表：用户信息表（User）、图片信息表（Picture）和图片特征表（Pic_features）。

在表5-1中，用户信息表（User）的用户编号（userId）作为唯一标识用户的主键。而姓名、电话号码和联系地址则作为用户在信息录入时的必填信息。图片信息表（Picture）中的图片编号（picId）是与人脸图片能够实现唯一匹配的主键，并通过外键用户编号（userId）与User表建立联系。图片特征表（Pic_features）中主要存放图片特征编号、图片编号和图片特征向量等相关信息，其中图片特征编号（featureId）是能对每一个特征向量进行唯一表示的主键，而图片编号（picId）作为外键与Picture表进行联系对人脸图片的相关信息进行访问。

表5-1数据表设计说明  

<html><body><table><tr><td>表序号</td><td>数据库表名</td><td>表的详细说明</td></tr><tr><td>1</td><td>User</td><td>用户编号、用户姓名、电话号码、联系地址、信息 录入时间</td></tr><tr><td>2</td><td>Picture</td><td>图片编号、图片名称、图片路径、上传时间、更新 时间、用户编号</td></tr><tr><td>3</td><td>Pic_features</td><td>图片特征编号、图片编号、图片特征向量、特征向 量更新时间</td></tr></table></body></html>

# 5.3．系统实现与展示

# 5.3.1开发环境

本系统基于Python 语言进行开发，使用Pycharm2019.1.164位作为编辑器。后端使用Flaskweb轻量级框架，人脸真假鉴别和识别两个算法的实现基于Tensorflow 和Keras 框架；前端则通过 Bootstrap 框架、Html、CSS、JavaSCript 和Ajax 等技术实现页面的渲染和前后端数据的交互。同时还选用SQLite作为数据库，通过第三方插件“sqlalchemy”实现对数据库的操作。

基于python 语言编写的轻量级可定制web 框架，具有轻便、灵活、安全和扩展性强等优点。该模块的核心功能构成虽然简单，但用户可以根据自身的开发需求添加其他的功能从而实现功能的扩展和丰富。Werkzeug和Jinja2是Flask中的两大核心函数库，分别用于处理业务请求和安全问题。

# 5.3.2主要功能实现与展示

平台的主页面如图5-4所示，在页面左侧的导航栏中展示了身份认证和信息录入两大主要功能，页面右半部分在平台初始化时自动加载身份认证的页面显示，如果点击导航栏的信息录入，页面右侧则关联录入信息的页面进行展示。

![](images/df3e660859120b7013f98660733dc48576c5a4f9528ec147337139b739a6f355.jpg)  
图5-4系统主页页面

当用户进入平台主页面选择身份认证或者信息录入时，首先需要点击页面右侧的“数据采集”按钮获取一张人脸图像。在点击“数据采集”按钮后页面弹出一个窗口，在这个窗口中，用户可以选择上传本地图片或者调用摄像头现场拍摄一张照片，图5-5是效果展示图。在该图片采集窗口，用户根据需要移动裁剪框或者放缩裁剪框的大小，在右侧会对图像的裁剪效果进行展示。

![](images/c15aa7935d6bfbeb48b65b7f14a2f020ce7b3845de9f06d37600ab3d35547f2e.jpg)  
图5-5数据采集页面

一个已经完成信息录入的用户，进入系统主页实现身份认证，通过图5-5展示的数据采集页面获取一张人脸图像，然后点击页面中的“身份认证”按钮向后台程序发送认证请求并将人脸图像数据的Base64编码传送到后台，首先后台对图像进行人脸检测，如果检测不到人脸，会返回给前端页面，请求重新上传或拍摄。如果检测到人脸，则调用 face_verification.py 的 get_result(img_dir)函数执行人脸鉴别算法，若返回的鉴定结果大于0.5，则为真实人脸，图片审核通过；然后通过get_faces_from_pic.py的recognize(img)函数得到这张人脸对应的特征向量，并和数据库中的数据进行相似性匹配，匹配方式采用欧式距离。最后将匹配到的人物信息返回给前端页面，如果匹配不到，则反馈前端页面没有该人物信息，查询用户是否录入相关信息。图5-6是身份认证成功的效果展示图，图5-7是人脸真假鉴别结果为假的效果展示图，该功能的关键代码如表5-2所示。

![](images/c80b133163fc4857abedd765541db09aafe6bdf199cb78e1e5460770df938152.jpg)  
图5-6身份验证成功的效果图

![](images/fdf0e109fe72655faa571232444697dd7ab72f1b7e741313567a68b1053867ec.jpg)  
图5-7人脸鉴伪结果为假的效果图

with open(img_name,'wb') as f: f.write(base64.b64decode(img_decode)）//对图像进行解码并临时存放   
$\mathrm { i m } \mathbf { g } = \mathbf { c } \mathbf { v } 2$ .imread(img_name) //使用opencv 读取图像   
data $= \{ \}$ （204 //存放验证结果   
vertification $\mathbf { \tau } =$ get_result(img_dir) //图片进行真假验证   
if(vertification $< 0 . 5$ ） data['vertification'] $\mathbf { \Sigma } =$ 'false'   
else: nameList $\ c =$ recognize(img) //图片鉴别为真，进行人脸识别 data['vertification'] $\mathbf { \Sigma } =$ 'true' data['recognition'] $\ O =$ nameList   
return json.dumps(data,ensure_asci $\circleddash$ False) //将验证结果以json数据返回前台

图5-8展示了平台的信息录入页面。用户进入该页面，首先进行图像采集以及相关身份信息的填写，然后点击“信息录入”按钮通过Ajax技术将图片的Base64码和相关个人信息打包成json字符串向后台发送post请求。后台接收数据后同样先进行人脸检测和对齐，然后执行人脸真假鉴定和特征提取工作，将得到的特征向量和数据库通过欧式距离进行相似性匹配，并将阈值设置为0.4。若数据库中已经存在该用户信息，则返回前端页面不予录入。否则，将用户提交的数据录入到数据库中，并在前端页面提示“信息录入成功”。

![](images/3ca769a40fd319ae2e364a786aa0ed3a8e172d5acb9be071a9becbaa27fbc6f5.jpg)  
图5-8信息录入页面

# 5.4系统测试及结果分析

本节对系统在真实场景下的身份识别准确率进行了测试。在测试过程中，我们针对同一个人的脸部有不同倾斜程度、不同遮挡范围、不同分辨率对系统进行了10次测试，称之为测试1；同时，还选择了10个人，在人脸角度、遮挡幅度和图像分辨率都比较适宜的情况下，每个人测试10次，总共100次，称之为测试2。系统测试结果如表5-3所示：

表5-3系统测试结果  

<html><body><table><tr><td>测试</td><td>测试总次数</td><td>识别成功次数</td></tr><tr><td>测试1</td><td>10</td><td>5</td></tr><tr><td>测试2</td><td>100</td><td>91</td></tr></table></body></html>

图5-9给出了部分系统测试结果，图中第一行为识别正确的人脸图像，第二行为识别失败的人脸图像。综合上述测试结果进行分析，在真实的场景下，当我们的采集的人脸图像在人脸角度、遮挡幅度和图像分辨率都比较适宜的情况下，系统具有较好的身份识别表现；而当遮挡幅度较大、人脸倾斜严重、光线昏暗、人脸分辨率不高的情况下，系统的身份识别结果容易出现错误或者识别身份不出的情况。但是大体上来讲，系统还是具有较好的身份识别表现。

![](images/696425c08b8e0277c4c08d5489bf930a8afc2efcfc850331bcc0a3d4a0a52e36.jpg)

# 5.5本章小结

本章节基于PythonFlask 轻量级框架对前两章提出的人脸真假鉴别和人脸识别算法进行了工程化的应用和展示。该Web系统通过身份认证和信息录入两大主要功能充分验证了本文所提出的两种人脸处理算法可以应用于身份认证领域。

# 第六章总结与展望

# 6.1全文总结

身份评估作为金融服务风险防御的第一道屏障，对金融消费者的信息和资金安全都起着关键性的作用。其中基于人脸生物特征的识别技术已经广泛应用在安防和支付等领域拥有广泛的应用场景，随着人工智能技术的不断创新和发展，人们可以利用计算机来合成逼真的人脸图像，这些图像通过肉眼很难辨别真伪，如果这些伪造的人脸图像应用在基于生物特征的身份识别系统和认证服务将造成严重的安全威胁。因此，本文主要围绕人脸真假鉴别和人脸识别两个方面来设计一个身份验证的展示和应用方案。

人脸识别任务，本文提出的人脸识别方案主要包括了两方面的内容：第一，设计 AgeGAN 模拟不同年龄阶段的人脸图像来实现数据增强；第二，采用主成分分析法进行特征压缩。该部分内容所做工作如下：

（1）选取CASIA-WebFace数据集的部分人脸样本，设计AgeGAN模拟不同年龄阶段的人脸图像来实现数据增强。同时对人脸图像数据进行检测和对齐处理并将图像裁剪成统一大小。

（2）人脸特征提取网络结构的主体采用VGG-16和ResNet-18网络结构，在此基础上进行了部分调整；此外为了在提取有效人脸特征的同时提高模型的训练效率，人脸特征提取的网络训练过程分为预训练和训练两部分。

（3）通过一系列对比实验验证基于AgeGAN的数据增强和主成分分析法降维方法对于人脸识别任务的有效性。

关于人脸真假鉴别任务，我们提出了基于神经网络的特征热图辅助分类的人脸真假鉴别方案。这部分内容所做工作如下：

（1）通过DeepFake、StyleGAN和FusVAE 网络模型实现人脸数据生成，并利用多种生成人脸数据构造了一个更加全面的鉴别数据集，同时对数据进行检测和对齐处理并将图像裁剪成统一大小。

（2）我们设计的网络模型在MesoNet模型的基础上进行了三点改进。首先引入特征热图，从VGG网络的不同卷积层获取特征热图，经放缩到原始图片的维度后与原始图片合并作为人脸鉴别网络的输入。然后通过在模型中引入空洞卷积，非对称卷积和全局平均池化层等来降低网络的参数量实现网络的轻量化设计。最后我们结合数据集的特点采用FocalLoss损失作为模型的优化函数。

（3）我们通过在公开和自定义数据集上的一系列对比实验验证了模型能够在降低参数量的同时提高模型的准确率和性能表现。

最后，我们基于Python的Flask 轻量级框架对提出的人脸真假鉴别和人脸识别算法进行了实现和部署。该web系统通过身份认证和信息录入两大主要功能对相关算法进行了应用和展示。

# 6.2后续工作展望

本文从人脸特征热图和数据增强的角度分别对真假鉴别和人脸识别任务进行了改进，并通过一系列对比实验来验证了所提出方案的有效性。论文未来的改进方向可以从以下角度入手：

（1）人脸识别任务可以尝试从多任务学习的方面入手，在搭建人脸识别网络的过程中，可以同时结合表情识别、年龄识别、性别识别等任务，验证这些任务彼此之间是否可以起到相辅相成和相互促进的作用，从而共同提高模型的特征提取能力。

（2）关于人脸真假鉴别任务，由于新的人脸生成方案层出不穷，能否从原理层面出发找到人脸生成方案的共性，从而一劳永逸的解决人脸真假鉴别问题。

# 致谢

转眼间已经是来到电子科技大学的第三个年头，意味着很快我的研究生学业生涯就要结束。这三年里在学习、科研和生活中经历的点点滴滴不仅让我学到了更多的为人处事的方法和态度，也让我在思想上变得更加的成熟和理智。

首先，感谢我的导师陈波老师三年来对我的悉心栽培和指导。在科研上，您告诉我怎么去寻找科研的方向，并且还提供相关的实验设备和专业资料；在生活上您教会我怎么与人相处和沟通，当情绪低落时，您会找我谈心，帮助我疏导。研一进校的时候您让我对未来做出的规划使我明确了自己的奋斗目标，能够对每个阶段的任务做出合理的安排。

接着，我还要感谢智感中心的所有老师，不仅给我们提供良好的实验室环境和科研氛围，还让学生根据自己的研究方向、擅长领域参加实际项目。这样既锻炼了我们的专业能力，更重要的是让我们在实际项目中体会到团队合作的重要性。

同时，还要感谢实验室的师弟师妹，同届的李哲希、陈吴、陈富泽、艾营，大家在平时的学习和生活中给予我的帮助。每次实验室有谁遇到困难，大家都会帮着一起寻找解决办法；有什么开心的事，我们也会组织一波团建。总之，406实验室是一个同甘共苦的大家庭，每一个人都是一颗温暖的小太阳。

最后，我要感谢我的家人，在物质和精神上的支持，是我前进路上永远的坚强后盾；同时，感谢我亲爱的男朋友王豪爽在电子科技大学三年来的陪伴、相互理解和鼓励。

# 参考文献

[1]苏保祥.加强金融反欺诈能力建设[J].中国金融,2018(10):72-74.   
[2]陈希琳.欺诈丛生 谁在蚕食消费金融?[J].经济,2018(08):66-74.   
[3]何开宇.国际生物识别技术场景创新与应用[J].中国银行业,2019(05):89-91.   
[4]简讯.人脸识别技术综述[J].电脑知识与技术,2019,15(02):171-172.   
[5]V Conotter, E Bodnari, G Boato, et al. Physiologically-based detection of computer generated faces in video[C].2014 IEEE International Conference on Image Processing (ICIP),Paris,France, 2014, 248-252.   
[6]L Tran, X Liu. Nonlinear 3d face morphable model[C]. Proceedings of the IEEE conference on computer vision and pattern recognition, Salt Lake City, Utah, USA,2018, 7346-7355.   
[7]L Tran, X Liu. On learning 3d face morphable model from in-the-wild images[J]. IEEE transactions on pattern analysis and machine intelligence, 2019.   
[8]JThies,M Zollhofer, M NieBner, et al. Real-time expression transfer for facial reenactment[J]. ACM Trans. Graph.,2015,34(6): 183:1-183:14.   
[9]I Korshunova, W Shi, JDambre, et al. Fast face-swap using convolutional neural networks[C]. 2017 IEEE International Conference on Computer Vision (ICCV), Venice,Italy,2017,3677-3685.   
[10] G Lample, N Zeghidour, N Usunier, et al. Fader networks: Manipulating images by sliding attributes[C]. Advances in Neural Information Processing Systems, California,2017,5967-5976.   
[11] X Chen,L Qing, X He, et al. FTGAN: A Fuly-trained Generative Adversarial Networks for Text to Face Generation[J]. Arxiv, 2019,abs/1904.05729.   
[12]I Goodfellow,JPouget-Abadie,M Mirza, et al. Generative adversarial nets[C]. Advances in neural information processing systems(NIPS), Montreal, Canada, 2014: 2672-2680.   
[13] X Mao,Q Li, H Xie, et al. Least squares generative adversarial networks[C]. 2017 IEEE International Conference on Computer Vision(ICCV), Venice, Italy, 2017,2794-2802.   
[14] A Radford, L Metz, S Chintala. Unsupervised representation learning with deep convolutional generative adversarial networks[J].ArXiv, 2015,abs/l511.06434.   
[15] K Sricharan,R Bala, M Shreve,et al. Semi-supervised conditional gans[J]. ArXiv,2017, abs/1708.05789.   
[16] H Zhang,T Xu,H Li, et al. Stackgan $^ { + + }$ :Realistic image synthesis with stacked generative adversarial networks[J]. IEEE transactions on pattern analysis and machine intelligence, 2018, 41(8): 1947-1962.   
[17] M.F. Hashmia,A. R. Hambarde,A. G. Keskar. Copy move forgery detection using DWT and SIFT features[C]. l3th International conference on intellient systemsdesignand applications(ISDA), Malaysia, 2013,188-193.   
[18] A Kashyap,B Suresh,M Agrawal, et al. Detection of splicing forgery using wavelet decomposition[C]. International Conference on Computing, Communication & Automation. IEEE, Greater Noida, India, 2015: 843-848.   
[19] D Cozzolino,G Poggi,L Verdoliva. Recasting residual-based local descriptors as convolutional neural networks: an application to image forgery detection[C]. Proceedings of the 5th ACM Workshop on Information Hiding and Multimedia Security, Philadelphia, Pennsylvania, USA, 2017: 159-164.   
[20] P Zhou, X Han, V I Morariu, et al. Learning rich features for image manipulation detection[C]. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, Salt Lake City, Utah,USA,2018: 1053-1061.   
[21] JYang, G Zhu, JHuang, et al. Estimating JPEG compression history of bitmaps based on factor histogram[J]. Digital Signal Processing,2015, 41: 90-97.   
[22] Lin Z, He J, Tang X, et al. Fast, automatic and fine-grained tampered JPEG image detection via DCT coefficient analysis[J]. Pattern Recognition, 2009, 42(11): 2492-2501.   
[23] Murali S, Chittapur G B,Anami B S. Comparision and analysis of photo image forgery detection techniques[J]. arXiv preprint arXiv:1302.3119,2013.   
[24]HFarid.Exposing digital forgeries from JPEG ghosts[J]. IEEE transactions on information forensics and security,2009,4(1): 154-160.   
[25]HHNguyen,JYamagishi,IEchizen. Capsule-forensics: Using capsule networks to detect forged images and videos[C]. 2019 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Brighton, England,2019: 2307-2311.   
[26] L Nataraj, T M Mohammed, B S Manjunath, et al. Detecting GAN generated fake images using co-occurrence matrices[J]. Electronic Imaging,2019, 2019(5): 532-1-532-7.   
[27] A Bharati, M Vatsa, R Singh, et al. Demography-based facial retouching detection using subclass supervised sparse autoencoder[C]. 2017 IEEE International Joint Conference on Biometrics (IJCB), Colorado, USA,2017, 474-482.   
[28] C. C. Hsu, C. Y. Lee, Y. X. Zhuang. Learning to detect fake face images in the wild[C]. 2018 International Symposium on Computer, Consumer and Control (IS3C), Taiwan, China, 2018, 388-391.   
[29] M.D. Kelly. Visual identification of people by computer[R]. California: Technical Report AI130, Stanford AI Project, 1970.   
[30] Kanade T. Picture processing by computer complex and recognition of human faces[D]. kyoto: Kyoto University,1973.   
[31] G.B. Huang,HLee,ELearned-Miller.Learning hierarchical representations for face verification with convolutional deep belief networks[C].20l2 IEEE Conference on Computer Vision and Pattern Recognition, Providence,RI, USA,2012, 2518-2525.   
[32] H Zhou, K. W. Wong, K. M. Lam. Feature-aging for age-invariant face recognition[C]. 2015 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA), Hong Kong, China,2015, 1161-1165.   
[33] X Shu, JTang, HLai, et al. Personalized Age Progression with Aging Dictionary[C].2015 IEEE International Conference on Computer Vision (ICCV), Santiago, Chile,2015,3970-3978.   
[34] S Hochreiter,JSchmidhuber. Long short-term memory[J].Neural computation,1997,9(8): 1735- 1780.   
[35] W Wang, Z Cui, Y Yan, et al. Recurrent face aging[C]. 2016 IEEE Conference on Computer Vision and Pattern Recognition,Las Vegas, NV, USA,2016,237-2386.   
[36]P Viola, M Jones.Rapid object detection using a boosted cascade of simple features[C]. 2001 IEEE computer society conference on computer vision and pattern recognition, Kauai, Hawai, USA, 2001, 1: I-I.   
[37] K Zhang,Z Zhang,Z Li, et al. Joint face detection and alignment using multitask cascaded convolutional networks[J]. IEEE Signal Processing Letters,2016,23(10): 1499-1503.   
[38] Y Taigman, M Yang, M. A.Ranzato, et al. Deepface: Closing the gap to human-level performance in face verification[C]. 2014 IEEE conference on computer vision and patern recognition, Columbus,America,2014, 1701-1708.   
[39] Y Sun, X Wang, X Tang. Deep learning face representation from predicting l0000 classes[C]. 2014 IEEE conference on computer vision and pattern recognition, Columbus,America, 2014, 1891-1898.   
[40] Y Sun, Y Chen, X Wang,et al.Deep learning face representation by joint identificationverification[C]. Advances in neural information processing systems(NIPS), Montreal, Canada, 2014,1988-1996.   
[41] K Fukushima. Neural network model for a mechanism of pattern recognition unaffected by shift in position-Neocognitron[J]. IEICE Technical Report,A,1979,62(10): 658-665.   
[42] YLeCun,L Bottou, YBengio, et al. Gradient-based learning applied to document recognition[J]. Proceedings of the IEEE,1998, 86(11): 2278-2324.   
[43]俞颂华.卷积神经网络的发展与应用综述[J].信息通信,2019(02):39-43.   
[44] Ian Goodfellow,Yoshua Bengio,Aaron Courville.深度学习[M].北京:人民邮电出版社,2017, 142-145   
[45] K Simonyan，A Zisserman. Very deep convolutional networks for large-scale image recognition[J]. ArXiv, 2014, abs/1409.1556.   
[46] K He, X Zhang, S Ren, et al. Deep residual learning for image recognition[C]. 2016 IEEE conference on computer vision and pattern recognition(CVPR),Las Vegas, NV, USA, 2016, 770- 778.   
[47] D. P. Kingma, P Dhariwal. Glow: Generative flow with invertible 1xl convolutions[C].Advances in Neural Information Processing Systems, Montreal, Canada, 2018: 10215-10224.   
[48] D.P. Kingma, M Welling. Auto-encoding variational bayes[J].ArXiv, 2013,abs/l312.6114.   
[49] 陈靖_.主成分分析(PCA)原理及推导[EB/OL]. http://blog.csdn.net/zhongkejingwang/article/details/42264479,2014-12-30.   
[50] B. C. Chen, C. S. Chen, W. Hsu. Face Recognition and Retrieval Using Cross-Age Reference Coding with Cross-Age Celebrity Dataset[J]. IEEE Transactions on Multimedia, 2015,17(6): 804-815.   
[51] Y Dong,L Zhen, S Liao, S Li. Learning face representation from scratch[J]. ArXiv, 2014, abs/1411.7923.   
[52] G.B. Huang,M Mattar, T Bergl. Labeled faces in the wild: A database for studying face recognition in unconstrained environments[DB]. http://vis-www.cs.umass.edu/lfw, 2007.   
[53] DAfchar, V Nozick,J Yamagishi, et al. Mesonet: a compact facial video forgery detection network[C]. 2018 IEEE International Workshop on Information Forensics and Security (WIFS)， Hong Kong, China, 2018, 1-7.   
[54]樊帆,康兵义.卷积神经网络可视化[J].电子技术与软件工程,2019(12):177.   
[55] M.D. Zeiler, R Fergus. Visualizing and understanding convolutional networks[C]. European conference on computer vision. Springer, Cham,2014, 818-833.   
[56] F.N. Iandola, S Han, M. W. Moskewicz, et al. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and $< 0 . 5$ MB model size[J].ArXiv, 2016,abs/1602.07360.   
[57] A. G. Howard, M Zhu,B Chen, et al. Mobilenets: Efficient convolutional neural networks for mobile vision applications[J].ArXiv,2017,abs/1704.04861.   
[58] X Zhang,X Zhou, MLin, et al. Shufflenet: An extremely efficient convolutional neural network for mobile devices[C]. 2018 IEEE conference on computer vision and pattern recognition, Salt Lake City, USA,2018, 6848-6856.   
[59]F Yu, V Koltun. Multi-scale context aggregation by dilated convolutions[J]. ArXiv, 2015, abs/1511.07122.   
[60] C Szegedy, V Vanhoucke, S Ioffe,et al. Rethinking the inception architecture for computer vision[C]. 20l6 IEEE conference on computer vision and pattern recognition, Las Vegas, America,2016,2818-2826.   
[61] E.L. Denton,W Zaremba, J Bruna, et al. Exploiting linear structure within convolutional networks for efcient evaluation[C]. Advances in neural information processing systems, Montreal, Canada, 2014, 1269-1277.   
[62] M Jaderberg, A Vedaldi, A Zisserman. Speeding up convolutional neural networks with low rank expansions[J]. ArXiv,2014, abs/1405.3866.   
[63] Oldpan.解密Deepfake(深度换脸）-基于自编码器的换脸技术[EB/OL]. https://oldpan.me/archives/deepfake-autoencoder-face-swap,2018-12-07.   
[64] T Karras, S Laine, T Aila. A style-based generator architecture for generative adversarial networks[C]. 2019 IEEE Conference on Computer Vision and Patern Recognition, California, USA,2019, 4401-4410.   
[65] Xiang Li,Bo Chen,Meijin Wen,and Haoshuang Wang. End-to-End FusVAE for Face Image Fusion[C]. Proceedings of the 2018 International Conference on Algorithms, Computing and Artificial Intelligence, Sanya, China, 2018,1-6.   
[66] A Rossler, D Cozzolino,L Verdoliva, et al. Faceforensics $^ { + + }$ : Learning to detect manipulated facial images[C]. 2019 IEEE International Conference on Computer Vision, California, USA, 2019, 1-11.   
[67] T Karras,T Aila, S Laine, et al. Progressive growing of gans for improved quality, stability, and variation[J].ArXiv,2017,abs/1710.10196.   
[68] L.Y.Lin,P Goyal,R Girshick, et al.Focal loss for dense object detection[C].2017 IEEE international conference on computer vision, Venice,Italy,2017, 2980-988.   
[69] L M Dang,S I Hassan, S Im,et al. Deep learning based computer generated face identification using convolutional neural network[J].Applied Sciences,2018, 8(12): 2610.

# 攻读硕士学位期间取得的成果

论文:

[1] Xiang Li, Bo Chen, Meijin Wen, and Haoshuang Wang. End-to-End Fus VAE for Face Image Fusion[C]. The 2O18 International Conference on Algorithms, Computing and Artificial Intelligence, Sanya, China, 2O18, 1-6.

[2] Xiang Li,Meijin Wen,Anlong Chen,and Bo Chen, "A Method for Face Fusion Based on Variational Auto-Encoder," 15th International Computer Conference on Wavelet Active Media Technology and Information Processing, Chengdu, China, 2O18, 77-80.

专利：

[3]一种人脸图像融合方法，申请号：CN201811250280.6

主研项目：[4]四川省科技项目：智能金融风险管控（项目编号：2018GZDZX0042）