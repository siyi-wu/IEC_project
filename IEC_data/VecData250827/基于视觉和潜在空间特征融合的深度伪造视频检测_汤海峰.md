重庆工商大学学报(自然科学版)  
Journal of Chongqing Technology and Business University(Natural Science Edition)  
ISSN 1672-058X,CN 50-1155/N

# 《重庆工商大学学报(自然科学版)》网络首发论文

题目： 基于视觉和潜在空间特征融合的深度伪造视频检测  
作者： 汤海峰，杨高明  
收稿日期： 2025-03-02  
网络首发日期： 2025-05-13  
引用格式： 汤海峰，杨高明．基于视觉和潜在空间特征融合的深度伪造视频检测[J/OL]．重庆工商大学学报(自然科学版).https://link.cnki.net/urlid/50.1155.n.20250512.1750.006

网络首发：在编辑部工作流程中，稿件从录用到出版要经历录用定稿、排版定稿、整期汇编定稿等阶段。录用定稿指内容已经确定，且通过同行评议、主编终审同意刊用的稿件。排版定稿指录用定稿按照期刊特定版式（包括网络呈现版式）排版后的稿件，可暂不确定出版年、卷、期和页码。整期汇编定稿指出版年、卷、期、页码均已确定的印刷或数字出版的整期汇编稿件。录用定稿网络首发稿件内容必须符合《出版管理条例》和《期刊出版管理规定》的有关规定；学术研究成果具有创新性、科学性和先进性，符合编辑部对刊文的录用要求，不存在学术不端行为及其他侵权行为；稿件内容应基本符合国家有关书刊编辑、出版的技术标准，正确使用和统一规范语言文字、符号、数字、外文字母、法定计量单位及地图标注等。为确保录用定稿网络首发的严肃性，录用定稿一经发布，不得修改论文题目、作者、机构名称和学术内容，只可基于编辑规范进行少量文字的修改。

出版确认：纸质期刊编辑部通过与《中国学术期刊（光盘版）》电子杂志社有限公司签约，在《中国学术期刊（网络版）》出版传播平台上创办与纸质期刊内容一致的网络版，以单篇或整期出版形式，在印刷出版之前刊发论文的录用定稿、排版定稿、整期汇编定稿。因为《中国学术期刊（网络版）》是国家新闻出版广电总局批准的网络连续型出版物（ISSN 2096-4188，CN 11-6037/Z），所以签约期刊的网络版上网络首发论文视为正式出版。

# 基于视觉和潜在空间特征融合的深度伪造视频检测

汤海峰，杨高明

安徽理工大学 计算机科学与工程学院，安徽 淮南 232001摘要：【目的】 随着深度学习技术的进步，深度伪造的视觉异常变得难以察觉，而现有检测方法大多是单一的识别伪造视频中的视觉特征，忽略了深度建模和视频数据分布特性方面带来的影响。为解决单一的视觉和生成模型特征挖掘不全面的问题，本文提出了一种基于视觉和潜在空间特征融合的深度伪造视频检测模型（Deepfake Video Detection Based on Fusion of Visual and Latent Spatial Features，VLSFFD）。【方法】 首先，将伪造视频帧序列通过自编码器（AE）进行帧的重建，挖掘潜在空间特征；其次，分别将原始视频帧和重建视频帧，通过卷积视觉 Transformer（Efficient-Swin）的结构进一步处理和提取特征；接着，将分别处理的特征经自适应特征融合模块（AFFM）进行特征融合；最后通过分类层检测视频真伪。【结果】 在 $\mathrm { F F ^ { + + } }$ 、DFDC 和 WildDeepfake 三大数据集中进行了对比实验，对比于以往同类型最高的模型性能，本模型在三大数据集内测试的检测准确率均分别增长了 $0 . 2 6 \%$ 、 $0 . 1 5 \%$ 和 $0 . 1 7 \%$ ，同时在DFDC 的跨数据集测试中相较以往的最优性能也增长了 $2 . 2 9 \%$ ，有效提高了模型检测性能和鲁棒性。【结论】 本模型充分挖掘和分析伪造视频的视觉和潜在空间特征，并将二者特征进行融合，有效的提升了深度伪造视频检测的泛化能力，显著缓解了现有方法在处理伪造视频时面临未知伪造的性能瓶颈。

关键词：深度伪造视频检测；自编码器（AE）；潜在空间特征；特征融合中图分类号：TP391.41;TP183 文献标识码：A

# Deepfake Video Detection Based on Fusion of Visual and Latent Spatial Features

TANG Haifeng, YANG Gaoming   
School of Computer Science and Engineering, Anhui University of Science and Technology, Anhui Huainan   
232001, China

Abstract：Objective With the advancement of deep learning technology, visual anomalies in deepfakes have become difficult to detect, and most existing detection methods primarily focus on identifying visual features in fake videos, ignoring the impact from deep modeling and video data distribution characteristics. To address the issue of incomplete feature extraction in single visual and generative models, this papar proposes a deepfake video detection model based on the fusion of visual and latent spatial features (VLSFFD). Methods Firstly, the original video frames sequence is reconstructed using an autoencoder (AE) to mine latent spatial features. Next, both the original and reconstructed video frames are processed through a Convolutional Vision Transformer (Efficient-Swin) to extract hierarchical features. The extracted features are then fused via an Adaptive Feature Fusion Module (AFFM). Finally, the fused features are fed into a classification layer to determine video authenticity. Results The experiments conducted on the three major datasets— $\mathrm { \cdot F F ^ { + + } }$ , DFDC, and WildDeepfake—demonstrated that the proposed model achieved accuracy improvements of $0 . 2 6 \%$ , $0 . 1 5 \%$ , and $0 . 1 7 \%$ , respectively, compared to the previous state-of-the-art models of the same type. Additionally, in crossdataset testing on DFDC, the model outperformed the prior best performance by $2 . 2 9 \%$ , effectively enhancing both detection capability and robustness. Conclusion  By thoroughly exploring and analyzing both visual and latent spatial features of deepfake videos, and effectively fusing these dual-modal features, our model successfully improves the generalization capability of deepfake detection. This breakthrough notably alleviates the performance limitations faced by existing methods when handling unknown forgery types in synthetic videos.

Key words：deepfake video detection; autoencoder; latent spatial features; features fusion

# 1  引言

深度伪造技术生成的视频愈发逼真，由于其潜在的滥用可能性和传播虚假信息的能力，深度伪造已成为公众关注的焦点[1-2]。深度伪造通过技术手段，如面部替换、面部重演、面部编辑和完整面部合成等，将一个人的面部特征叠加到另一个人的脸上，可借助多种开源工具[3-4]轻松制作，对个人隐私和信息安全构成重大威胁。因此开发准确可靠且能对多种伪造均有效的深度伪造视频检测[5]技术迫在眉睫。

现有的深度伪造视频检测技术主要分为基于单帧视觉特征和基于时序特征的两大类方法。这些方法各有优势，但也存在明显的局限性，尤其是在面对新型伪造技术和跨数据集泛化方面的挑战。

在单帧视觉特征检测方面，研究者们提出了多种创新性方法：瞿远近和吴起[6]提出一种改进高斯滤波网络的方法，将高斯滤波拓展到空域和值域上，实现有效的检测。Liu 等[7]创新性地结合空间图像和相位谱特征，显著提升了上采样伪影的捕捉能力；Zhao 等[8]提出了一种基于源特征不一致性的检测方法，引入配对自一致性学习和不一致图像生成器相结合的学习框架，进一步提高检测准确率；Wang 等[9]设计的 M2TR 模型则通过在不同尺度的图像块上检测局部不一致性，利用多尺度分析大幅增强了检测性能。然而，这些基于单帧视觉特征的方法主要依赖于伪造过程中产生的视觉伪影，对于高质量生成的伪造内容检测能力有限，且容易受到压缩、噪声等后处理操作的影响导致检测性能下降。

为弥补单帧方法的不足，时序特征检测方法应运而生。Li 等[10]提出一种多实例学习框架（S-MIL），开创性地实现了实例到视频级的直接映射；Gu 等[11]通过设计 STIL 模块有效捕捉伪造视频中的时空不一致性，提供了一种新的时序建模范式；Zheng 等[12]提出了端到端的框架，利用时序一致性，有效提取时序特征，显著提升模型泛化能力；Nie等[13]提出了一种基于 Transformer 的方向性不一致性扩散学习框架（DIP），利用视频中的水平和垂直方向上的时序不一致性重建图像，进而充分提取特征，为时序分析提供了新思路。尽管时序方法在捕捉帧间不一致性方面表现出色，但其计算复杂度高，且对于高质量伪造视频中微妙的时序一致性问题检测效果仍不理想。

最新研究趋势表明，时空多特征融合检测方法已展现出巨大潜力。Gu 和 Chen 等[14]提出的动态不一致性建模框架充分实现了特征的有效整合；Zhao 和 Yu 等[15]通过设计正则化损失来引导局部信息的选择和时序信息的融合，并取得了显著的检测效果；Zhao 和 Wang 等[16]通过时空特征协同分析，引入新颖的分解空间时间自注意力机制和自减机制，捕捉空间伪影和时间不一致性，为应对新型伪造技术提供了创新性解决方案。然而，这些融合方法主要集中在视觉层面的特征提取和融合，忽略了深度生成模型在潜在空间留下的独特痕迹，这些痕迹对于区分高质量伪造内容至关重要。

随着生成对抗网络（GAN）、变分自编码器（VAE）和扩散模型（DM）等先进生成模型的快速进展，现代深度伪造几乎不留任何明显的视觉线索，使得单靠视觉伪影来检测深度伪造变得更加困难。因此，有研究[17-20]开始聚焦于视频的生成痕迹进行检测，探索潜在空间特征对伪造检测的价值。这些方法通过分析生成模型在潜在空间留下的特征模式，为检测提供了新的视角。但同单一的视觉方法一样，单一的基于生成模型的方法同样也缺少视觉特征的细节信息，例如光照、阴影等视觉痕迹，导致在复杂场景下的检测性能受限。

综合分析现有方法，我们发现以下关键问题需解决：(1)单一视觉特征的提取方法无法全面捕捉伪造内容的多维特性；(2)现有融合方法缺乏对视觉特征和潜在空间特征的有效融合机制；(3)模型面对未知伪造类型的适应性仍有待提高。

为了解决上述问题，本文提出了一种基于视觉和潜在空间特征融合的深度伪造视频检测方法（VLSFFD）。该架构通过利用 Efficient-Swin 架构的卷积视觉 Transformer 联合学习视频局部与全局视觉特征，充分提取表面细节信息；同时引入自编码器（AE），结合全局注意力模块来学习潜在空间特征，有效捕捉视频帧生成过程中的微妙问题。最后通过自适应特征融合模块，VLSFFD 能够同时利用视觉特征中的表面细节和潜在空间特征中的生成模式，从而形成更全面、更鲁棒的检测框架。与现有方法相比，本文的主要贡献在于：(1)提出了一种新颖的视觉和潜在空间双特征提取框架；(2)设计了高效的自适应特征融合机制，实现了不同维度特征的有机结合；(3)通过大量实验验证了所提方法在多种伪造类型和跨数据集场景下的优越性能。

# 2  模型框架

融合视觉和潜在空间特征的深度伪造视频检测模型的整体框架如图 1 所示，主要框架为原始视频帧和 AE 重建视频帧的双流处理网络，分为重建、特征处理、特征融合等步骤。VLSFFD 主要有3 个主要模块：AE 帧重建模块、Efficient-Swin 特征处理模块和 AFFM 特征融合模块。AE 帧重建模块的编码器和解码器能够重建原始视频帧。由于AE 可以从潜在空间和重建误差中提取特征，因此学习到的潜在空间能够有效识别真实内容中的不一致伪造特征。Efficient-Swin 特征处理模块将原始视频帧和重建视频帧输入到网络中，利用

EfficientNet 和 Swin Transformer 来全面提取和处理视频帧的局部和全局特征。在这两个网络的连接处，引入一个新的混合嵌入模块，以更好地集成它们的特性并优化特征提取性能。AFFM 特征融合模块有效地融合原始视频帧和重建视频帧的特征，从而挖掘出更详细和全面的视觉特征和潜在空间特征。最后，融合后的特征通过激活函数 ReLU 和全连接层进行分类，从而实现对深度伪造视频的精确检测。

![](images/9cec6229f96301cca58a7ca1c52419cb82f3b402513205142bccc60fc81a56f2.jpg)  
图 1 VLSFFD 模型整体框架结构

Fig. 1 The overall framework architecture of the VLSFFD model

# 2.1 损失函数

AE 的损失函数主要是重建误差损失组成，用来衡量重建帧和原始帧之间的差异。AE 的目标是通过从潜在空间随机采样来学习输入图像的有意义潜在表示，挖掘数据的潜在空间特征，并重建输入的视频帧，同时最小化 AE 损失。重建误差损失使用均方误差（MSE）进行度量，确保重建的帧与原始帧高度相似。AE 的损失函数定义如式(1)所示：

$$
{ \mathrm { L } } _ { _ { A E } } = { \mathrm { L } } _ { _ { M S E } } = { \frac { 1 } { N } } \sum _ { i = 1 } ^ { N } { \left| x _ { i } - x _ { i } ^ { \prime } \right| } _ { 2 } ^ { 2 }
$$

其中， $x _ { i }$ 是原始帧； $\mathbf { \lambda } _ { x _ { i } ^ { \prime } } ^ { }$ 是由 AE 解码器重建的帧； $N$ 是样本数量。

由于本文检测模型是二分类任务，最后分类只有两个类别（真实或伪造），使用大小为 2 的线性映射层，并采用交叉熵损失函数来计算分类损失，这对于处理二分类任务更为有效。交叉熵损失函数定义如式(2)所示：

$$
\mathrm { L } _ { c { \cal E } } = - [ y \log \hat { y } + ( 1 - y ) \log ( 1 - \hat { y } ) ]
$$

其中，若人脸视频帧已被篡改，则 $y$ 设为 1，否则设为 0。

综上，最终总损失函数是交叉熵损失和 AE损失的总和，如式(3)所示：

$$
\mathrm { L } = \alpha \mathrm { L } _ { _ { A E } } + \beta \mathrm { L } _ { _ { C E } }
$$

其中， $\alpha$ 和 $\beta$ 是权重系数，均默认为 1。

# 2.2  AE 帧重建

AE 是一种无监督神经网络模型，通常由编码器（Encoder）和解码器（Decoder）组成，旨在通过特征压缩与重建实现数据表征学习，从而挖掘数据的潜在空间特征。AE 重建帧模块在传统卷积自编码器基础上引入全局注意力模块（GLB），以增强对重要特征的关注能力。模块的整体结构如图 2 所示。

编码器通过卷积与下采样操作逐步提取输入图像的高维抽象特征，其结构由 5 级特征提取层和一个全局注意力模块构成。每层卷积层后接相同的 ReLU 激活函数和最大池化层，卷积层都采用 $3 { \times } 3$ 的卷积核，步长为 1，填充为 1，从而在每次卷积操作后将输入的宽度和高度缩小一半，同时逐步增加特征通道的维度，从 3 通道逐步扩展到 16、32、64、128，直到 256 通道。其次，在卷积层之后，引入了一种创新的全局注意力机制（GLB）应用于 256 通道的特征图，以捕捉图像中的全局依赖关系，进一步提升特征的表达能力。

在GLB 的注意力处理过程中，使用三个 $1 \times 1$ 卷积层分别生成查询（Q）、键（K）和值（V）向量。具体流程如下：先计算 Q 与 K 的点积，得到相似性得分矩阵。对得分矩阵进行缩放（Scaling）以稳定梯度，随后通过 Softmax 函数归一化生成注意力权重。利用注意力权重对 V 向量执行加权和操作，生成上下文增强的特征表示。随后将加权和结果与输入特征图通过残差连接相加，保留原始特征信息的同时融合全局注意力信息，最终输出优化后的特征图。GLB 的优势在于其能够建模远距离像素之间的相互关系,通过计算所有空间位置间的相似性，捕获全局上下文信息，弥补卷积操作的局部感受野限制。

解码器则是通过转置卷积（TransposedConvolution）逐步恢复空间分辨率，重建原始图像，其结构由 5 级转置卷积层构成。转置卷积通过可学习的上采样参数重建细节，相比插值法能更好地恢复高频信息。ReLU 激活增强非线性表达能力，末层使用 ReLU 约束输出值域为非负，适应图像像素特性。每层转置卷积层后接相同的ReLU激活函数，转置卷积层的卷积核大小为 $2 { \times } 2$ ，步长为 2，填充为 1。每层转置卷积逐步恢复分辨率至输入尺寸，在这一过程中，特征通道的维度逐渐减少，从 256 通道数减少到 64、32、16，末层输出 3 通道以匹配 RGB 图像空间。

![](images/df737ad961890b58af28c2510c686d3b91f32f8ce746dc8333728184808c003a.jpg)  
图 2 AE 帧重建模块结构

Fig. 2 The architecture of the Autoencoder-Based Frames Reconstruction Module

# 2.3 Efficient-Swin 特征处理

Efficient-Swin 特征处理模块是一种创新的卷积视觉 Transformer 架构，该网络模型结合了EfficientNet 和 Swin Transformer 的网络结构，能够有效地处理视频帧中的复杂视觉特征，如图 3中所示。先利用 EfficientNet 初步提取多尺度特征，再由 Swin Transformer 进一步提取全局特征。

EfficientNet 通过其创新的复合缩放策略和自动化的架构搜索，在保持较低计算量和参数量的同时，提供了高效的特征提取能力。同时复合缩放策略也增强了多尺度特征提取能力，极大地提升了特征表示能力。

![](images/b4dbbeda1b0be05e73cf0c171ccd30be5fe8c43b6c184bb12b61f74430d6bc0b.jpg)  
图 3  Efficient-Swin 特征处理 Fig. 3 Efficient-Swin Feature Processing

Swin Transformer 则是一种基于 Transformer架构的模型，其核心特性包括移位窗口机制和分层结构，通过自注意力机制有效地捕捉局部和全局特征，能够精确地识别伪造图像中的局部不一致性和全局模式变化，包含 4 个 Stage 的处理。

在 Efficient-Swin 特征处理模块中，重建视频帧和原始视频帧分别作为输入，经过两个相同结构的 Efficient-Swin 网络进行处理，这里的网络结构使用的是 EfficientNet_B0 规格的复合缩放和 Swin_Tiny 的 Swin Transformer 架构。在每个网络中，输入首先通过 EfficientNet_B0 进行初步的特征提取，随后通过一个混合嵌入模块将提取到的特征嵌入到一个紧凑且信息丰富的向量中 。 生 成 的 特 征 向 量 然 后 传 递 给 Swin

Transformer 进行进一步的处理，以便更精细地识别复杂的伪造特征。混合嵌入模块是该架构的关键设计之一，可有效地衔接 EfficientNet_B0 和Swin Transformer 之间的特征表示，不仅优化了两种架构之间的对接，同时也有效地增强了视觉特征提取的表现力。具体而言，混合嵌入模块从EfficientNet_B0 中输入特征图，并通过一个 $1 \times 1$ 的二维卷积层将特征图的通道维度缩减至768维的嵌入空间。接着，将得到的特征图展平并转置为一系列特征向量，这些向量随后被传递到SwinTransformer 中进行进一步的特征提取和处理。

# 2.4 自适应特征融合

自 适 应 特 征 融 合 模 块 （ Adaptive FeatureFusion Module, AFFM）旨在通过计算通道注意力权重对原始特征和重建特征进行加权融合，进一步融合特征。AFFM 整体框架结构如图 4 所示。

![](images/ad1adf5e0433973865835873ba8055fb173fa9e39a4ee63d30e153f37decc3f6.jpg)  
图 4  AFFM 特征融合模块框架结构Fig. 4 The framework structure of the AFFM

该模块首先将重建特征和原始特征先通过自适应平均池化操作对每个输入特征计算通道平均池化，获取每个通道的全局信息，然后通过两次卷积对通道数进行压缩和恢复，加入 ReLU激活函数增强非线性表达能力，通过 Sigmoid 生成通道注意力权重最后，利用计算出的注意力权重加权输入特征，并将它们加权融合在一起。

通道注意力机制通过学习每个通道的权重来提升特征表示的质量。在实际应用中，某些通道可能包含更多的信息，而其他通道可能包含噪声或不相关的特征。通过引入通道注意力机制，AFFM 特征融合模块能够使网络自动选择重要的通道，加权融合后能够在不同来源的特征之间建立联系，有效捕捉到多源特征间的相关性，从而实现更全面的信息集成。

# 3  实验与结果分析

# 3.1 数据集和视频预处理

本 模 型 使 用 的 数 据 集 包 括 DFDC 、FaceForensics $. + + ^ { [ 2 1 ] }$ 和 WildDeepfake[22]。

DFDC（DeepFake Detection Challenge）数据集是目前公开可用的最大深度伪造检测数据集，包含超过 十万个高分辨率的真实和伪造视频。这些视频涵盖了各种现实世界场景，涉及不同的拍摄角度、光照条件，并使用多种深度伪造技术生成。

FaceForensics $^ { + + }$ （ $\mathrm { F F ^ { + + } } .$ ）数据集包含一千个原始 YouTube 视频，采用了四种不同的面部操控方法进行处理。该数据集还提供了两种压缩质量：C23 和 C40。

WildDeepfake 数据集由数千个在现实环境中拍摄的深度伪造视频和图像组成，涵盖了最前沿的伪造技术，并模拟了在实际场景中可能发生的各种伪造类型，可评估并比较模型在现实世界场景中的对抗扰动能力。

预处理实验采用了更精细的视频预处理技术。与传统方法随机从视频中抽取单帧不同，本文应用了一种稀疏采样策略，从视频中选择多个连续帧，以捕捉重要的动态信息和不一致性。具体来说，对于每个视频，提取 15 帧，并随机选择了 3 个不重叠的连续帧组，每组包含 5 个连续的帧。为了有效地从视频中提取高质量的图像帧，并确保它们包含丰富的动态和空间信息，从而提高后续模型训练和检测的准确性，数据预处理过程中采取了以下步骤：

首先先进行人脸的检测与定位，使用OpenCV、face-recognition、BlazeFace 和 Dlib 等深度学习库中的功能，首先检测并定位视频中的人脸区域。为了精确裁剪人脸区域，利用 Dlib 提供的 shape_predictor 模型对人脸进行关键点检测，并根据这些关键点精确地定位和裁剪人脸区域。

其次再统一图像尺寸，每个提取的人脸图像帧都被调整为 $2 2 4 \times 2 2 4 \times 3$ 的尺寸，以满足网络输入要求。

最后，手动检查提取的人脸区域图像的质量。

对于质量较差的帧进行了剔除，确保后续处理阶段只使用高质量的帧图像。

# 3.2 评价指标和参数设置

实验中使用准确率（Accuracy, ACC）、曲线下面积（Area Under Curve, AUC）作为实验的一个评价指标，下面是每个指标的定义、计算方法和公式：

$F _ { A C C }$ 衡量的是模型预测正确的比例，即所有预测中，正确预测的比例，如式(4)所示：

$$
F _ { _ { A C C } } = \frac { N _ { _ { T P } } + N _ { _ { T N } } } { N _ { _ { T P } } + N _ { _ { T N } } + N _ { _ { F P } } + N _ { _ { F N } } }
$$

其中， $N _ { T P }$ 表示正确预测为伪造的伪造视频数量， $N _ { T N }$ 表示正确预测为真实的真实视频数量，$N _ { F P }$ 表示错误预测为伪造的真实视频数量， $\mathit { N } _ { \mathit { F N } }$ 表示错误预测为真实的伪造视频数量。

$F _ { A U C }$ 是通过计算不同阈值下的 $F _ { T P R }$ （TruePositive Rate）和 $F _ { F P R }$ （False Positive Rate），绘制ROC曲线计算面积，通过调整分类阈值，从而表示模型在不同阈值下的分类性能。 $F _ { A U C }$ 值越大，表示模型越能区分真实和伪造视频的能力。计算公式分别如式(5)—式(7)所示：

$$
F _ { _ { T P R } } = \frac { N _ { _ { T P } } } { N _ { _ { T P } } + N _ { _ { F N } } }
$$

$$
F _ { _ { F P R } } = \frac { N _ { _ { F P } } } { N _ { _ { F P } } + N _ { _ { T N } } }
$$

$$
F _ { _ { A U C } } = \int _ { 0 } ^ { 1 } F _ { _ { T P R } } ( F _ { _ { F P R } } ) d F _ { _ { F P R } }
$$

其中， $F _ { T P R }$ 是实际为正类的样本中，被模型正确预测为正类的比例; $\mathop { F _ { F P R } }$ 是实际为负类的样本中，被模型错误预测为正类的比例。

# 3.3 实验环境

具体的实验细节如下：实验使用timm库来加载EfficientNet_B0和Swin_Tiny的类定义及其预训练模型权重。同时在整个模型训练过程中，采用Adam优化器进行参数优化，学习率设置为0.001。为了增强训练数据的多样性和鲁棒性，使用了 Albumentations 库中的一系列数据增强技术，例如向图像添加高斯噪声（GaussNoise）和随机裁剪（RandomCrop）图像区域等。模型的批次大小设置为16，并在所有数据集上进行了50轮训练。为了训练、验证和测试我们的模型，视频经过预处理转换成帧图像后，这些图像被按大约80:15:5的比例划分。本文的主要实验是在视频上进行的（与单个帧相比）,同时为了进行评估，我们从DFDC、 $\mathrm { F F ^ { + + } }$ 和WildDeepfake数据集中分别抽取了3,200个视频用于测试。以上实验均基于PyTorch在Linux平台的NVIDIA RTX 3090 GPU上进行实现。

# 3.4  模型的时空复杂度

# 3.4.1 时间复杂度

我们采用 FLOPs（浮点运算次数）作为核心指标来量化模型的计算复杂度。在标准实验配置下（输入分辨率 $2 2 4 \times 2 2 4$ 、NVIDIA RTX 3090 GPU、批次大小 16），对各模块进行了细致的计算开销分析，结果详见表 1。本实验采用混合精度训练（FP16）优化计算效率，所有 FLOPs 值均为实际测量结果。

Swin-Tiny 的注意力机制作为计算量最大的组件，这主要源于其独特的窗口多头注意力（W-MSA）机制和 MLP 层的密集矩阵运算。具体而言，在 $4 { \times } 4$ 的窗口划分策略下，每个窗口需要计算 8 个注意力头的交互，这种精细的全局建模能力虽然计算代价较高，但对捕捉细微的伪造痕迹至关重要。AE 帧重建模块的编码器部分作为第二大计算瓶颈，其高计算量来自于多级下采样过程中的卷积运算和特征变换，特别是最后一层的$3 \times 3$ 卷积核在 256 维特征空间上的密集计算。实验发现，该模块提取的潜在空间特征对低频伪造伪影的检测具有不可替代的作用。

表 1  模型时间复杂度分析  

<html><body><table><tr><td colspan="2">Table 1 Time complexity analysis of the model</td></tr><tr><td>模块</td><td>FLOPs 计算量来源 (G)</td></tr><tr><td>AE</td><td>编码器(3.1G)+解码 3.2 器(0.1G)</td></tr><tr><td>EfficientNet- B0</td><td>MBConv深度可分 0.39 离卷积</td></tr><tr><td>Swin-Tiny</td><td>窗口多头注意力(W- 4.5 MSA)+MLP</td></tr><tr><td>AFFM</td><td>通道注意力+MLP 0.08</td></tr><tr><td>分类头</td><td>全连接层 0.01</td></tr><tr><td>总计</td><td>8.18</td></tr></table></body></html>

# 3.4.2 空间复杂度

我们通过模型参数量评估VLSFFD的计算存储需求，以衡量其空间复杂度。如表2所示，模型总参数量为 $4 5 . 7 \mathrm { M }$ ，在保证多模态融合能力的同时有效控制了计算成本。AE帧重建模块包含5.3M参数，由编码器和解码器负责关键的重建特征提取；Efficient-Swin特征处理模块采用轻量化设计， EfficientNet-B0的5.3M参数主要分布在16个MBConv块中，每个块包含扩展、深度卷积、压缩的三阶段结构，而Swin-Tiny的28M参数则集中在4个stage的Transformer层，混合嵌入模块（0.5M）也有效桥接了两种架构的特征空间，总参数量达33.8M，平衡了计算效率与特征表达能力；而AFFM仅需0.8M参数，分为通道注意力部分和MLP，实现了高效的多模态特征融合。

Table 2 The parameter quantities of each module of the   
表 3  数据集内的实验结果  

<html><body><table><tr><td colspan="3">model</td></tr><tr><td>模块</td><td>子模块</td><td>参数量 占比 (%)</td></tr><tr><td rowspan="2">AE</td><td>编码器</td><td>(M) 3.2 7.0</td></tr><tr><td>解码器 2.1</td><td>4.6</td></tr><tr><td rowspan="3">Efficient- Swin</td><td>EfficientNet-B0</td><td>5.3 11.6</td></tr><tr><td>Swin-Tiny</td><td>28.0 61.3</td></tr><tr><td>混合嵌入模块</td><td>0.5 1.1</td></tr><tr><td>AFFM</td><td>通道注意力 +MLP</td><td>0.8</td><td>1.8</td></tr><tr><td>分类头</td><td>ReLU+FC</td><td>0.3</td><td>0.7</td></tr></table></body></html>

# 3.5 对比实验结果与分析

# 3.5.1 数据集内的对比实验

为全面评估VLSFFD模型的检测性能，我们在FF $^ { + + }$ 、DFDC和WildDeepfake三个基准数据集上进行了严格的内部测试。如表3所示，模型在所有测试场景中均展现出卓越的性能表现。

具体而言，在FF $^ { + + }$ 数据集测试中，模型在HQ（高质量）和LQ（低质量）两种条件下对四种不同伪造类型（Deepfakes、Face2Face、FaceSwap和NeuralTextures）进行了综合评估。

实验结果显示，VLSFFD以平均 $0 . 2 6 \%$ 的优势取得了最高的ACC（准确率）指标。在更具挑战性的DFDC数据集上，模型在VCC和AUC的两项关键指标上均达到最优水平，其中ACC提升$0 . 1 5 \%$ ，AUC提升 $0 . 6 5 \%$ 。同样，在WildDeepfake数据集测试中，模型继续保持领先优势，ACC和AUC分别提升0.17%和 $0 . 0 9 \%$ 。实验结果充分证明，VLSFFD模型不仅具备优异的伪造视频检测能力，还在不同数据分布下展现出强大的泛化性能，为实际应用场景中的深度伪造检测提供了可靠解决方案。

表 2  模型各模块参数量  
Table 3 Results of experiments within the dataset   

<html><body><table><tr><td>FF++ 模型方法</td><td></td><td>DFDC</td><td>WildDeepfake</td><td></td></tr><tr><td>ACC(%)</td><td>AUC(%) ACC(%)</td><td>AUC(%)</td><td>ACC(%)</td><td>AUC(%)</td></tr><tr><td>SMIL[10], ACM MM 2020 95.51</td><td>97.45</td><td>91.67 93.13</td><td>81.38</td><td>86.29</td></tr><tr><td>STIL[11],ACM MM 2021 95.79</td><td>97.59 91.17</td><td>94.74</td><td>83.96</td><td>88.19</td></tr><tr><td>FTCN[12], CVPR 2021 97.18</td><td>98.89 94.62</td><td>97.88</td><td>87.68</td><td>91.86</td></tr><tr><td>DIL[14], AAAI 2022 97.27</td><td>99.59 94.19</td><td>97.41</td><td>85.27</td><td>91.09</td></tr><tr><td>ECGL[15], ICASSP 2022 96.41 ISTVT[16], TIFS 2023</td><td>98.54</td><td>97.67 95.82</td><td>86.07</td><td>92.42</td></tr><tr><td>96.00</td><td>96.70</td><td>92.10</td><td>1</td><td></td></tr><tr><td>DIP[13], IEEE MM 2024 95.72</td><td>97.31</td><td>1</td><td>85.60</td><td>91.12</td></tr><tr><td>SLF[19], CVPR 2024</td><td>98.40</td><td>= 96.10</td><td></td><td></td></tr><tr><td>VLSFFD(Ours) 97.53</td><td>99.26</td><td>97.82</td><td>98.53 87.85</td><td>92.51</td></tr></table></body></html>

# 3.5.2 跨数据集的对比实验

为了进一步评估 VLSFFD 模型在跨数据集场景下的泛化能力，特别是针对未知伪造技术的检测性能，实验设计了一套严格的跨数据集测试方案。

具体而言，模型首先在 $\mathrm { F F ^ { + + } }$ 数据集上进行全面训练，学习基础的伪造特征表示；随后，在不进行任何微调的情况下，直接在包含全新伪造技术的 DFDC 数据集上进行测试。这种实验设计能够有效验证模型对未知伪造方法的适应能力和特征泛化性能。实验结果如表 4 所示，VLSFFD模型在跨数据集测试中展现出显著的性能优势：在关键评价指标方面，VCC 和 AUC 均达到对比模型中的最高水平，其中分类准确率 ACC 提升$2 . 2 9 \%$ ，AUC 提升 $1 . 0 6 \%$ 。这一结果不仅证实了模型架构的有效性，更体现了其在处理跨数据集、未知伪造技术时的强大鲁棒性。值得注意的是，这种性能提升是在不依赖目标域数据的情况下实现的，说明模型具有强大的泛化能力，学习到的深度伪造特征具有优异的可迁移性，为实际应用中可能遇到的新兴伪造技术检测提供了可靠解决方案。

表 4  跨数据集的实验结果Table 4 Results of experiments across datasets  

<html><body><table><tr><td rowspan="2">模型方法</td><td>FF++训练 DFDC 测试</td></tr><tr><td>ACC(%) AUC(%)</td></tr><tr><td>SMIL[10]</td><td>64.74 66.79</td></tr><tr><td>STIL[11]</td><td>65.95 67.22</td></tr><tr><td>FTCN[12]</td><td>68.80 74.00</td></tr><tr><td>DIL[14]</td><td>70.93 72.52</td></tr><tr><td>ECGL[15]</td><td>71.39 73.09</td></tr><tr><td>ISTVT[16]</td><td>72.60 74.20</td></tr><tr><td>DIp[13]</td><td>72.81 79.90</td></tr><tr><td>SLF[19]</td><td>- 80.80</td></tr><tr><td>VLSFFD</td><td>73.68 80.96</td></tr></table></body></html>

3.6  消融实验结果与分析

为验证 VLSFFD 各模块的贡献，我们在$\mathrm { F F ^ { + + } }$ 训练、DFDC 跨数据集测试的设定下进行了四组消融实验：

VLSFFD- $\mathbf { \nabla } \cdot \mathbf { a }$ ：移除 AE 帧重建模块。

VLSFFD- $\cdot \beta$ ：移除 EfficientNet_B0。

VLSFFD- $\cdot \gamma$ ：移除 Swin Transformer。

VLSFFD- $\cdot \sigma$ ：移除 AFFM 模块，仅使用特征加权。

如表 5 所示的实验结果表明，任一模块的缺失都会导致性能显著下降，其中 AE 模块的移除影响最大（VLSFFD- $\mathbf { \nabla } \cdot \mathbf { a }$ ），验证了潜在空间特征的重要性。结果充分证明各模块的必要性：AE 模块挖掘深层伪造痕迹，Efficient-Swin 实现高效特征提取，AFFM 优化多模态融合，三者协同工作显著提升了检测性能。

表 5  消融实验结果  
Table 5 Results of ablation experiments   

<html><body><table><tr><td rowspan="2">变体模型实验</td><td colspan="2">FF++训练</td></tr><tr><td>DFDC 测试</td><td></td></tr><tr><td>VLSFFD-α</td><td>ACC(%)</td><td>AUC(%) 72.87</td></tr><tr><td>VLSFFD-β</td><td>67.32 71.84</td><td>76.51</td></tr><tr><td>VLSFFD-Y</td><td>70.33</td><td>75.10</td></tr><tr><td>VLSFFD-0</td><td>70.28</td><td>75.02</td></tr><tr><td>VLSFFD</td><td>73.68</td><td>80.96</td></tr></table></body></html>

# 3.7  可视化实验结果与分析

为了更好地展示模型在挖掘视觉不一致性以及对伪造特征提取上的优势。针对于融合处理之后的特征，使用 Grad-CAM 对所用数据集中的一些示例进行了可视化，将所提取的视频连续帧提取了一部分连续帧并生成了热图，其中颜色越亮越暖表示伪造的可能性越高，如图 5 所示。从结果来看，所提出的模型关注面部区域，并且能够很好地捕捉帧间的不一致性。实验表明了本文所提模型框架，除了关注眼睛、嘴巴、鼻子和耳朵等等器官的局部空间不一致性外，也能够微妙地捕捉到全局不一致性。

![](images/3570c46f6e575e56a8360c538f1e70ad6f4ab071f608763c2b447238819e444f.jpg)  
图 5  可视化结果分析  
Fig. 5 Visualization Result Analysis

# 4  结论

本文所提出的 VLSFFD 模型框架在利用卷积视觉 Transformer 进行视觉伪影特征提取的基础上，集成了 AE 帧重建，挖掘数据的潜在空间特征，以准确识别伪造和真实内容之间的不一致特征，同时全局注意力模块使其能够从全局特征中进行特征分析；最后通过将视觉和潜在空间特征进行融合，充分提高伪造视频检测的泛化能力。通过在包括 DFDC、FF $^ { + + }$ 和 WildDeepfake 在内的多样化数据集上的大量实验，本文模型在ACC、AUC 两项指标上展示了优异的性能，优于其他现有模型，体现了出色的实用性和鲁棒性。实验结果表明，充分挖掘视觉和潜在空间特征并将其进行融合，可以有效的提高模型检测性能。

然而，本文研究也存在一定的局限性，尚未完全实现对模型参数效率的优化，未来将探索一些提高效率的方法，希望进一步减少参数数量。同时探索对于帧序列进行关键帧确定的方法，显著降低模型训练量。

参考文献(References):   
[1] 李晓龙,俞能海,张新鹏,等.数字媒体取证技术 综述[J].中国图象图形学报, 2021,26(6):1216- 1226. LI Xiaolong, YU Nenghai, ZHANG Xinpeng, et al. Overview of digital media forensics technology [J]. Journal of Image and Graphics, 2021,26(6):1216-1226.   
[2] 李旭嵘,纪守领,吴春明,等.深度伪造与检测技 术综述[J].软件学报, 2021,32(2):496-518. LI Xurong, JI Shouling, WU Cunming, et al. Survey on deepfakes and detection techniques [J]. Journal of Software, 2021,32(2): 496-518.   
[3] JIANG L, LI R, WU W, et al. Deeperforensics1.0: A large-scale dataset for real-world face forgery detection [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Piscataway: IEEE Press, 2020: 2889-2898.   
[4] KORSHUNOVA I, SHI W, DAMBRE J, et al. Fast face-swap using convolutional neural networks [C]//Proceedings of the IEEE International Conference on Computer Vision. Piscataway: IEEE Press, 2017: 3677-3685.   
[5] KAUR A, NOORI HOSHYAR A, SAIKRISHNA V, et al. Deepfake video detection: challenges and opportunities [J]. Artificial Intelligence Review, 2024, 57(6): 159.   
[6] 瞿远近,吴起.基于改进高斯滤波网络的深度 伪造检测方法[J].重庆工商大学学报(自然科 学版), 2023, 40(4):41-47. QU Yuanjin, WU Qi. Depth Forgery Detection Method Based on Improved Gaussian Filter Network[J]. Journal of Chongqing Technology and Business University(Natural Science Edition), 2023, 40(4):41-47   
[7] LIU H, LI X, ZHOU W, et al. Spatial-phase shallow learning: rethinking face forgery detection in frequency domain [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. Piscataway: IEEE Press, 2021: 772-781.   
[8] ZHAO T, XU X, XU M, et al. Learning selfconsistency for deepfake detection [C]//Proceedings of the IEEE International Conference on Computer Vision. Piscataway: IEEE Press, 2021: 15023-15033.   
[9] WANG J, WU Z, OUYANG W, et al. M2tr: Multi-modal multi-scale transformers for deepfake detection [C]//Proceedings of the 2022 International Conference on Multimedia Retrieval. 2022: 615-623.   
[10]LI X, LANG Y, CHEN Y, et al. Sharp multiple instance learning for deepfake video detection [C]//Proceedings of the 28th ACM International Conference on Multimedia. NewYork: ACM, 2020: 1864-1872.   
[11]GU Z, CHEN Y, YAO T, et al. Spatiotemporal inconsistency learning for deepfake video detection [C]//Proceedings of the 29th ACM International Conference on Multimedia. NewYork: ACM, 2021: 3473-3481.   
[12]ZHENG Y, BAO J, CHEN D, et al. Exploring temporal coherence for more general video face forgery detection [C]//Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition. Piscataway: IEEE Press, 2021: 15044-15054.   
[13]NIE F, NI J, ZHANG J, et al. DIP: diffusion learning of inconsistency pattern for general deepfake detection [J]. IEEE Transactions on Multimedia. Piscataway: IEEE Press, 2024, 15(1): 1-13.   
[14]GU Z, CHEN Y, YAO T, et al. Delving into the local: Dynamic inconsistency learning for deepfake video detection [C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2022, 36(1): 744-752.   
[15]ZHAO X, YU Y, NI R, et al. Exploring complementarity of global and local spatiotemporal information for fake face video detection [C]//ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing. Piscataway: IEEE Press, 2022: 2884-2888.   
[16]ZHAO C, WANG C, HU G, et al. ISTVT: interpretable spatial-temporal video transformer for deepfake detection [J]. IEEE Transactions on Information Forensics and Security, 2023, 18(2): 1335-1348.   
[17]BEN AISSA F, HAMDI M, ZAIED M, et al. An overview of gan-deepfakes detection: proposal, improvement, and evaluation [J]. Multimedia Tools and Applications, 2024, 83(11): 32343- 32365.   
[18]KHALID H, WOO S S. Oc-fakedect: Classifying deepfakes using one-class variational autoencoder [C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops. Piscataway: IEEE Press, 2020: 656-657.   
[19]JONGWOOK CHOI, TAEHOON KIM, YONGHYUN JEONG, et al. Exploiting style latent flows for generalizing deepfake video detection [C]//Proceedings of the IEEE International Conference on Computer Vision and Pattern Recognition. Piscataway: IEEE Press, 2024: 1133-1143.   
[20]AGHASANLI A, KANGIN D, ANGELOV P. Interpretable-through-prototypes deepfake detection for diffusion models [C]//Proceedings of the IEEE International Conference on Computer Vision. Piscataway: IEEE Press, 2023: 467-474.   
[21]ROSSLER A, COZZOLINO D, Verdoliva L, et al. Faceforensics $^ { + + }$ : learning to detect manipulated facial images [C]//Proceedings of the IEEE International Conference on Computer Vision. Piscataway: IEEE Press, 2019: 1-11.   
[22]ZI B, CHANG M, CHEN J, et al. Wilddeepfake: a challenging real-world dataset for deepfake detection [C]//Proceedings of the 28th ACM International Conference on Multimedia. NewYork: ACM, 2020: 2382-2390.