# 基于轻量级光流法的深度伪造视频检测方法

耿浩琦，　 张建岭，　 丁博文（中国人民公安大学信息网络安全学院， 北京　 100038）

摘　 要　 近年来，随着深度伪造技术的快速发展，伪造视频逼真程度日渐提高，社会影响也日愈严重。 然而，传统的伪造视频检测方法强调单个视频帧的帧内属性，较少从帧间关系中学习时间信息，运行过程中可能产生巨大内存和计算成本。 因此，为减小模型运行参数量和提高检测准确率，设计了一种基于轻量级光流法的伪造人脸视频检测方法。 模型由特征提取、光流推断、分类检测模块构成。 首先，在特征提取模块，通过对输入的图像进行高维特征提取；其次，在光流推断模块，引入一种自适应流场的估算方法进行光流推断；最后，在分类检测模块，对 Xcep⁃tion 网络结构进行优化，同时使用一种轻量化分离卷积模块对前馈网络进行改进，网络浮点运算次数缩小至原来的$2 4 \%$ ，解决了参数量大以及特征捕捉不充分的问题。 所提出的光流模型较传统 FlowNet2 在参数量上缩小到原来的$3 \%$ ，运行时间缩短到原来的 $4 2 \%$ 。 实验结果表明，所提出的检测模型在主流数据集 $\mathrm { F F + + }$ 上的跨域实验平均AUC 和 ACC 分别达到了91. 3 和81. 5，证明了算法的有效性和泛化性。

关键词　 深度伪造检测； 深度学习； 光流法； 卷积神经网络； 自适应估算； 轻量级中图分类号 D918. 2 文献标志码　 A

# Deepfake Video Detection Method Based on Lightweight Optical Flow Method

GENG Haoqi，　 ZHANG Jianling，　 DING Bowen（School of Information and Cyber Security， People's Public Security University of China， Beijing 100038， China）

Abstract： In recent years， with the rapid development of deepfake technology， the realism of fake videos has been continuously improving， and their social impact has become increasingly serious． However， tra⁃ ditional fake video detection methods emphasize the intra⁃frame attributes of a single video frame and rare⁃ ly learn the temporal information from the inter⁃frame relationship， which may incur huge memory and computational costs during operation． Therefore， a fake face video detection method based on lightweight optical flow method was designed to reduce the number of running parameters in model and improve the detection accuracy． The model consisted of three modules： feature extraction， optical flow inference and classification detection． Firstly， the high⁃dimensional features of the in⁃put image were extracted in the feature extraction module． Secondly， an adaptive flow estimation method was introduced for optical flow inference in the optical flow inference module． Finally， in the classification detection module， the struc⁃ ture of Xception network was optimized， and a lightweight separate convolution module was used to im⁃ prove the feedforward network． The number of floating⁃point operations in the network was reduced to $24 \%$ of the original， addressing the issues of large parameter volume and insufficient feature capture．

Compared with the traditional FlowNet2， the number of parameters of the proposed optical flow model was reduced to $3 \%$ of the original， and the running time was shortened to $42 \%$ of the original． The experi⁃ mental results show that the proposed detection model achieves average AUC and ACC of 91. 3 and 81. 5 respectively in the cross⁃domain experiments on the mainstream dataset $\mathrm { F F ~ + ~ + ~ }$ ， proving the effective⁃ ness and generalization of the algorithm．

Key words： deepfake detection； deep learning； optical flow； CNN； adaptive estimation； lightweight

# 0 引言

深度伪造（Deepfake）技术，是制作假视频的关键技术之一，也称为 AI 换脸技术。 这种技术一旦被攻击者恶意利用，将对社会的政治、经济造成不可预知的后果，典型的案例即俄罗斯和乌克兰总统的深度伪造视频被恶意上传到社交媒体上，从而推动针对目标国家民众和国家利益的虚假叙事，冲击目标国家民众的心理。 更有甚者将一些知名歌星、影星等公众人物的脸“移花接木”到色情明星身上，伪造色情片非法牟利，对个人名誉权与肖像权构成严重侵害。 目前主流的伪造方法是运用生成对抗网络（Generative Adversarial Network， GAN） 或 卷 积 神 经网络（ Convolutional Neural Network，CNN） 等机器学习算法，通过大量的编码和解码训练，增强模型的鲁棒性，最后实现将一张图片的人脸面部特征成功地“移植”到另一张图片的人脸上。 由于视频是由连续的帧构成的，只需替换每张图片中的人脸面孔，就可以获得一个新的、非常逼真的伪造视频。 具体地说，首先对被模仿人脸的视频进行逐帧变换，得到大量照片，然后用人脸面部代替被模仿人脸的面部，最后对更换后的照片进行假视频再合成，达到欺骗人眼、产生错误认知的目的。

尽管目前主流的检测手段很大程度上基于深度学习技术，但深度学习固有的局限性已经成为制约深度伪造视频检测技术进步的关键因素。 基于空域和频域的检测缺乏泛化性能，且忽略了视频帧之间的时序的特征。 基于其他的检测方法提高了检测准确率但却忽视了模型成本的巨大，实现起来很难部署。 基于频域的检测方法在跨库测试中表现欠佳，鲁棒性能有待提高。 目前现有的研究都是通过增大网络结构，利用大模型、提升复杂度来提升检测效果，导致模型的参数量、成本、运行时间较大，冗余模块较多。 如果考虑到将模型部署到实时检测的服务器上，其实现难度较大。 在此基础上，文本提出了一种基于轻量级光流法的伪造人脸视频检

测方法。

本文的主要工作和贡献如下：

（1）针对现有深度伪造检测方法存在的忽略帧间的时序关系、参数量大和处理速度慢等问题。 提出了一种轻量级的光流模型来提取伪造视频中的时序特征，同时将自适应流场的估算方法引入到光流推断模块当中，从而提高了检测速度。

（2）针对传统分类网络参数量大，分类检测模块冗余，特征提取不充分等问题，本文在 Xception 网络的基础上进行了剪枝操作，优化网络框架。 同时提出了一种轻量化分离卷积模块（Lightweight Sepa⁃ration Convolution Block，LSCB） 应用于网络中的特征提取。

# 1　 相关工作

传统的检测方法中，深度伪造检测方法分为基于空间特征和基于频域特征以及其他检测方法［1］。基于空间特征即针对视频分解为帧的基础上，以每帧为对象进行处理，挖掘其在空间特征上潜在的伪影，提取空域特征进行分析。 基于频域特征即集中挖掘图像高频信号、相位谱等，利用提取到的频域特征或进行特征融合后进行分析。 其他检测方法主要是通过区块链、生物特征等区别传统方法的形式，通过提取视频关键信息或者嵌入水印，而后检测视频完整性的一种方法。

# 1. 1　 基于空间特征的检测方法

Sabir 等［2］用 CNN 先对各帧图像提取特征，再用循环神经网络对相邻帧图像间时序关系进行挖掘，即用循环神经网络整合时空特征来检测图像。Zheng 等［3］以人为方式设定卷积核时间维度，将卷积核长宽维度设为 1，从而可以专注于时间维度的特征提取。 Guo 等［4］提出了一种分层多特征融合的人脸图像伪造检测策略，采用多个分支的特征提取网络，确保每个分支专注于学习特定的伪造特征层次，进而对不同层次的伪造特征进行分类。 俞洋等［5］通过提取人脸的关键点来定义人脸的关键区域，并引入上下文注意力机制来增强模型对这些关键区域的关注。 这样做可以有效降低无关区域对预测结果的干扰。 虞楚尔等［6］分析了伪造图像内在的身份特征缺失，提出基于真实人脸计算伪造人脸的身份信息损失的检测方法。

# 1. 2　 基于频域特征的检测方法

Doloriel 等［7］探索深度伪造检测的蒙版图像建模，提出了一种通过频率掩蔽的新型深度伪造检测器。 Song 等［8］提出一个用于在跨域人脸伪造检测的深度学习框架。 通过跨多个帧的相关表示以及来自 RGB 和频率域的互补线索来挖掘潜在的一致性。 Li 等［9］提出了一种新的单中心损失 SCL，在约束自然人脸和篡改人脸类间的离散性的同时，仅聚合类内差异较小的自然人脸。 Tang 等［10］捕捉视频帧的时空特征，使用时空极余弦变换来提取相关特征。

与传统的针对单一空域和频域特征的方法较为不同，Hasan 等［11］发明了一个基于区块链的视频真实和完整性验证机制，通过追踪链上的地址来验证视频的真伪性，但其代价和成本相较传统机器学习要高。 Shao 等［12］提出了 DGM4 大模型结构，进一步考虑了定位图像篡改区域。 Agarwal 等［13］提出了一个综合人耳静态与动态特征的辨析策略，通过关注特定面部部分以聚焦关注点，以增强伪造人脸检测的广泛适用性。 Liu 等［14］研究者提出了一个名为时间身份不一致网络的架构，旨在通过发掘同一身份视频中人脸之间的差异性来辨识伪造视频。

# 1. 3 光流法

光流法是图像处理和计算机视觉领域的一项技术，其作用是分析和确定图像内部物体的运动状态。光流本质上是图像中每个像素点的运动向量，揭示了物体在场景中的移动情况。 通过比较连续图像帧中相邻像素的变化，可以计算出光流矢量，这些变化揭示了场景中物体的运动。

在计算光流矢量的过程中，通常会关注像素亮度的变化以及空间和亮度梯度之间的变化。 利用这些信息，可以通过解决诸如 Lucas⁃Kanade 方法或Horn⁃Schunck 方法等优化问题来确定每个像素点在水平和垂直方向上的运动分量。 这些分量最终定义了光流矢量的大小和方向，描述场景中物体运动的详细信息。 如图1 所示，每个圆圈的颜色逐渐加深，代表不同的位移程度。 通过观察这些圆圈相对于光流矢量的位置变化，可以推断出物体的运动轨迹和速度变化情况。

光流矢量O 8 432

光流的分类基本上分为两种：稠密光流、稀疏光流。 稀疏光流主要应用在目标跟踪、运动分析、AR和 VR 技术中理解用户的视线以及在计算机动画中控制物体的运动；稠密光流技术，作为一种在图像或特定区域内对每个像素点进行精确配准的方法，能够测定图像上每个像素的运动矢量，从而生成一个详尽的稠密光流场。 这项技术广泛应用于运动监测与分析、视频内容解析、以及视频中物体和场景变化的检测与跟踪。 本文通过应用稠密光流技术处理连续帧，利用得到的光流场，精确提取人脸时序特征，从而有效捕捉到人脸伪造的痕迹。

# 2　 基于轻量级光流法的伪造人脸视频检测

现有的伪造视频检测方法强调单个视频帧的帧内属性，未能从帧间关系中学习时间信息，导致跨库测试中泛化性能力较差，同时也未能考虑到运行过程中耗费的巨大内存和计算成本。 在视频的压缩过程中，时序信息隐藏在帧与帧之间［15］，而光流法可以针对上下文的特征纹理信息捕捉其帧间时序信息，以增强模型的鲁棒性和泛化性。 因此，本文基于光流法构建了一种基于轻量级光流法的深度伪造检测模型。 整体模型如图2 所示。

![](images/e2c6b40af5466b392e90c39c50c3bba7c392631a4b9f6cda275432a0c3ef4d65.jpg)  
图1　 光流矢量方向表示运动方向  
图2　 基于轻量级光流法的深度伪造视频检测模型

Mini＿flowNet 的设计主要分为两个核心模块：特征提取模块和光流推断模块。 在特征提取模块当中通过一个双流网络，对所输入的前后帧进行计算高维特征。 在光流推断模块中，使用一种自适应的流场估计算法，对在上一模块中习得的时序特征进行平均特征融合和光流推断。

为了降低模型的可训练参数量，增加模型的泛化性并防止过拟合，对 Xception 网络的原有框架结构进行简化。 通过删减了中间部分的特征提取层，从而降低了模型的复杂度。 此外，使用了一种轻量化分离卷积模块，对前馈神经网络进行改进，使其能够更高效地提取特征

# 2. 1　 基于 Retinaface 的人脸数据预处理

在深度伪造视频的过程中，通常会进行逐帧的处理，所以原始视频的帧当中通常含有大量的不相关信息。 因此，本文在处理需要检测的视频前，首先使用等间隔抽样技术从视频中筛选出33 帧，然后应用 Mini＿flowNet 模型进行光流图的计算，输入两张图片，经过光流计算后可产生一张光流图，最终得到32 张光流图。 Retinaface 算法融合了外部监督与自我监督的多任务学习策略，可以对不同大小的人脸进行高精度的像素级定位，确保算法能够适应各种大小和复杂环境下的人脸识别。 为了保证输入图像大小，并增强模型的鲁棒性，对提取的人脸图像进行缩放，设置输出 $2 9 9 \times 2 9 9$ 像素的固定尺寸的人脸图像作为后续模型的输入。

# 2. 2 改进的轻量级 Mini＿flowNet

传统的 FlowNet 将两张图像按通道维重叠后输入，由两个紧凑的子网络组成。 受到 Liteflownet［16］和 FlowNet［17］ 的启发，本文设计了一种轻量级的Mini＿flowNet 架构。 首先对 Liteflownet 中的特征提取模块和光流推断模块进行缩减，将 6 层的网络结构改成两层进行训练。 其次在进行流场估计时，对匹配模块和特征细化模块的计算公式进行了优化。

框架的核心包括两个子网络。 这两个子网络分别被命名为 NetA 和 NetB。 NetA 负责将输入的图像进行高维特征提取。 NetB 由流推理和正则化组成，用于从粗略到精细层面的光流推断并生成光流图。Mini＿flowNet 整体框架如图 3 所示。

![](images/8606df8000750461e1d9ed8a79a03cc3efa72314b15734e4eee0cf9f47374603.jpg)  
图 3　 Mini＿flowNet 整体框架

# 2. 2. 1 特征提取模块

参照传统网络设计方法［18］，特征提取网络设计成双流网络架构，每一个流都用来描述特征，网络通过一系列的处理步骤，将输入的图像转换为不同尺度的多维特征表示。

在特征提取过程，从伪造图像 I1 和 I2 提取到的高维特征 F（I1）和 F（I2）推断出伪造图像流场。借助传统方法中解决图像位移的方法，采取特征扭曲（f⁃warp） 即线性插值的方法来减少 F（I1） 和F（I2）之间的特征距离，起到光流修正的作用。F（I2）通过 f⁃warp 向 F（I1）扭曲， $\operatorname { f } .$ ⁃warp 可表示为：$\widetilde { F } ( X ) = \sum _ { X _ { s } ^ { i } \in N ( X _ { s } ) } F ( X _ { s } ^ { i } ) \left( 1 - \vert x _ { s } - x _ { s } ^ { i } \vert \right) \left( 1 - \vert y _ { s } - y _ { s } ^ { i } \vert \right)$

其中，

$$
\boldsymbol { X } _ { s } = \boldsymbol { X } + \dot { \boldsymbol { X } }
$$

式中， ${ \widetilde { F } } ( X )$ 是扭曲函数，表示在目标坐标的插值结果， $X _ { s }$ 表示输入特征图中原始特征点的源坐标，$N ( X _ { s } )$ 表示 $X _ { s }$ 的4 个邻居像素点， $x _ { s }$ 和 $y _ { s }$ 分别表示$X _ { s }$ 的横纵坐标， $x _ { s } ^ { i }$ 和 $y _ { s } ^ { i }$ 分别表示 $X _ { s } ^ { i }$ 的横纵坐标， $X$ 表示插值特征图上的特征点的目标坐标， $\dot { X }$ 表示子像素的位移量。 通过插值方法获得扭曲后的特征，可有效减少特征之间的位移偏差，更好地提取伪造图像之间的光流特征。

# 2. 2. 2 光流推断模块

模型通过神经网络的特征提取，已提取了双通道的空间基本特征，但对其时序特征建模还不够充分。 将来自相邻帧的空间特征按照一定规则计算其相关性，从而得到时序特征。 在效果上可以达到捕捉有用的时序信息而忽略无关的背景信息。 光流推断网络结构主要由两个部分组成：推理模块和正则化模块。

在推理模块 M：受 Liteflownet3［19］ 自适应空间特征聚合方法［20］的启发，对流场的计算匹配过程进行优化，提出一种自适应流场的估算方法（EMAF）用来计算光流场，对特征相关性计算之后进行特征融合，以达到平均特征的目的，自适应流场的估算方法可表示为公式（3）：

$$
X _ { _ m } = \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \ M _ { i } \big ( C \big ( F ( I 1 ) , \widetilde { F } ( I 2 ) \big ) \big ) + S \dot { X }
$$

式中， $X _ { m }$ 是融合后的光流特征， $N$ 是描述符的个数，实验中选择 $2 , M _ { i }$ 是权重因子， $s$ 是平滑项，用于平滑特征， $\dot { \boldsymbol X }$ 表示特征图各像素的位移量， $\boldsymbol { c }$ 是成本量，是通过高级特征 $F ( I 1 )$ 和扭曲后特征 $\widetilde { F } ( I 2 )$ 的相关性计算得出的，成本量的计算过程如公式（4）：

$C ( F ( a ) , F ( b ) ) = F ( a ) F ( b ) / L$ （4）式中， $L$ 是特征向量的长度。 随后参考 Liteflownet的基本思路，将其送入正则化模块，最终输出光流特征，转化为光流图送入分类网络进行判别。

双流网络的设计使得空间特征在不同分辨率下得到有效提取，控制了模型的复杂度，确保了从高到低空间分辨率的信息都被充分考虑，增强了特征的表达能力。 光流推断模块进一步通过相关性计算和特征融合捕捉到时序信息，这不仅丰富了深度伪造特征维度，还忽略了无关背景信息，增强了模型的鲁棒性。 本文所提出的自适应流场估算方法优化了特征匹配过程，使得模型能够更准确地估计相邻帧之间的运动信息，能够发现更细粒度的特征，这有助于识别细微的伪造痕迹。 这种综合了光流域和时域特征的处理方法，能够显著提高深度伪造检测的准确性和效率。

# 2. 2. 3 参数比较

使用 NVIDIA GeForce RTX 3090 的机器测量光流产生的运行时间，对于单个视频中抽取的 33 张帧，通过光流计算得到32 张光流图。 通过处理相同数据，对比不同模型的参数量与运行时间。 Mini＿flowNet 的参数量（M）相对于 FlowNet2 缩小到原来的 $3 \%$ ，运行时间（S）缩小到原来的 $4 2 \%$ 。 具体的参数对比结果如表1 所示。

表1　 不同光流网络的参数对比［21］  

<html><body><table><tr><td></td><td>FlowNetC</td><td>SpyNet</td><td>Mini_flowNet</td><td>FlowNet2</td></tr><tr><td>层数</td><td>26</td><td>35</td><td>87</td><td>115</td></tr><tr><td>参数</td><td>39.16</td><td>1.20</td><td>4.35</td><td>162.49</td></tr><tr><td>时间</td><td>37.18</td><td>87.23</td><td>34.37</td><td>81.98</td></tr></table></body></html>

# 2. 2. 4　 光流对特征提取的可解释性

在经过深度伪造或者其他操纵技术的伪造视频当中，人脸的某些细节和纹理，尤其是人脸器官的边缘区域，可能会展现出不自然的动态变化。 通过对比分析原始人脸与伪造人脸，可直接用肉眼观察到人脸边缘细节上的不一致，这些差异可能表明深度伪造的操作痕迹，训练过程中的该部分参数将受到重点关注。

在进行光流计算时，通过计算图像序列中像素点的颜色变化来估计物体运动。 这种颜色变化直观上可以看作是运动偏移的相对大小，即光流向量模长的大小。 通常，颜色较浅的光流区域表示相邻帧较小的运动偏移，颜色较深的光流区域则表示较大的运动偏移。 伪造图像与原始图像的光流对比如图4 所示。

![](images/74fd6cfaea6cc34efa308a6355bcf2e0428dccb5bedc58817622b260b3af6e64.jpg)  
图4　 伪造前后稠密光流对比图

在伪造人脸视频的制作中，由于难以精确复制人脸的细微纹理和细节，因此可能会出现不自然的偏移。 这些不合理的偏移量在光流图像中表现为颜色较深、边缘较为粗糙的特点。 当将这些光流特征可视化时，相邻帧之间像素点的颜色值差异会增大，这种细节上的差异可以被网络捕捉并作为检测伪造视频的线索。

# 2. 3 改进的轻量级 Mini＿xception

# 2. 3. 1　 Xception 网络

Xception［22］ 网络是在 Inception V3 网络［23］ 的基础上进行改进。 Inception 其核心思想是融合多种卷积层，通过并行的方式增强计算能力。 各个卷积层处理后生成的特征图在深度方向上进行堆叠，以此构建出一个更深层的结构。 通过反复使用 Inception网络结构，可以有效地扩展网络的深度与宽度，这不仅有助于提高深度学习模型的精确度，还能有效避免过拟合问题。 这种方法有效地减少了参数量和计算量，同时保持了模型的性能。

Xception 网络的主要创新在于使用深度可分离卷积来替代 Inception 模块中的常规卷积层。 与传统的卷积滤波器不同，深度可分离卷积能够将通道和空间维度的处理分开进行。 这种方法有效地减少了参数量和计算量，同时保持了模型的性能。 模块结构如图5 所示。

![](images/e78d3ac63d536bed98c7355a8418f4295d2466d7dfa77ce5c0344a174396f954.jpg)  
图 5　 Xception 模块结构

# 2. 3. 2 基于 Mini＿xception 的分类检测模块

基于 Xception 的设计理念，为简化 Xception 的复杂结构并减少参数的冗余，本文对网络结构进行了优化。 通过去除 Xception 的 Middle flow 模块，有效减少了模型的参数量，改进后的网络结构如图 6所示。 光流图已经包含了丰富的帧间信息，因此删减掉一些重复的深度可分离卷积层，将 Block4 到Block11 的部分删除，只保留 4 个块作为特征提取层，同时将4 个 Block 替换为轻量化分离卷积模块LSCB。

这种优化后的网络在参数量只有6. 7M 的前提下在检测精度上仍然保持了高性能。 因此，本文使用了这种改进策略，并将其命名为 Mini＿xception。具体的网络输入和通道数结构如表2 所示。

![](images/d5f53009de5f0d6a6b02bb30fba75d9c96b9a812a232b0f6b4bbbc71a9ceefcc.jpg)  
图 6　 Mini＿xception 的网络结构

表 2　 Mini＿xception 的输入和通道结构  

<html><body><table><tr><td>输入</td><td>操作</td><td>通道</td></tr><tr><td>299 ×299×3</td><td>2×2卷积</td><td>32</td></tr><tr><td>149 ×149 ×32</td><td>3×3卷积</td><td>64</td></tr><tr><td>147 ×147 ×64</td><td>LSCB1</td><td>128</td></tr><tr><td>74 ×74 ×128</td><td>LSCB2</td><td>256</td></tr><tr><td>37 ×37×256</td><td>LSCB3</td><td>728</td></tr><tr><td>1</td><td>1</td><td>1</td></tr><tr><td>19 ×19 ×728</td><td>LSCB4</td><td>1024</td></tr><tr><td>10 ×10×1024</td><td>3×3 深度可分离卷积</td><td>1536</td></tr><tr><td>10 × 10 ×1 536</td><td>3×3深度可分离卷积</td><td>2048</td></tr><tr><td>10 ×10 ×2 048</td><td>1×1全局平均池化</td><td></td></tr></table></body></html>

# 2. 3. 3 轻量化分离卷积模块 LSCB

为充分利用模型所学习到的特征，对前馈网络进行改进，受聚类剪枝［24］ 启发，通过 Network Slim⁃$\mathrm { m i n g } ^ { [ 2 5 ] }$ 对网络进行压缩。 本文设计并使用了一种轻量化分离卷积模块，为减少参数量防止过拟合，本文在不改变基本框架之上，将Silu 激活函数引入，更有利于对特征的平滑处理。 轻量化分离卷积模块可以表示为：

$$
L S C B ( x ) = A \nu g p ( S e p ( S i l u ( x ) ) ) + C o n v ( x )
$$

式中，Aνgp 表示平均池化操作，Sep 表示深度可分离卷积，Silu 表示 Silu 激活函数，Conv 表示卷积操作。 LSCB 模块结合了 Silu 激活函数和可分离卷积处理的特征以及直接从输入中提取的特征，这种多路径的特征融合有助于网络捕获更丰富的信息。 平均池化操作和参数量的缩减均能产生正则化效应，这有助于模型泛化到新的数据上。 LSCB 模块的设计考虑了在减少模型复杂度的同时保持特征提取能力，这是其能够有效提高模型准确性的关键所在。轻量化分离卷积模块结构如图7 所示。

![](images/d925bed24b42485f93ba031a631f4740d4131c0e2c76c6e128c316d3fe94f95d.jpg)  
图7　 轻量化分离卷积模块结构

# 3　 实验分析

# 3. 1　 实验数据

随着深伪技术的不断增强，伪造的数据集也在不断地增扩，其中较为经典的数据集有 FaceForen⁃sics $+ + ^ { [ 2 6 ] }$ 、 DFDC［27］ 、 Celeb⁃DF［28］ 、 Faceshifter［29］ 。为验证本文方法的有效性，本文选择FaceForensics $+ +$ 和 DFDC 两个数据集上进行测试。

FaceForensics $+ +$ 数据集是在 2019 年推出的，专门用于人脸面部伪造视频的评测。 包含了4 种篡改技术：Face2Face、FaceSwap、Deepfake 以及 Neural⁃Textures。 其中，前两种技术是通过计算机图形技术来生成伪造视频的，而后两种则是基于深度学习技术。 该数据集拥有 1 000 个视频样本，每个篡改技术类别下都有 1 000 个相应的视频样本，加上原始未篡改的视频，总量达到5 000 个视频。

DFDC 数据集包括了119 197 个视频，每个视频时长都为10 s，但是帧率从 $1 5 \sim 3 0$ fps 不等，分辨率也从 $3 2 0 \times 2 4 0 \sim 3 ~ 8 4 0 \times 2 ~ 1 6 0$ 不等。 训练视频中有19 197 个视频是由大约 430 名演员真实拍摄的片段，剩余100 000 个视频是由真实视频生成的伪造视频。 换脸生成使用了多种主流换脸生成算法，使得数据集包含尽可能多的换脸视频。

Celeb⁃DF 是 一 个 在 IEEE Conference on Com⁃puter Vision and Pattern Recognition （CVPR） 2020 上发布的大型深度伪造视频数据集。 它包含了590 个从 YouTube 收集的真实名人视频和5 639 个对应的深度伪造合成视频，旨在推动深度伪造检测技术的发展。 这个数据集提供了一个挑战性的环境，用于测试和改进现有的深度伪造检测算法。

Face Shifter 是一个两阶段可用于任意两张人脸图像换脸的模型，由 AEI⁃Net 和 HEAR⁃Net 两部分组成，经过合适的训练，AEI⁃Net 本身就已经可以得到不错的换脸效果，在此基础上，可以再训练一个HEAR⁃Net，着重解决目标图像脸部遮挡问题，并进一步对换脸效果进行优化。 模型推理速度快，并且能够生成质量较高的换脸视频。

# 3. 2 实验环境

本文中的实验在 NVIDIA GeForce RTX 3090 上进行测试，使用版本为 1. 13 的 PyTorch 深度学习框架实现。 实验平台为版本号为 Ubuntu 16. 04. 6 LTS的 64 位 Linux 操作系统，显卡内存大小为 $2 4 \ \mathrm { G B }$ 。CPU 版本为 Intel（ R） Core（ TM） i9－10920X $@$ 3. 50GHz，内存大小为 $3 2 \ \mathrm { G B }$ 。

# 3. 3　 实验设置

模型的超参数用于控制模型的行为和性能，超参数的设置会影响训练速度、收敛性、泛化能力。 常见的超参数有学习率、Batch＿size、迭代次数、优化器的选择、激活函数的选择等。 实验中将学习率调整为 0. 001，Batch＿size 设置为 128，迭代次数设置为 50次，优化器选择 Adam 优化算法。

目前主流的激活函数有 Sigmoid、Tanh、Relu 和变体 Relu6 等。 激活函数的类型选择与模型的输出密不可分，通过实验对比本文选取 Relu 作为激活函数。

# 3. 4 评价指标

在分类器的设计上，使用了 SoftMax 函数来转换分类器的输出。 由于本文所处理的问题属于二分类任务，因此采用了与传统二分类问题相同的评价标准，即准确率（ACC）。 通过光流法对视频中所抽取的帧进行计算光流，分类网络对计算的光流进行真假预测，统计视频中所抽取的光流预测结果，最终对视频的真伪做决策。 准确率的具体计算过程可表示为公式（6）：

$$
A C C = { \frac { T P + T N } { T P + F P + T N + F N } }
$$

式中， $T P$ 为真例预测为真，TN 为真例预测为假， $F P$ 为假例预测为真， $F N$ 为假例预测为假。 在消融实验和库内跨域实验中引入了 F1 和 AUC 作为评价模型良好的一个指标。 为计算不同超参数对模型训练的影响，本文选取交叉熵损失函数（Loss）作为评价指标。 损失函数的计算过程可以表示为：

$$
L o s s = - \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \ \big [ y _ { i } \mathrm { l o g } ( p _ { i } ) + ( 1 - y _ { i } ) \mathrm { l o g } ( 1 - p _ { i } ) \big ]
$$

式中， $N$ 表示样本数量， $y _ { i }$ 是第 $i$ 个样本的真实标签， $p _ { i }$ 是本文网络模型对视频的预测分类结果， $\log$ 通常指自然对数。

# 3. 5 结果分析

# 3. 5. 1 模型优化实验

传统的 Xception 网络结构深、参数量较大，为充分验证本文所提优化网络方法的有效性，对改进过程中的有效参数进行测试，在不影响检测准确率的前提下减小参数，降低每秒浮点运算次数。 通过删减（Del）部分网络特征提取层减小参数，并保证整体网络输入输出不变的前提下进行训练，随后将设计轻量化分离卷积模块应用到网络层内部（ $\mathrm { D e l } +$ LSCB），以弥补删减网络造成的特征提取不充分。以参数量、每秒浮点运算次数、检测准确率作为评价指标，保持每次实验的参数和其他条件一致，将处理好的放在模型 $\mathrm { F F } + +$ 中的4 个数据集上进行测试计算平均准确率。 最终实验结果如表3 所示。

表3　 模型优化实验结果  

<html><body><table><tr><td></td><td>Xception</td><td>Del</td><td>Del + LSCB</td></tr><tr><td>参数量(M)</td><td>20.81</td><td>7.89</td><td>6.73</td></tr><tr><td>浮点运算数(G)</td><td>33.76</td><td>15.06</td><td>8.26</td></tr><tr><td>准确率/%</td><td>96.87</td><td>95.29</td><td>97.31</td></tr></table></body></html>

从实验结果可得，在删减优化的过程中，参数和浮点运算数都有所下降，但同样导致了网络性能的下降，即准确率的下降。 通过设计的分离卷积模块的引入很好的弥补了这一缺点。 相比于传统的Xception，优化后的网络模型在参数量上减少至原来的 $3 2 \%$ ，每秒浮点运算次数缩小至原来的 $2 4 \%$ ，准确率约提高了 $0 . 4 7 \%$ 。

# 3. 5. 2 消融实验

为了分析本文所提出算法优化和网络模块结构的改进对本文模型的增益，进行消融实验来进行验证。 将传统的光流法和卷积神经网络（ $\mathrm { O F + C N N } )$ 1组合［30］ 作为 Baseline，分别引入 Mini＿flowNet（ Mf）和 Mini＿xception（Mx）来讨论各部分对模型准确率的增益，保持每次实验的参数和其他条件一致。将模型分别在 Deepfake、Face2Face、FaceSwap、Neu⁃ralTexture 和 DFDC 数据集上进行训练和测试，以ACC 和 F1 作为评价指标，最终实验结果如表 4所示。

表4　 消融实验结果  

<html><body><table><tr><td></td><td></td><td></td><td colspan="2">Deepfake</td><td colspan="2">Face2Face</td><td colspan="2">FaceSwap</td><td colspan="2">NeuralTexture</td><td colspan="2">DFDC</td></tr><tr><td>Baseline</td><td>Mf</td><td>Mx</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td><td>ACC</td><td>F1</td></tr><tr><td>√</td><td>1</td><td>1</td><td>91. 21</td><td>93.38</td><td>93.15</td><td>92.86</td><td>90.01</td><td>88.54</td><td>91.91</td><td>93.89</td><td>81.06</td><td>85.29</td></tr><tr><td>√</td><td>√</td><td></td><td>94.46</td><td>95.19</td><td>93.61</td><td>97.92</td><td>95.26</td><td>96.15</td><td>92.83</td><td>94.13</td><td>82.61</td><td>86.43</td></tr><tr><td>√</td><td>1</td><td>√</td><td>96.23</td><td>96.51</td><td>97.73</td><td>95.39</td><td>96.18</td><td>92.39</td><td>93.41</td><td>97.28</td><td>82.27</td><td>88.98</td></tr><tr><td>√</td><td>√</td><td>√</td><td>99.42</td><td>98.07</td><td>99. 25</td><td>99.84</td><td>97. 66</td><td>98.51</td><td>92.91</td><td>93.76</td><td>85.09</td><td>87.72</td></tr></table></body></html>

从表 4 中的数据可得，不引入 Mf 和 $\operatorname { M x }$ 之前，其检测能力较差，几个数据集上的准确率最高仅为$9 3 . 1 5 \%$ 。 通过引入 Mf 轻量级光流法之后，模型更加注重其时序上的特征，准确率都有所提高，F1 在Face2Face 数据集上的表现达到了 $9 7 . 9 2 \%$ ；通过只引入 $\mathbf { M } \mathbf { x }$ 和加入了 LSCB 模块，对局部信息的特征提取进 一 步 增 强， 准 确 率 分 别 达 到 了 $9 6 . 2 3 \%$ 、$9 7 . 7 3 \% . 9 6 . 1 8 \% . 9 3 . 4 1 \%$ 和 $8 2 . 2 7 \%$ ，平均准确率提升了 $1 \%$ 左右；同时引入两者之后，模型学习了通道的相关性，准确率分别达到了 $9 9 . 4 2 \%$ 、 $. 9 9 . 2 5 \%$ 、

97. $6 6 \%$ 、 $9 2 . 9 1 \%$ 和 $8 5 . 0 9 \%$ ，平均准确率提升了约$1 . 5 1 \%$ 。 证明了光流法和改进后的分类网络可以提升模型整体性能。

# 3. 5. 3 超参数实验

为验证本文算法选择激活函数的合理性和正确性，对于不同激活函数对模型效果的影响进行实验分析。 在本文模型的基础上，选择了 Sigmoid、Relu、Relu6 这3 种激活函数进行实验。 保持每次实验的参 数 和 其 他 条 件 一 致。 在 Deepfake、 Face2Face、DFDC 3 个数据集上进行测试各模型检测的性能。

比较结果如图8、图9、图10、图11 所示。

![](images/9d77d91c4da45d3582aeda2b92b4628af9ab9f1281920c36df2eaf2b35af52b7.jpg)  
图8　 激活函数在 Deepfake 上的损失函数表现

![](images/24534b0ee9662668c9bed69a64d122a337b56b25862f6f564b2b0010265e92ac.jpg)  
图9　 激活函数在 Face2Face 上的损失函数表现

![](images/e110ddc5d4f862da07936c2dd0b4d62e48b5785acd6665d28dcc358e8c91ec67.jpg)  
图10　 激活函数在 DFDC 上的损失函数表现

由以上结果可知，在本文网络框架的基础之上，通过改变分类网络中不同的激活函数 Relu6、Relu以及 Sigmoid。 10 次 Epoch 之后模型逐渐收敛，在经过了使用 Relu 激活函数的模型损失降低最快，收敛之后的损失平均较另外两个激活函数下降了 0. 03。由图12 可知，在引入不同激活函数的情况下，使用Relu 激活函数的准确率平均提高了约 $1 . 2 \%$ 。 这证明了本文前部分激活函数超参数选择的结论，即Relu 对本实验模型最友好。

# 3. 5. 4 泛化性实验

传统的深度伪造检测方法主要是通过在大量的训练数据集上训练模型，以学习到真伪数据的特征差异，进而用于对测试集数据的判断。 但是，如果训练集不能很好地代表现实世界中的各种伪造手法，那么模型在遇到新的伪造数据时，其性能可能会大打折扣。 往往模型训练当中训练和测试集合都较单一，所训练的参数鲁棒性较差。 因此，评估这类模型的重要方法之一就是进行泛化性测试实验。

![](images/801803124c5454088545e44a600da227580446f8aa950efcd05ba96a7d8fd43f.jpg)  
图11　 不同激活函数识别伪造视频的准确率

在泛化性测试实验中，通常会使用不同于训练集的数据集来评估模型的性能。 本文选择了 $\mathrm { F F } + +$ 的4 个数据集进行训练，在 Celeb、DFDC 和 Faceshifter（Fasr）数据集上进行测试，保持每次实验的参数和其他条件一致。 在另外两个数据集上进行测试，通过将本文所提模型对比不同的算法，以 AUC 作为评价指标，实验结果如表5 所示。

表5　 跨库测试实验［30］  

<html><body><table><tr><td></td><td>Celeb</td><td>DFDC</td><td>Fasr</td><td>平均</td></tr><tr><td>Xception</td><td>1</td><td>65.41</td><td>1</td><td>65.4</td></tr><tr><td>MS -TCN</td><td>72. 62</td><td>57.74</td><td>79.92</td><td>70. 1</td></tr><tr><td>Mesconet</td><td>62.31</td><td>56.75</td><td>86.94</td><td>68.6</td></tr><tr><td>C3D</td><td>64.24</td><td>53.52</td><td>83.65</td><td>67.1</td></tr><tr><td>Lipforensics</td><td>74. 24</td><td>68.57</td><td>93.43</td><td>78.7</td></tr><tr><td>F3-net</td><td>67.25</td><td>61.44</td><td>91. 61</td><td>73.4</td></tr><tr><td>Capsule</td><td>64.52</td><td>65.84</td><td>87.52</td><td>72. 6</td></tr><tr><td>multi-task</td><td>75.77</td><td>68.12</td><td>86.75</td><td>76.8</td></tr><tr><td>RECCE</td><td>73.53</td><td>62.02</td><td>83.56</td><td>73.0</td></tr><tr><td>UCI</td><td>77.95</td><td>70.32</td><td>93.64</td><td>80.6</td></tr><tr><td>本文模型</td><td>79.32</td><td>73.48</td><td>93.59</td><td>82.1</td></tr></table></body></html>

在跨库测试实验当中，模型在 Celeb 和 DFDC数据集上进行实验，AUC 均优于主流算法。 通过产生的光流图，模型在训练阶段有针对性的学习到了伪造图片的伪造特征，在分类网络中可以很好地利用这些特征进行跨库验证，从而获得较高的准确率。但是在 Fasr 数据集跨库测试的表现还有待提高，未来工作会进一步融合频域信息或增加细粒度特征从而改进模型下游的特征提取方法。

# 3. 5. 5 库内跨域实验

为进一步验证本文方法的检测有效性，将本文所采用的方法在权威数据库 $\mathrm { F F } + +$ 中的 4 个分域数据集 Deepfake、Face2Face、NeuralTextures、Fac⁃eSwap 数据集上进行了库内跨域测试，并将所得结果与近几年 较 新 的 EfficientNet、multi⁃task、UCI 等主流算法进行比较，保持每次实验的参数和其他条件一致，评价指标选择 AUC 和 ACC。 其中的 3个数据集作为训练集，另一个数据集作为测试集，最终实验结果如表6 所示。

表6　 与主流算法对比的库内跨域实验［31］  

<html><body><table><tr><td></td><td colspan="2">Deepfake</td><td colspan="2">FaceSwap</td><td colspan="2">Face2Face</td><td colspan="2">NeuralTexture</td><td colspan="2">平均</td></tr><tr><td></td><td>AUC</td><td>ACC</td><td>AUC</td><td>ACC</td><td>AUC</td><td>ACC</td><td>AUC</td><td>ACC</td><td>AUC</td><td>ACC</td></tr><tr><td>LSTM</td><td>88.4</td><td>77.5</td><td>85.3</td><td>75.2</td><td>85.9</td><td>76.2</td><td>84.7</td><td>74.5</td><td>86.1</td><td>75.8</td></tr><tr><td>C3D</td><td>88.3</td><td>77.0</td><td>84.0</td><td>72.4</td><td>81.3</td><td>72.3</td><td>83.7</td><td>72.2</td><td>84.3</td><td>73.5</td></tr><tr><td>MS-TCN</td><td>83.0</td><td>71.0</td><td>83.4</td><td>73.3</td><td>88.2</td><td>77.3</td><td>85.2</td><td>74.3</td><td>85.1</td><td>74.0</td></tr><tr><td>EfficientNet</td><td>82.9</td><td>72.6</td><td>81.3</td><td>69.6</td><td>84.3</td><td>74.7</td><td>79.6</td><td>78.1</td><td>82.0</td><td>73.7</td></tr><tr><td>Mesconet</td><td>89.2</td><td>79.2</td><td>85.4</td><td>75.1</td><td>83.0</td><td>71.7</td><td>82.4</td><td>74.1</td><td>85.0</td><td>75.0</td></tr><tr><td>Lipforensics</td><td>92.3</td><td>83.8</td><td>87.3</td><td>77.9</td><td>93.0</td><td>82.9</td><td>84.4</td><td>72.6</td><td>89.2</td><td>79.3</td></tr><tr><td>F3-net</td><td>92.4</td><td>82.4</td><td>90.5</td><td>81.8</td><td>92. 2</td><td>81.1</td><td>86.5</td><td>75.8</td><td>90.4</td><td>80.3</td></tr><tr><td>Capsule</td><td>87.8</td><td>76.2</td><td>83.9</td><td>75.3</td><td>86.1</td><td>76.5</td><td>86.7</td><td>77.4</td><td>86.1</td><td>76.3</td></tr><tr><td>multi-task</td><td>86.9</td><td>76.8</td><td>82.5</td><td>70.9</td><td>85.0</td><td>76.4</td><td>86.3</td><td>76.7</td><td>85.2</td><td>75.2</td></tr><tr><td>UCI</td><td>92.3</td><td>81.5</td><td>88.9</td><td>78.5</td><td>93.2</td><td>83.9</td><td>87.9</td><td>78.6</td><td>90.6</td><td>80.6</td></tr><tr><td>RECCE</td><td>84.7</td><td>75.9</td><td>86.4</td><td>74.7</td><td>88.3</td><td>78.2</td><td>82.4</td><td>73.6</td><td>85.4</td><td>75.6</td></tr><tr><td>本文模型</td><td>92.0</td><td>82.6</td><td>90.8</td><td>80.2</td><td>94.4</td><td>83.2</td><td>88.0</td><td>79.9</td><td>91.3</td><td>81.5</td></tr></table></body></html>

此实验对比了主流的深度伪造检测算法，证明了基于轻量级光流法在库内跨域实验上的检测准确率具有一定的优势。 但是对于 Deepfake 和 Face⁃Swap 数据集的训练还有改进的空间。

在本文所采取的实验中，消融实验验证了本文所提模块对提升网络整体检测准确率的有效性；通过引入不同的超参数实验，确定最适应本文模型的激活函数 Relu；通过在不同数据集上进行跨库测试验证了模型的泛化性；通过库内跨域实验与主流算法对比证明了本文模型的准确性。

# 4　 结论与展望

为解决现有深度伪造检测方法存在的模型参数量大和模型训练时间长等问题，本文在基于传统的光流法深度伪造检测的基础上，对传统的 FlowNet进行优化，提出了一种轻量级光流法，并与其他主流方法相对比在生成光流图质量相同的情况下，减少了计算光流的成本，证明本文模型的有效性。 在分类阶段，对 Xception 网络结构进行优化，提出了一种轻量化分离卷积模块应用到网络当中，在不影响检测准确率的前提下减少了参数量。 未来的工作将从以下几个方面进行：

（1）提高模型的可解释性。 本文模型能够精确地进行真伪视频的二分类判断，但是当涉及到判断特定人脸视频是否被伪造时，基于光流算法的解释性不够明确。 未来的研究需要探索如何将深度学习技术与光流法有效融合，以提升伪造人脸检测方案的解释性，并且同时优化检测的性能。

（2）引入细粒度的特征提取模块和通道注意力机制，对其空域、频域等特征进行提取分析并进行特征融合进行检测。 通过结合不同来源或不同类型的特征信息能够提高模型的性能和泛化能力。

（3）在数据处理阶段引入数据增强，提高模型的鲁棒性。 在现实场景下，社交媒体上传播的视频内容经常会通过各种处理，如分辨率调整、剪辑和压缩等，这些处理可能会让伪造视频的分辨率下降、篡改痕迹变淡，甚至导致关键特征丢失，从而提高了检测难度。 未来的研究需要在数据预处理阶段采用更有效的数据增强方法或构建高效率的网络结构，以增强算法的鲁棒性。

# 参 考 文 献

［1］　 张璐，芦天亮，杜彦辉． 人脸视频深度伪造检测方法综述［J］． 计算机科学与探索，2023，17（1）：1 － 26．  
［2］　 SABIR E，CHENG J，JAISWAL A，et al． Recurrent conv⁃olutional strategies for face manipulation detection in vide⁃os［J］． Interfaces，2019，3（1）：80 － 87．  
［3］　 ZHENG Y，BAO J，CHEN D，et al． Exploring temporalcoherence for more general video face forgery detection［C］∥Proceedings of the IEEE ／ CVF international confer⁃ence on computer vision，2021：15044 － 15054．  
［4］　 GUO X， LIU X H， REN Z Y， et al． Hierarchical fine⁃grained image forgery detection and localization ［ C］ ∥Proceedings of the IEEE ／ CVF Conference on ComputerVision and Pattern Recognition，2023：3155 － 3165．  
［5］　 俞洋，袁家斌，蔡纪元，等． 基于非关键掩码和注意力机制的深度伪造人脸篡改视频检测方法［J］． 计算机科学，2023，50（11）：160 － 167．  
［6］　 YU C R，ZHANG X H，DUAN Y X，et al． Diff⁃ID： an ex⁃plainable identity difference quantification framework forDeepFake detection［J］． IEEE Transactions on Dependa⁃ble and Secure Computing，2023，2（4）：986 － 998．  
［7］　 DOLORIEL C T，CHEUNG N M． Frequency masking foruniversal deepfake detection ［ C］ ∥2024 IEEE Interna⁃tional Conference on Acoustics， Speech and Signal Pro⁃cessing （ICASSP），2024：13466 － 13470．  
［8］　 SONG L C，FANG Z，LI X D，et al． Adaptive face forgerydetection in cross domain［ C］ ∥Proceedings of EuropeanConference on Computer Vision⁃ECCV 2022，2022：467 －484．  
［9］　 LI J M，XIE H T，LI J H，et al． Frequency⁃aware discrim⁃inative feature learning supervised by single⁃center lossfor face forgery detection［ C］ ∥2021 IEEE ／ CVF Confer⁃ence on Computer Vision and Pattern Recognition（CVPR），2021：6454 － 6463．  
［10］　 TANG W，WO Y，HAN G Q． Geometrically robust videohashing based on ST⁃ PCT for video copy detection［ J］．Multimedia Tools and Applications， 2019， 78 （ 15 ）：21999 － 22022．  
［11］　 HASAN H R，SALAH K． Combating deepfake videos u⁃sing blockchain and smart contracts［ J］． IEEE Access，2019，7（4）：41596 － 41606．  
［12］　 SHAO R， WU T X， WU J L， et al． Detecting andgrounding multi⁃modal media manipulation ［ J ］． IEEETransactions on Pattern Analysis and Machine Intelli⁃gence IEEE，2024，46（8）：5556 － 5574．  
［13］　 AGARWAL S， FARID H． Detecting deep⁃fake videosfrom aural and oral dynamics ［ C］ ∥ 2021 IEEE ／ CVFConference on Computer Vision and Pattern RecognitionWorkshops（CVPRW），2021：981 － 989．  
［14］　 LIU B P，LIU B，DING M，et al． TI2Net： temporal iden⁃tity inconsistency network for deepfake detection［ C］ ∥Proceedings of the IEEE ／ CVF Winter Conference on Ap⁃plications of Computer Vision，2023：4691 － 4700．  
［15］　 LI J， ZHANG H， WAN W， et al． Two⁃class 3D⁃CNNclassifiers combination for video copy detection ［ J ］．Multimedia Tools and Applications，2018，3（1）：1 － 13．  
［16］　 HUI T， TANG X， LOY C． Liteflownet： A lightweightconvolutional neural network for optical flow estimation［J］． IEEE on Computer Vision and Pattern Recogni⁃tion，2018，7（4）：8981 － 8989．  
［17］　 FISCHER P，DOSOVITSKIY A，ILG E，et al． FlowNet：learning optical flow with convolutional networks ［ J ］．IEEE on Computer Vision，2015，10（2）：2758 － 2766．  
［18］　 JADERBERG M，KAREN S，ANDREW Z． Spatial trans⁃former networks ［ J］． Advances in Neural InformationProcessing Systems，2015，28（5）：729 － 736．  
［19］　 HUI T W，LOY C C． Liteflownet3： Resolving correspon⁃dence ambiguity for more accurate optical flow estimation［C］ ∥ Computer Vision⁃ECCV 2020： 16th EuropeanConference，2020：169 － 184．  
［20］　 LIU S T，HUANG D，WANG Y H． Learning spatial fu⁃sion for single⁃shot object detection［J］． ArXiv，2019．  
［21］　 DOSOVITSKIY A，FISCHER P，ILG E，et al． Flownet：Learning optical flow with convolutional networks［ C］ ∥IEEE International Conference on Computer Vision，2015：2758 － 2766．  
［22］　 CHOLLET F． Xception： deep learning with depthwiseseparable convolutions［ C］ ∥2017 IEEE Conference onComputer Vision and Pattern Recognition （ CVPR ），2017：1800 － 1807．  
［23］　 SZEGEDY C，VANHOUCKE V，IOFFE S，et al． Rethin⁃king the inception architecture for computer vision［ J］．IEEE，2016：2818 － 2826．  
［24］　 LIU Z，LI J G，SHEN Z Q，et al． Learning efficient conv⁃olutional networks through network slimming［ C］ ∥2017IEEE International Conference on Computer Vision （IC⁃CV），2017：2755 － 2763．  
［25］　 VADERA S，AMEEN S． Methods for pruning deep neu⁃ral networks［ J］． IEEE Access，2022，10（4）：63280 －63300．  
［26］ ROSSLER A， D COZZOLINO， VERDOLIVA L， et al．Learning to detect manipulated facial images［ J］． ArX⁃iv，2019．  
［27］ DOLHANSKY B， HOWES R， PFLAUM B， et al． Thedeepfake detection challenge （ DFDC ） preview dataset［J］． ArXiv，2019．  
［28］ LI Y Z，YANG X，SUN P，et al． Celeb⁃df： A large⁃scalechallenging dataset for deepfake forensics ［ C ］ ∥ Pro⁃ceedings of the IEEE ／ CVF Conference on Computer Vi⁃sion and Pattern Recognition，2020：3207 － 3216．  
［29］　 LI L，BAO J，YANG H，et al． FaceShifter： towards highfidelity and occlusion aware face swapping［ J］． ArXiv，2019．  
［30］ PALLABI S，DHOLARIA D． A hybrid CNN⁃LSTM mod⁃el for video deepfake detection by leveraging optical flowfeatures［ C］ ∥ 2022 International Joint Conference onNeural Networks，2022：1 － 7．  
［31］　 CHU B L，XU X，YOU W K， et al． Unearthing commoninconsistency for generalisable deepfake detection ［ J］．ArXiv，2023．

（责任编辑 于瑞华）