# 《计算机技术与发展》网络首发论文

题目： 基于通道注意力和特征融合的伪造语音检测研究  
作者： 汤龙，雷震春  
DOI： 10.20165/j.cnki.ISSN1673-629X.2025.0136  
收稿日期： 2025-01-13  
网络首发日期： 2025-06-10  
引用格式： 汤龙，雷震春．基于通道注意力和特征融合的伪造语音检测研究[J/OL]．计算机技术与发展. https://doi.org/10.20165/j.cnki.ISSN1673-629X.2025.0136

网络首发：在编辑部工作流程中，稿件从录用到出版要经历录用定稿、排版定稿、整期汇编定稿等阶段。录用定稿指内容已经确定，且通过同行评议、主编终审同意刊用的稿件。排版定稿指录用定稿按照期刊特定版式（包括网络呈现版式）排版后的稿件，可暂不确定出版年、卷、期和页码。整期汇编定稿指出版年、卷、期、页码均已确定的印刷或数字出版的整期汇编稿件。录用定稿网络首发稿件内容必须符合《出版管理条例》和《期刊出版管理规定》的有关规定；学术研究成果具有创新性、科学性和先进性，符合编辑部对刊文的录用要求，不存在学术不端行为及其他侵权行为；稿件内容应基本符合国家有关书刊编辑、出版的技术标准，正确使用和统一规范语言文字、符号、数字、外文字母、法定计量单位及地图标注等。为确保录用定稿网络首发的严肃性，录用定稿一经发布，不得修改论文题目、作者、机构名称和学术内容，只可基于编辑规范进行少量文字的修改。

出版确认：纸质期刊编辑部通过与《中国学术期刊（光盘版）》电子杂志社有限公司签约，在《中国学术期刊（网络版）》出版传播平台上创办与纸质期刊内容一致的网络版，以单篇或整期出版形式，在印刷出版之前刊发论文的录用定稿、排版定稿、整期汇编定稿。因为《中国学术期刊（网络版）》是国家新闻出版广电总局批准的网络连续型出版物（ISSN 2096-4188，CN 11-6037/Z），所以签约期刊的网络版上网络首发论文视为正式出版。

# 基于通道注意力和特征融合的伪造语音检测研究

汤 龙，雷震春

(江西师范大学 计算机信息工程学院，南昌 360111)

摘 要：随着深度学习技术的迅猛发展，语音伪造技术对自动说话人验证系统的安全性构成严峻挑战，语音伪造检测系统依旧面临准确率不足、场景单一等问题。本文提出了一种结合通道注意力和特征融合的伪造语音检测方法，以解决语音伪造检测系统面临的一系列问题。为了聚集丰富的上下文信息和融合尺度不一致的特征，本文提出了双分支通道注意力模块，利用深度卷积沿通道维度聚合多尺度上下文信息，同时在两个分支上捕捉全局和局部特征信息；然后提出了注意力特征融合模块，将 LFCC 特征经过真实语音 GMM 和欺骗语音 GMM 得到对数高斯概率特征，随后基于注意力进行特征融合以学习具有通道上下文信息和全局局部特征信息的交互特征，解决了特征融合机制场景单一的问题。与基线系统相比，本文最佳系统AFF-ResNet 在 ASVSpoof2021LA 数据集上的 EER 和 min t-DCF 分别降低 $3 7 . 5 \%$ 和 $1 5 . 3 \%$ 。实验结果表明，本文所提方法显著提升语音欺骗检测的准确率。

关键词：伪造语音检测；对数高斯概率特征；通道注意力；深度卷积；多尺度上下文信息；特征融合中图分类号：TP391 文献标识码：Adoi:10.20165/j.cnki.ISSN1673-629X.2025.0136

# The Research on Deepfake Speech Detection Based on Channel

Attention and Feature Fusion

TANG Long，LEI Zhen-chun

(School of Computer and Information Engineering，Jiangxi Normal University，Nanchang 360111，China)

Abstract：With the rapid development of deep learning technologies, speech spoofing techniques pose serious threats to the security of automatic speaker verification systems. Speech spoofing detection systems still face challenges such as insufficient accuracy and limited application scenarios. This paper proposes a speech spoofing detection method that combines channel attention and feature fusion to address these issues in speech spoofing detection systems.This paper proposes speech spoofing detection method combining channel attention and feature fusion to address the challenges faced by deepfake speech detection systems.To gather rich contextual information and fuse features with inconsistent scales, a dual-branch channel attention module is proposed, which uses Depth-wise convolution along the channel dimension to aggregate multi-scale contextual information, while capturing both global and local feature information in two separate branches. Then, an attention feature fusion module is proposed, where the LFCC features are processed through the bonafide speech GMM and spoofed speech GMM to obtain Log-Gaussian probability features. Attention-based feature fusion is then applied to learn interactive features that incorporate both channel context information and global-local feature information, addressing the issue of a single scenario in the feature fusion mechanism.Compared to the baseline system, the proposed best system, AFF-ResNet, reduces the EER and min t-DCF by $3 7 . 5 \%$ and $1 5 . 3 \%$ , respectively, on the ASVSpoof2021LA dataset. Experimental results show that the proposed method significantly improves the accuracy of speech spoofing detection.

Keywords:deepfake speech detection;log-gaussian probability features;channel attention;depth-wise convolution; multi-scale context information;feature fusion

# 0 引言

自 动 说 话 人 确 认 （ Automatic SpeakerVerification，ASV）的任务是利用输入的说话人语音验证说话人的身份，通过分析说话人的声音特征，如音高、音色、声调、发声方式等将不同说话人的身份区分开。与指纹识别、虹膜识别等生物识别技术相比，说话人识别（Speaker Recognition）技术不仅有便于采集、易操作、成本较低等优点，还可以实现非面对面接触识别，大大降低了用户的风险。语音合成和语音转换（VC）技术可用于欺骗ASV 系统。语音伪造检测的任务是开发一种能够有效区分欺骗语音和真实语音的反欺骗系统。防止伪造语音的欺骗，确保输入语音的真实性是近年来语音领域研究的热点问题之一。因此设计有效的语音伪造检测方法，提高 ASV 系统的安全性具有重大意义。

目前语音反欺骗方法主要由前端特征提取和后端分类两大部分组成，将前端生成的声学特征输入到后端分类器进行分类。在前端声学特征方面，研究者提出许多不同的声学特征，以提高特征的判别能力。Todisco[1]等人使用常数 Q 变换（Constant QTransform，CQT）代替傅里叶变换，提出了常数 Q倒 谱 系 数 （ Constant Q Cepstral Coefficients ，CQCC），被广泛应用在现有的语音伪造检测系统中。Sahidullah[2]等人提出基于滤波器的线性频率倒谱 系 数 (Linear Frequency Cepstral Coefficients ，LFCC)，与传统的梅尔频率倒谱系数（Mel FrequencyCepstral Coefficients，MFCC）相比更专注于高频的特征。在 ASVSpoof2021 挑战赛中，LFCC 和 CQCC都是作为官方基线实验的主要方法。相比于以上特定人工设计的声学特征，直接通过深度神经网络从原始波形提取前端特征的方法也取得了巨大的进展。Tak[3]等人首次在语音伪造检测中应用 RawNet2深度神经网络，利用 RawNet2 强大的表征能力直接从原始波形中提取更通用、更高层次的特征表示，避免了人工特征的设计依赖。近年来，随着预训练模型的发展，研究者也尝试将预训练的模型和方法应用到语音伪造检测当中。Xie[4]等人使用 wav2vec预训练模型作为新的特征提取器，直接对原始波形进行处理，得到更具鲁棒性的语音特征表示。Lee J[5]等人提出了利用不同特征空间使用 wav2vec2.0 预训练模型合成伪影的有效性，并研究了不同架构和该特征空间结合的欺骗 CM 性能。Cai $\mathrm { D } ^ { [ 6 ] }$ 等人提出利用自动语音识别任务对 Conformer 进行预训练，将任务中训练得到的知识转移到说话人识别任务中，显著提高了各种模型的鲁棒性。

在后端分类模型中，随着深度学习技术的快速发展，更多的神经网络被用于语音伪造检测任务，尤 其 是 卷 积 神 经 网 络 （ Convolutional NeuralNetwork，CNN）在语音伪造检测任务表现出强大的性能。Lavrentyeva[7]等人提出在卷积层中引入最大特征图（Max Feature Map，MFM）函数，并提出轻量级卷积神经网络（Light Convolutional NeuralNetwork ， LCNN ） ， ASVSpoof2017 挑 战 赛 和ASVSpoof2019 挑战赛 LA 场景下的最佳单系统均基于该 LCNN 网络。He[8]等人为了解决卷积神经网络训练过程中出现的网络退化和梯度消失等问题，提出了残差网络（Residual Networks，ResNet）模型，被广泛用语音和图像等领域。Lai[9]等人提出了基于 ResNet 和挤压-激励网络（Squeeze-ExcitationNetwork，SENet）[10]的 ASSERT 检测模型，并引入了统计池化的方法来解决语音伪造攻击的问题。Liu[11]等人提出了基于顺序和并行特征注意融合机制的 ResNet 模型，该模型使用注意力机制来学习基于特征内容的融合权值，从而动态的融合身份映射特征和残差学习特征，并取得了很好的改进效果。

虽然上述提到的声学特征提取方法和后端分类改进模型在语音分类任务中表现出优异的性能，但它们只依赖于流行的深度学习架构，而将传统机器学习方法与流行的深度学习方法相结合的网络在 ASV 任务中很少被研究。经典的高斯混合模型（Gaussian Mixture Model，GMM）[12]通常作为 ASV挑 战赛的基线模型，被广泛应用于语音伪造检测领域。Lei[13][14] 等人提出的由高斯混合模型（Gaussian Mixture Model，GMM）与深度神经网络融合的 GMM-Transformer 以及 GMM-ResNet 模型，该模型在语音欺骗检测逻辑访问任务中取得了优异的检测效果。Wen[15]等人也基于不同数据增强和攻击方法提出了多分支的GMM-MobileNet模型。尽管上述将传统机器学习和流行深度学习相结合的网络性能明显优于经典的欺骗检测系统，但研究结果表明其仍有很大的改进空间。

注意力机制无论是在频域、时域和通道上都会存在互补、有区别的上下文信息，且适用于不同的欺骗攻击。Guo Y[16]等人提出一种新型的多层融合注意力分类器，该分类器在时间层级别捕获音频特征的互补信息，但是并没有考虑到音频特征的上下文信息。 $\mathrm { W u ~ H ^ { [ 1 7 ] } }$ 等人提出多尺度特征聚合层（Multi-Feature Aggregate，MFA）和动态卷积操作集成到反欺骗框架中，但动态卷积操作增加了额外的计算负担，且复杂的模型结构容易产生过拟合。Zhang[18]等人提出的 MFA-Conformer 模型将每个Conformer 块的输出进行连接，然后进行层归一化处理，以聚合浅层特征和深层特征。Wang Y[19]等人同样也提出 MFA-Conformer 模型将不同层的全局和局部特征进行聚合，并利用预训练的 Conformer模型来增强 CM 系统的鲁棒性。J[20]等人使用预训练 模型 wav2vec2 作 为特 征 提取 器， 将 每个Transfomer 层的输出特征进行时间归一化之后再加权求和，以得到后端分类器的输入特征。但预训练模型计算复杂度较高，计算花费代价太高。Lai [21]等人提出 InterFormer 网络，虽然通过双向特征交互模块（BFIM）和选择性融合模块（SFM）分别实现局部和全局特征融合，但是忽略了 通道上的特征信息且模型不够轻量级。Zhou Y[22]等人提出了轻量级跨维度交互注意（LCIA）模块来学习跨频域和时域的欺骗信息，但不足的是 LCIA 模块没有充分融合上下文信息，忽略了语音帧之间的相关特征。

针对以上问题，提出双分支通道注意力模块和注意力特征融合网络，双分支通道注意力模块在聚合全局和局部特征信息的同时还关注多尺度通道上下文信息；注意力特征融合网络将基于通道注意力的特征进行融合以学习具有通道上下文信息和全局局部特征信息的交互特征，从而精确地识别语音伪造信息。

本文主要贡献如下：（1）提出了双分支通道注 意 力 模 块 （ Dual-Branch Channel AttentionModule，DB-CAM），通过变化空间池化的方法在多个尺度上实现通道注意力，利用深度卷积在通道维度聚合多尺度上下文信息，同时捕捉全局和局部特征信息；（2）提出了基于注意力的特征融合模块（Attention Feature Fusion，AFF），将经过真实语音GMM 和欺骗语音GMM 处理得到的对数高斯概率特征基于注意力进行特征融合以学习具有通道上下文信息和全局局部特征信息的交互特征。

# 1 相关工作

# 1.1 高斯混合模型

GMM[14]是一种经典的统计学习模型，它是一种利用多维概率密度函数对语音信号进行建模的方法，其优点是具有良好的拟合任意类型分布的能力。高斯混合模型（GMM）是 $N$ 个具有不同的混合权重和不同的参数的高斯概率密度函数，即均值和协方差矩阵的加权和。其概率密度函数如公式（1）所示：

$$
P ( x ) = \sum _ { i = 1 } ^ { N } w _ { i } p _ { i } ( x )
$$

其中， $N$ 为 GMM 的阶数，表示高斯分量的个数； $x$ 为 $D$ 维特征向量； $\textstyle \boldsymbol { W } _ { i }$ 为第 $i$ 个高斯分量的权重，且满足 $\sum _ { i = 1 } ^ { N } w _ { i } = 1 \ ; \ p _ { i } ( x )$ 是维数为 $D$ 的高斯分量，其概率密度函数如公式（2）所示：

$$
p _ { i } ( x ) = { \frac { 1 } { ( 2 \pi ) ^ { D / 2 } \mid { \Sigma _ { i } } \mid ^ { 1 / 2 } } } e x p \{ - { \frac { 1 } { 2 } } ( x - \mu _ { i } ) ^ { \prime } { \Sigma _ { i } } ^ { - 1 } ( - \mu _ { i } ) \}
$$

其中， $\mu _ { i }$ 为均值向量， $\Sigma _ { i }$ 为协方差矩阵。

GMM 模型参数的训练常采用最大期望（Expectation-Maximization，EM）算法，训练的目的是找到均值、协方差和权重三个参数使得似然概率最大。EM 算法训练过程为：初始化均值、协方差和权重参数，计算期望，最大化期望，然后重复后面两个步骤直到模型收敛。

# 1.2 对数高斯概率特征

GMM 是一种传统的说话人识别分类器。对每一个语音特征，传统的 GMM 独立地累加多个高斯分量上的概率密度值，并没有单独考虑每个高斯分量的得分分布情况，同时也忽略了相邻语音帧之间的关系。由于真实语音和伪造语音在特征空间上的信息分布不同，所以它们在所有高斯分量上的得分分布也不同。Lei[13]等人提出的 GMM-ResNet 模型考虑所有 GMM 分量上的得分情况和帧之间的联系，并取得了良好的检测性能。本文将GMM-ResNet模型作为对比实验的模型之一，并在此基础上进行改进以提高模型的检测性能。本文使用对数高斯概率特征对LFCC 特征在每个高斯每个高斯分量的得分分布信息进行建模。对于一个 $D$ 维的原始帧特征$x$ 计算得到的对数高斯概率特征 $y$ 的大小等于GMM的阶数，分量 $y _ { i }$ 如公式（3）所示：

$$
y _ { i } = l o g p _ { i } ( x ) = - \frac { 1 } { 2 } x ^ { ' } \Sigma _ { i } ^ { - 1 } x + x ^ { ' } \Sigma _ { i } ^ { - 1 } \mu _ { i }
$$

然后，计算整个训练集上 $y _ { i }$ 的均值 mean 和标准差 $s t d$ ，并对 $y _ { i }$ 进行均值方差归一化，得到对数高斯概率特征 $f _ { i }$ ，如公式（4）所示：

$$
f _ { i } = \frac { y _ { i } - m e a n _ { y _ { i } } } { s t d _ { y _ { i } } }
$$

# 2 模型框架

# 2.1 双分支通道注意力

注意力机制[23][24]可以捕获全局和局部依赖的关系，防止网络过拟合，提高网络的泛化能力。在这部分中，提出了双分支通道注意力模块（ Dual-Branch Channel-Attention Model ，DB-CAM），并设计了两个分支来分别获取特征的全局和局部通道依赖，获取细粒度的特征表示，并基于 GMM-ResNet 模型提出了双分支通道注意力ResNet （ Dual-Branch Channel-Attention ResNet ，DBCA-ResNet）网络。DB-CAM 块如图 1 所示，通过变化空间池化的方法，可以在多个尺度上实现通道注意力。为了尽可能保持轻量化，我们仅在注意力模块内部提取局部上下文信息和全局上下文信息，本文选择深度卷积（Depth-wise Conv）作为通道上下文聚合器，它仅利用每个空间位置的逐点通道交互来提取特征的上下文信息。

![](images/c7ef815790dda7b5ea9a3e8007ee26131a154211afedce328bc1239393825f93.jpg)  
图 1 双分支通道注意力模块

对于局部通道分支，为了节省参数，给定局部特征 L（X） $\in \mathbf { R } ^ { \mathrm { { B } \times C \times T } }$ ，其中 B 表示批大小，C 表示通道数，T 表示输入语音的帧长，对局部特征 $\mathrm { ~ L ~ }$ （X）计算如公式（5）所示:

$$
L ( X ) = \beta ( C o n \nu 2 ( \delta ( \beta ( C o n \nu 1 ( X ) ) ) ) )
$$

其中 $X$ 表示输入特征， $\beta$ 表示批处理归一化， $\delta$ 表示 ReLU 激活函数， $C o n \nu 1$ 和 $C o n \nu 2$ 均是核大小为 1 的深度卷积，在每个通道上进行卷积操作，为了提取局部特征的上下文信息和防止过拟合，通过深度卷积对通道进行逐点交互，通过比例因子 $\mathbf { r }$ 来调整通道数，本文使用的比例因子 $\scriptstyle \mathbf { r } = 2$ 。

对于全局通道分支，值得注意的是 G（X）具有与输入特征 X 相同的形状，并且在这里加入了全局平均池化来保留和突出低层次特征中的信息，同样也使用了深度卷积和比例因子 $\mathbf { r }$ 提取全局特征的上下文信息和防止过拟合。对全局特征 G（X）计算如公式（6）所示：

$$
G ( X ) = \beta ( C o n \nu 2 ( \delta ( \beta ( C o n \nu 1 ( G A P ( X ) ) ) ) )
$$

其中 $G A P$ 表示全局平均池化，首先通过全局平均池化对输入特征 $X$ 进行处理，得到全局通道分支的特征，然后同样通过深度卷积对通道进行逐点交互来获取全局特征上下文信息，并且使用同样大小的比例因子 $\scriptstyle 1 = 2$ ，最后得到全局通道分支的输出特征。

给定全局特征 ${ \bf G } \left( { \bf X } \right)$ 和局部特征 L（X），可以获得DB-CAM 块的细粒度特征 $\mathbf { \boldsymbol { X } } ^ { \prime } \in \mathbf { \mathbb { R } } ^ { \mathrm { { \scriptscriptstyle B \times C \times T } } }$ 如公式（7）所示：

$$
\begin{array} { r } { \boldsymbol { X } ^ { \prime } = \boldsymbol { X } \otimes \boldsymbol { M } \left( \boldsymbol { X } \right) = \boldsymbol { X } \otimes \boldsymbol { \sigma } ( \boldsymbol { L } ( \boldsymbol { X } ) \oplus \boldsymbol { G } ( \boldsymbol { X } ) ) } \end{array}
$$

其中 M（ $( X )$ 表示 DB-CAM 块生成的注意力权重，σ表示 Sigmoid 激活函数， $\oplus$ 表示逐元素相加，⊗表示逐元素相乘。

基 于 上 述 DB-CAM 模 块 ， 本 文 提 出 了DBCA-ResNet 模型。DBCA-ResNet 用于伪造语音检测流程如图 2 所示。DBCA-ResNet 首先根据原始语音数据得到 LFCC 特征，经过计算得到对数高斯概率特征，再经过 ResNet 得到输出特征 X，其中ResNet 中使用 MFA 层以聚合浅层特征和深层特征得到进一步的特征表示，同时输出特征的通道大小也扩大到原来的 6 倍，然后将 ResNet 的输出特征输入到 DB-CAM 块中提取特征的多尺度上下文信息以及全局局部通道依赖，最后输入到全连接层得到最后的输出结果。

![](images/cf4344bd41089079ac064160558b2663c4e7ca6d44973a4025f1ac15ab431ee2.jpg)  
图 2 DBCA-ResNet 模型伪造语音检测流程图

# 2.2 注意力特征融合

为了增强模型的鲁棒性、提高检测准确率，本文同时训练了真实语音 GMM 和欺骗语音 GMM。随后采用了注意力特征融合的方式，将经过真实语音GMM 和欺骗语音GMM 得到的特征进行注意力融合，提高了模型的整体性能。

先将原始的LFCC 特征经过真实语音GMM 和欺骗语音 GMM 分别得到对数高斯概率特征，再将两个对数高斯概率特征分别作为 ResNet 模型的输入，随后经过 MFA 层来聚合多层次的特征，最后经过卷积下采样操作得到的输出特征图 X， $\mathrm { \Delta Y ~ \in ~ }$ $\mathbf { R } \mathbf { B } { \times } \mathbf { C } { \times } \mathbf { T }$ 。基于双分支通道注意力模块 $\mathrm { ~ \bf ~ M ~ }$ ，注意力特征融合（Attention Feature Fusion，AFF）可以表示为如公式（8）所示：$Z = ( 1 + \mathbf { M } \left( \mathbf { X } \oplus \mathbf { Y } \right) ) \otimes \mathbf { X } + ( 1 - \mathbf { M } \left( \mathbf { X } \oplus \mathbf { Y } \right) ) \otimes \mathbf { Y }$ （8）

其中 $X$ 表示真实语音数据的特征， $Y$ 表示欺骗语音数据的特征， $Z \in \mathrm { R } ^ { \mathrm { { B } \times \mathrm { { C } \times T } } }$ 表示输出的融合特征。$M$ 表示输入特征经过 DB-CAM 块之后得到的注意力权重。

AFF 模块如图 3 所示，X，Y 两个特征经过元素相加得到新的初始集成，之后输入到DB-CAM 块得到新的通道注意力权重，随后再将注意力权重和原始输入特征 X，Y 进行运算，其中实线表示 1＋M $A ~ ( \mathrm { \nabla { X } \oplus \mathrm { \nabla { Y } } } )$ ），虚线表示 $_ { 1 - \mathrm { { M } } }$ （ $\mathcal { X } \oplus \mathcal { Y } )$ ）。应该注意的是，融合权重 $1 + \mathbf { M }$ （ $\Chi \oplus \Upsilon )$ 由 0 和 2 之间的真实的组成， $1 { \mathrm { - M } } \ ( \mathbf { \mathbf { \mathbf { X } } } \oplus \mathbf { \mathbf { \mathbf { Y } } } )$ 也是如此，这使得网络能够在 X 和 $\mathrm { \Delta Y }$ 之间进行软选择或加权平均。经过DB-CAM 块之后得到左右两分支的输出特征，再将输出特征进行向量拼接最终得到 AFF 模块的输出特征 Z。

![](images/63674bc399598f26554062b6c57be74b1b44b53458da95cec62a7c4ec6f1710a.jpg)  
图 3 注意力特征融合模块

基于上述 AFF 模块，本文提出了 AFF-ResNet模型。AFF-ResNet 在两个分支上分别经过真实语音GMM 和欺骗语音 GMM 得到两个分支的输出特征X，Y。AFF-ResNet 用于伪造语音检测流程如图 4所示。在前端语音特征训练阶段，首先将 LFCC 特征经过真实语音GMM 和欺骗语音GMM 处理得到两个分支的输出，再经过 ResNet 模型得到输出特征 X 和 Y，其中 ResNet 中使用 MFA 层以聚合浅层特征和深层特征得到进一步的特征表示，同样输出特征的通道大小也扩大到原来的 6 倍。为了减少模型的参数量和避免过拟合问题，采用卷积下采样操作将通道大小重新缩减为原来的大小 C，分别得到特征图 X, $\scriptstyle \mathbf { Y ^ { \mathrm { { B } \times C \times T } } }$ 。在特征融合阶段，将特征图 X，Y经过AFF块进行注意力特征融合得到新的细粒度融合特征图 $Z ^ { \mathrm { B \times C \times T } }$ ，最后再经过全连接层得到最后的输出结果。

![](images/a85fe3786c22990d71acdb64163c78b3fad822e4f03534b959849ebba2a120ef.jpg)  
图 4 AFF-ResNet 模型伪造语音检测流程图

# 3 实验设置

# 3.1 数据集

为 了 评 估 模 型 的 有 效 性 ， 本 文 在ASVSpoof2019[25]和 ASVSpoof2021[12]数据集中的逻辑访问（Logical Access，LA）子集上进行了实验。ASVSpoof2019LA 数据集来源于 VCTK 语料库，真实语音直接选取自该语料库，共采集 107 名说话者的真实语音；欺骗语音则是在真实语音的基础上使用多种不同的语音合成与语音转换技术得到的，采样率均为 16khz。如表 1 所示，ASVSpoof2019LA数据集被划分为训练集、开发集与评估集三个部分，LA 场景共包含 12483 段真实语音和 108978段伪造语音。

表 1 ASVSpoof2019LA 数据集分布  

<html><body><table><tr><td rowspan="2">数据集</td><td colspan="2">说话人数</td><td colspan="2">语音条数</td></tr><tr><td>男性</td><td>女性</td><td>真实语音</td><td>伪造语音</td></tr><tr><td>训练集</td><td>8</td><td>12</td><td>2580</td><td>22800</td></tr><tr><td>开发集</td><td>4</td><td>6</td><td>2548</td><td>22296</td></tr><tr><td>评估集</td><td>21</td><td>27</td><td>7355</td><td>63882</td></tr></table></body></html>

同时，本文还选取 ASVSpoof2021LA 数据集来验证模型的性能。ASVSpoof2021LA 中提供了真实语音和伪造语音共 181566 条语音数据。值得一题的是，与 ASVSpoof2019LA 数据集中所有语音都是干净的有所不同，考虑到实际场景部署，例如在电话通信中进行人员身份认证，这可能会导致性能下降。因此，ASVSpoof2021LA 为了缩小实际场景和理想条件下的差异，将真实和欺骗语音在 IP 电话和公共电话交换网络之间传输并处理。

由于 ASVSpoof2021LA 中并不包括单独的训练集和开发集，因此本文使用 ASVSpoof2019LA 的训练集和开发集作为 ASVSpoof2021LA 的训练集和开发集来进行实验。

# 3.2 数据增强

众所周知，深度神经网络受益于数据增强(DataAugmentation，DA)生成的额外训练样本，DA 能减少过拟合，从而提高模型的泛化能力和分类性能。目前对原始波形进行数据增强的方法主要分为两种： $\textcircled{1}$ 利用外部数据集进行数据增强，如添加背景噪声和混响等； $\textcircled{2}$ 直接对原始波形进行操作，如RawBoost[26]、a-law 编解码、 $\mu$ -law 编解码、FIR 滤波等。本文采用 RawBoost 增强方法对原始波形进行数据增强，引入线性和非线性卷积以及与脉冲信号相关的加性噪声和与静态信号无关的加性噪声，将新的原始波形对 ASVSpoof2019LA 训练集进行扰动进行数据增强。

# 3.3 评价指标

本文使用官方评价指标：等错误率（Equal ErrorRate ，EER ）和最小检测代价函数（MinimumDetection Cost Function，min t-DCF）来检测模型性能。EER 和 min t-DCF 的值越低，模型检测性能越好。

等 错 误 率 是 错 误 接 受 率 （ False AcceptanceRate，FAR）与错误拒绝率（False Rejection Rate，FRR）同时达到最小值的平衡点，EER 值越低，表明系统的检测性能越优秀，能够准确区分真实和伪造语音。FAR 和 FRR 的计算公式分别如公式（9）和公式（10）所示：

$$
F A R = \frac { N _ { F A } } { N _ { S } } { \times } 1 0 0 \% 
$$

$$
F R R = \frac { N _ { F R } } { N _ { B } } { \times 1 0 0 \% }
$$

其中， $N _ { F A }$ 为被错误接受的样本数， $N _ { S }$ 为错误的样本总数， $\mathit { N _ { F R } }$ 为错误拒绝的样本数， $N _ { B }$ 为正确的样本总数。

最小检测代价函数是错误接受率和错误拒绝率的加权和，t-DCF 借鉴了最小风险贝叶斯决策进行系统可靠性评估。实际中，ASV 系统可能遇到合法用户、临时冒充的非法用户以及试图恶意操纵ASV 决策的攻击者，该指标综合考虑了不同情况下的误判代价，其计算公式如公式（11）所示：

$$
\begin{array} { r } { \begin{array} { r } { t - D C F = C _ { f r } \times F R R \times P _ { t a r g e t } } \\ { + C _ { f a } \times F A R \times ( 1 - P _ { t a r g e t } ) } \end{array} } \end{array}
$$

其中， $C _ { f r }$ 和 $C _ { f a }$ 分别为 $F R R$ 和 $F A R$ 的惩罚系数，$C _ { f r }$ 为误拒真实用户的代价， $C _ { f a }$ 为误接受伪造语音的代价。 $P _ { t a r g e t }$ 表示测试说话人为真实说话人的先验概率， $1 - P _ { t a r g e t }$ 表示测试说话人为欺骗说话人的先验概率。当提高 $C _ { f r }$ 时，系统会降低 $F A R$ 但会增加 $F R R$ ，t-DCF 计算时会更侧重降低 $F A R$ ；当提高 $P _ { t a r g e t }$ 时，系统会倾向于接受更多样本为真实，平衡点会向降低FR 的方向移动。

为了消除代价权重对算法性能评估的偏向性，更好的比较系统在平衡 $F R R$ 和 FAR 两类错误的能力，设置 $C _ { f r } { = } C _ { f a } { = } 1 . 0 \ _ { \circ }$ 由于 ASVSpoof 数据集中真实语音占比通常更高，因此设置 $P _ { t a r g e t } { = } 0 . 0 1$ 来避免模型的过拟合，确保在分布偏移的情况下算法依然有效。当设定这三个参数的值之后，有一组 FAR 和FRR 使得 t-DCF 的值最小，即 min t-DCF，此时系统综合性能达到最好。

# 3.4 参数设置

为 了 进 行 公 平 的 比 较 ， 实 验 实 现 了LFCC-ResNet、GMM-ResNet、DBCA-ResNet 以及AFF-ResNet 模型。其中，ResNet 包含 6 个相同通道数的残差块，本文将 LFCC-ResNet 作为基线系统。

实验使用LFCC 特征作为欺骗检测模型的原始声学特征。LFCC 特征是根据 ASVSpoof2021[12]基线配置提取的。使用一个具有 10ms 帧移， $2 0 \mathrm { m s }$ 帧长，1024 点傅里叶变换，通过截断或重复，将提取的 LFCC 特征固定长度为 400。高斯混合模型使用MSR Identity Toolbox 工具箱进行 30 次的 EM 迭代训练，以得到高斯分量分别为 128、256 和 512 的GMM。

实验使用 PyTorch 框架实现 LFCC-ResNet，GMM-ResNet、DBCA-ResNet 和 AFF-ResNet 模型。所有模型使用交叉熵损失作为损失函数，初始学习率为 0.0001 的 Adam 优化器进行训练，学习速率由ReduceLROnPlateau 调度器进行调整，最小学习率设置为 $1 0 ^ { - 8 }$ ，batchsize 设置为 32，迭代次数为 100。

# 4 实验结果及分析

# 4.1 DBCA-ResNet 在 ASVSpoofLA 数据集上的实验

表 2 显 示 了 DBCA-ResNet 模 型 在ASVSpoof2019LA 和 ASVSpoof2021LA 数据集上的实验结果。从表 2 可以看出，本文提出的DBCA-ResNet 取得了最优的结果，DBCA-ResNet在 ASVSpoof2019LA 和 ASVSpoof2021LA 数据集上的 EER 分别为 $1 . 3 8 \%$ 和 $2 . 6 0 \%$ ，min t-DCF 分别为 0.0397 和 0.2469。与 LFCC-ResNet 模型相比，DBCA-ResNet 在 ASVSpoof2021LA 数据集上 EER和 min t-DCF 分别相对降低了 $34 . 2 \%$ 和 $12 . 7 \%$ ；与GMM-ResNet 模 型 相 比 ， DBCA-ResNet 在ASVSpoof2021LA 数据集上 EER 和 min t-DCF 分别相对降低了 $14 . 2 \%$ 和 $3 . 8 6 \%$ 。这说明 DB-CAM 块在两个通道注意力分支上关注了全局和局部特征，提取了特征的多尺度上下文信息，从而得到细粒度的

特征表示。

# 4.2 AFF-ResNet 在 ASVSpoofLA 数据集上的实验

表 3 显 示 了 AFF-ResNet 模 型 在ASVSpoof2019LA 和 ASVSpoof2021LA 数据集上的实验结果。从表 3 可以看出，本文提出的AFF-ResNet 表 现 最 优 。 AFF-ResNet 在ASVSpoof2019LA 和 ASVSpoof2021LA 数据集上的EER 分别为 $1 . 3 3 \%$ 和 $2 . 4 7 \%$ ，min t-DCF 分别为0.0378 和 0.2394 。 与 DBCA-ResNet 模 型 相 比 ，AFF-ResNet 模 型 在 ASVSpoof2019LA 和ASVSpoof2021LA 数据集上 EER 分别相对降低了$3 . 7 6 \%$ 和 $4 . 7 9 \%$ ，min t-DCF 分别相对降低了 $5 \%$ 和$3 . 0 4 \%$ 。这说明 AFF 块将经过真实语音 GMM 和欺骗语音 GMM 的输出特征基于注意力进行特征融合，从而得到了具有通道上下文信息和全局局部特征信息的交互特征。

表 2 DBCA-ResNet 在 ASVSpoofLA 数据集上的实验结果  

<html><body><table><tr><td rowspan="2">模型</td><td colspan="3">ASVSpoof2019LA 特征阶数</td></tr><tr><td>min t-DCF EER/%</td><td>min t-DCF</td><td>EER/%</td></tr><tr><td rowspan="3">LFCC-ResNet</td><td>128 0.0790 2.50</td><td>0.3057</td><td>4.70</td></tr><tr><td>256 0.0690 2.21</td><td>0.2592</td><td>4.20</td></tr><tr><td>512 0.0708 2.20</td><td>0.2828</td><td>3.95</td></tr><tr><td rowspan="3">GMM-ResNet</td><td>128 0.0572 1.96</td><td>0.2711</td><td>3.64</td></tr><tr><td>256 0.0415 1.49</td><td>0.2592</td><td>3.17</td></tr><tr><td>512 0.0413 1.47</td><td>0.2568</td><td>3.03</td></tr><tr><td></td><td>128 0.0528</td><td>1.77 0.2536</td><td>3.06</td></tr><tr><td rowspan="3">DBCA-ResNet 256 512</td><td>0.0431</td><td></td><td></td></tr><tr><td>1.41</td><td>0.2495</td><td>2.75</td></tr><tr><td>0.0397 1.38</td><td>0.2469</td><td>2.60</td></tr></table></body></html>

表 3 AFF-ResNet 在 ASVSpoofLA 数据集上的实验结果  

<html><body><table><tr><td rowspan="2">模型</td><td rowspan="2">特征阶数</td><td colspan="2">ASVSpoof2019LA</td><td colspan="2">ASVSpoof2021LA</td></tr><tr><td>min t-DCF</td><td>EER/%</td><td>min t-DCF</td><td>EER/%</td></tr><tr><td>GMM-ResNet</td><td>512</td><td>0.0413</td><td>1.47</td><td>0.2568</td><td>3.03</td></tr><tr><td>DBCA-ResNet</td><td></td><td>0.0397</td><td>1.38</td><td>0.2469</td><td>2.60</td></tr><tr><td rowspan="3">AFF-ResNet</td><td>128</td><td>0.0471</td><td>1.71</td><td>0.2552</td><td>3.03</td></tr><tr><td>256</td><td>0.0509</td><td>1.65</td><td>0.2502</td><td>2.77</td></tr><tr><td>512</td><td>0.0378</td><td>1.33</td><td>0.2394</td><td>2.47</td></tr></table></body></html>

# 4.3 与其他模型对比

表 4 显示了不同模型在 ASVSpoof2021LA 数据集上的实验结果。从表 4 中可以看出，与LFCC+LCNN 模 型 相 比 ， AFF-ResNet 在

ASVSpoof2021LA 数据集上 EER 和 min t-DCF 分别相对降低了 $7 3 . 3 \%$ 和 $30 . 5 \%$ 。这说明了 AFF-ResNet网络不仅可以有效的融合通道上下文信息，关注语音帧之间的相关特征，并且可以捕捉欺骗语音中与真实语音不一致的特征信息。与 W2V2-128 模型相比，AFF-ResNet 在 ASVSpoof2021LA 数据集上 EER和 min t-DCF 分别相对降低 $30 . 2 \%$ 和 $1 1 . 5 \%$ 。虽然Group MS-ResNet 的 结 果 优 于 本 文 提 出 的AFF-ResNet 模型，但 Group MS-ResNet 中采用 LGP特征分组和多分支卷积的方法极大地增加了模型的参数量，而 AFF-ResNet 模型考虑到实际情况中计算消耗和准确率的平衡问题，这说明本文提出的AFF-ResNet 模型不仅可以将不同特征进行注意力融合以学习具有通道上下文信息和全局局部特征信息的交互特征，来精确的识别语音伪造信息，而且在时间成本和计算资源的低需求是其他模型不具备的，满足了实际部署的需求。

表 4 与其他模型在 ASVSpoof2021LA 数据集上的实验  

<html><body><table><tr><td colspan="3">结果对比</td></tr><tr><td rowspan="2">模型</td><td colspan="2">ASVSpoof2021LA</td></tr><tr><td>min t-DCF</td><td>EER/%</td></tr><tr><td>LFCC+LCNN[12]</td><td>0.3445</td><td>9.26</td></tr><tr><td>MobileNet[15]</td><td>0.3231</td><td>6.80</td></tr><tr><td>WavLM+MFA[16]</td><td>0.3197</td><td>5.08</td></tr><tr><td>RawNet2[17]</td><td>0.3099</td><td>5.31</td></tr><tr><td>MFA-Conformer[19]</td><td>0.3197</td><td>5.65</td></tr><tr><td>WavLM[27]</td><td>0.3048</td><td>5.23</td></tr><tr><td>AMENet[28]</td><td>0.2706</td><td>4.07</td></tr><tr><td>wav2vec2.0[29]</td><td>0.2792</td><td>3.67</td></tr><tr><td>W2V2-128[20]</td><td>0.2704</td><td>3.54</td></tr><tr><td>Group MS-ResNet[30]</td><td>0.2318</td><td>2.24</td></tr><tr><td>DBCA-ResNet</td><td>0.2469</td><td>2.60</td></tr><tr><td>AFF-ResNet</td><td>0.2394</td><td>2.47</td></tr></table></body></html>

# 5 结束语

本文首先提出双分支通道注意力模块，通过改变空间池化的方法，在不同的通道分支上实现通道注意力，并使用深度卷积沿通道维度聚合多尺度上下文信息，同时捕捉全局和局部特征信息；随后提出注意力特征融合模块，将经过真实语音 GMM 和欺骗语音GMM 得到对数高斯概率特征基于注意力融合以学习具有通道上下文信息和全局局部特征信息的交互特征。所提的 AFF-ResNet 网络显著提升伪造语音检测的准确率。在未来的工作中，我们将探索更先进的网络架构以进一步提高反欺骗系统的性能。

# 参考文献：

[1] TODISCO M ,HÉCTOR DELGADO, EVANS N.

Constant Q cepstral coefficients: A spoofing countermeasure for automatic speaker verification[J].Computer Speech & Language, 2017, 45:516-535.   
[2] SAHIDULLAH, MD, KINNUNEN, TOMI., HANILCI, CEMAL. A comparison of features for synthetic speech detection[C]//International Speech Communication Association. Dresden: IEEE, 2015:2087-2091.   
[3] TAK H, PATINO J, TODISCO M, et al. End-to-end anti-spoofing with rawnet2[C]//ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing. Toronto: IEEE, 2021:6369-6373.   
[4] XIE Y, ZHANG Z, YANG Y. Siamese network with wav2vec feature for spoofing speech detection[C]//International Speech Communication Association. Brno: ICSA, 2021: 4269-4273.   
[5] LEE J W, KIM E, KOO J, et al. Representation selective self-distillation and wav2vec 2.0 feature exploration for spoof-aware speaker verification[J]. arXiv:2204.02639, 2022.   
[6] CAI D, WANG W, LI M, et al. Pretraining conformer with asr for speaker verification[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing. Greek island of Rhodes: IEEE, 2023: 1-5.   
[7] LAVRENTYEVA G, NOVOSELOV S, TSEREN A, et al. STC antispoofing systems for the ASVspoof2019 challenge[J]. arXiv:1904.05576, 2019.   
[8] HE K, ZHANG X, REN S, et al. Deep residual learning for image recognition[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. Las Vegas: IEEE, 2016: 770-778.   
[9] LAI C I, CHEN N, VILLALBA J, et al. ASSERT: Anti-spoofing with squeeze-excitation and residual networks[J]. arXiv:1904.01120, 2019.   
[10] HU J, SHEN L.Squeeze-and-excitation networks[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. Salt Lake City: IEEE, 2018: 7132-7141.   
[11] LIU B, CHEN Z, QIAN Y. Attentive feature fusion for robust speaker verification[C]//International Speech Communication Association. Incheon: ICSA, 2022: 286-290.   
[12] YAMAGISHI J, WANG X, TODISCO M, et al. ASVspoof 2021: accelerating progress in spoofed and deepfake speech detection[J]. arXiv:2109.00537, 2021.   
[13] LEI Z, YU H, YANG Y, et al. Attention network with gmm based feature for asv spoofing detection[C]//Chinese Conference on Biometric Recognition. Shanghai: Springer, 2021: 458-465.   
[14] LEI Z, YAN H, LIU C, et al. Two-path gmm-resnet and gmm-senet for asv spoofing detection[C]//ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing. Singapore: IEEE, 2022: 6377-6381.   
[15] WEN Y, LEI Z, YANG Y, et al. Multi-path gmm-mobilenet based on attack algorithms and codecs for synthetic speech and deepfake detection[C]//International Speech Communication Association. Incheon: ICSA, 2022: 4795-4799.   
[16] Guo Y, Huang H, Chen X, et al. Audio deepfake detection with self-supervised wavlm and multi-fusion attentive classifier[C]//ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing. Seoul: IEEE, 2024: 12702-12706.   
[17] WU H, ZHANG J, ZHANG Z, et al. Robust spoof speech detection based on multi-scale feature aggregation and dynamic convolution[C]//ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing. Seoul: IEEE, 2024: 10156-10160.   
[18] ZHANG Y, LV Z, WU H, et al. Mfa-conformer: multi-scale feature aggregation conformer for automatic speaker verification[J]. arXiv:2203.15249, 2022.   
[19] WANG Y, NISHIZAKI H, LI M. Pretraining conformer with asr or asv for anti-spoofing countermeasure[J]. arXiv:2307.01546, 2023.   
[20] MARTÍN-DOÑAS J M, ÁLVAREZ A. The vicomtech audio deepfake detection system based on wav2vec2 for the 2022 ADD challenge[C]//ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing. Singapore: IEEE, 2022: 9241-9245.   
[21] LAI Z H, ZHANG T H, LIU Q, et al. Interformer: interactive local and global features fusion for automatic speech recognition[J]. arXiv:2305.16342, 2023.   
[22] ZHOU Y, ZHANG J, ZHANG P. Spoof speech detection based on raw cross-dimension interaction attention network[C]//Chinese Conference on Biometric Recognition. Beijing: Springer, 2022: 621-629.   
[23] FAN D P, WANG W, CHENG M M, et al. Shifting more attention to video salient object detection[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Long Beach: IEEE, 2019: 8554-8564.   
[24] DAI Y, GIESEKE F, OEHMCKE S, et al. Attentional feature fusion[C]//Proceedings of the IEEE/CVF winter conference on applications of computer vision. Waikoloa: IEEE, 2021: 3560-3569.   
[25] TODISCO M, WANG X, VESTMAN V, et al. ASVspoof 2019: future horizons in spoofed and fake audio detection[J]. arXiv:1904.05441, 2019.   
[26] TAK H, KAMBLE M, PATINO J, et al. Rawboost: A raw data boosting and augmentation method applied to automatic speaker verification anti-spoofing[C]//ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing. Singapore: IEEE, 2022: 6382-6386.   
[27] 曾高俊,任英杰,芦天亮,等.一种结合自监督预训练与多 种注意力适配器微调的伪造语音检测方法[J].小型微型 计 算 机 系 统 http://kns.cnki.net/kcms/detail/21.1106.TP.20250117.1524. 026.   
[28] 陈佳.基于深度学习的欺骗语音检测研究[D].杭州：杭州 电子科技大学,2024.   
[29] 吴敦志,陈为真.基于聚类中心的浅层特征融合伪造语 音检测[J].计算机工程与设计, 2024, 45(10):2922-2928.   
[30] 曹梦奇.基于改进的多分支 ResNet 模型的合成语音伪 造检测研究[D].南昌：江西师范大学,2024.