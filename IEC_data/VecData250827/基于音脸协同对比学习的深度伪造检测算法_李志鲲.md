# 基于音脸协同对比学习的深度伪造检测算法

李志鲲，叶登攀†空天信息安全与可信计算教育部重点实验室，武汉大学 国家网络安全学院，湖北 武汉 430072收稿日期：2023⁃04⁃26  †通信联系人 Email：yedp@whu. edu. cn基金项目：国家自然科学基金（62072343，U1736211）；国家重点研发计划（2019QY（Y）0206）第一作者：李志鲲，男，硕士生，现从事深度伪造及检测研究。E-mail：2016301500065@whu. edu. cn摘 要： 主流的伪造检测方法从视觉单模态出发，通过检测视频帧中的伪影进行伪造检测，然而不同伪造方法会引入不同的伪影，导致这类方法有受限的性能和较差的泛化能力。同时，当前利用音频信息进行伪造检测的工作并没有充分利用音频信息发掘视觉模态的篡改。观察到自然视频中的音频和人脸具有内在协同性，而伪造方法均会对这种特性造成破坏，因此本文提出了一种音脸协同驱动的深度伪造检测算法，称为SFSD（Speech-Face Synergy Detect）。该算法提出了音脸协同对比学习策略，在通用视频数据集上构建样本，模拟伪造方法对音脸协同性的破坏，实现了对大量无标注真实视频的利用，提升了模型性能和泛化能力。算法构建了多模态模型SFformer（Speech-Face Transformer），其通过注意力瓶颈引导音脸模态浓缩并交融必要的信息，减少冗余信息的干扰，提升了模型的特征提取能力并改进了检测性能。在公开数据集FakeAVCeleb 上的大量实验表明，SFSD 在预训练后准确率为 $7 2 . 1 2 \%$ ，超越部分基准方法；在经过迁移训练后准确率达到 $89 . 5 1 \%$ ，高于先前工作，并且泛化能力有所提升。

关 键 词： 伪造检测；对比学习；多模态融合中图分类号： TP37 文献标志码：A

文章编号：1671-8836（2025）02-0173-14

# Deepfake Detection Algorithm Based on Speech-Face Synergy Contrastive Learning

LI Zhikun，YE Dengpan

Key Laboratory of Aerospace Information Security and Trusted Computing，Ministry of Education，School of Cyber Science and Engineering，Wuhan University，Wuhan 430072，Hubei，China

Abstract：Most mainstream deepfake detection methods are based on visual single-modality, which detects deepfake videos by identifying fake artifacts in video frames. However, different deepfake methods may introduce different artifacts; therefore such methods have limited performance and poor generalization ability. Meanwhile, the current work utilizing audio information does not fully leverage audio information to uncover tampering in the visual modality. Natural videos have intrinsic synergy between speeches and faces, and deepfake methods this disrupt the intrinsic synergy between speeches and faces; this paper proposes a speech-face synergy-driven deepfake detection algorithm, called SFSD (Speech-Face Synergy Detection). The core of the algo⁃ rithm is the audio-face synergy contrastive learning strategy, which constructs samples on a general video dataset to simulate the disruption of audio-face synergy using forgery methods and pre-trains the detection model on these samples. This strategy utilizes a large number of unlabeled real videos and enhances model performance and generalization capability. A multimodal model named SFformer (Speech-Face Transformer) is constructed, which utilizes an attention bottleneck to guide the condensation and fusion of essential information in the audio-face modality, reduces interference from redundant information, improves the model's feature ex⁃ traction capability, and enhances detection performance. Numerous experiments on the public dataset FakeAVCeleb demonstrate that the accuracy of SFSD can reach $7 2 . 1 2 \%$ after pre-training, surpassing some benchmark methods, and the accuracy reaches

$89 . 5 1 \%$ after transfer learning, which is higher than that of previous studies, and improves the generalization ability. Key words：deepfake detection；contrastive learning；multimodal fusion

# 0 引 言

基于生成对抗网络[1]和变分自编码器[2]的深度伪造（Deepfake）技术可以合成虚假的人物肖像或者声音[3-6]，使得真实视频中的人物身份、面部表情或者唇部运动遭到篡改。Deepfake 技术在动画、虚拟人和在线教育等领域的应用，吸引了学术界和工业界越来越多从业者的目光。然而，该项技术的滥用也会引发严重问题，非专业人员可以使用易获取的公开应用程序,如 DeepFaceLab[7]，方便地伪造逼真的人脸，部分恶意用户会利用此技术制作色情视频，损害公民的肖像权和名誉权，挑战社会道德和秩序，甚至危害国家安全。因此，设计伪造检测方法，辨别互联网上泛滥的伪造视频很有必要。目前已经有许多工作[8-11]致力于检测深度伪造视频。其中，基于视觉单模态伪造检测方法是一种被广泛应用的方法，它将深度伪造检测视为一个图像二分类的任务，通过深度伪造图像中的伪影（如上采样过程中带来的色域和频域空间的变化和后处理过程给图像引入的痕迹）辨别伪造视频帧，实现视频伪造检测。然而这类方法存在显著的缺陷，即过于依赖视频帧中的伪造特征进行检测，而不同的伪造方法因为使用不同的生成模型和后处理过程，会给图像引入不同的伪造特征，因此这类工作会严重过拟合于训练数据集，很难学习到具有泛化能力的特征，导致性能受限，并且对未知的伪造方法检测性能较差。

由于上述基于视觉单模态伪造检测方法的缺陷，近年来一些学者尝试利用音频信息，辅助视觉信息更好地发掘视觉模态的篡改，增强伪造检测方法的性能，如 Chugh 等[12]利用小时间段内的模态不和谐分数判别视觉模态是否存在伪造，Cheng 等[13]利用伪造方法会破坏视频中音脸身份一致性的特点进行伪造检测，Mittal 等[14]通过检测视频中音频与人脸表达出的情感倾向是否一致来辨别伪造视频。不过这些工作仍有不足之处：自然视频中音脸之间的协同特性是多种方面的，而这些工作中的方法局限于使用一个方面的协同特性进行伪造检测。当伪造手段对这种单一特性破坏程度较少，如利用相同人的语音修改唇形并未对身份一致性和情感匹配性造成破坏时，这些方法会受限；与此同时，这些方法对音脸信息单独进行特征提取，没有利用音脸互补信息增强特征提取能力。

当前的伪造方法不仅会对面部进行篡改，还会对视频中的音频模态进行篡改。因此，有工作研究同时检测视频中是否存在一种或两种模态被篡改。这类工作虽然扩充了视觉单模态伪造检测任务，聚焦于检测多个模态的篡改，但并没有研究如何利用音频信息更好地发掘视觉模态的伪造。此外，这类方法共性问题在于，伪造数据集中的视频规模有限，仅仅利用伪造数据集进行训练不利于让模型学习视频中音脸关系表征。因此需要设计一种方案，合理地使用网络空间中存在的大量未标注的真实视频，让模型学习到自然视频丰富的音脸内在表示。

为了解决以上问题，本文设计了音脸协同检测（Speech-Face Synergy Detect）的 算 法 ，提 出 利 用 视频间音脸协同特性进行伪造检测，以更加全面地利用音频信息发掘视觉模态伪造。本文认为视频中音脸协同性表现在以下两个方面：1） 语音与人脸的身份性应当是一致的；2） 语音内容与人脸面部运动应当是同步的。而深度伪造方法会对这种特性造成破坏。为了引导网络学习到真实视频和伪造视频的音脸协同性差异，本文提出了音脸协同对比学习策略，依据上述定义的协同性，在完全真实的视频数据集上构建样本，模拟伪造方法对视频音脸协同性的破坏，并对模型进行预训练。该策略建立在以下共识基础上：首先，伪造数据集的规模远小于真实音视频数据集，因此利用对比学习的方法构建样本，可以增大模型的数据投喂量，让模型更具有泛化能力，防止模型过拟合；其次，通过对比学习预训练，模型可以学习到更好的参数分布，在迁移至伪造数据集上训练时能够得到更佳的性能。

为了更好地利用语音与人脸模态的互补信息，并对音脸间的协同性进行建模、提取音脸融合特征进行伪造检测，本文基于Transformer[15]结构构建了SFformer(Speech-Face Transformer),引入多模态融合学习思想[16]，在网络的前半部分对语音和人脸分别进行特征提取，在网络的后半部分实现模态信息的融合与交换。由于音脸跨模态的信息是彼此互补与冗余的，为了让音脸间交换必要的信息、减少冗余信息的使用并降低计算量，设计了基于注意力瓶颈（Attention Bottleneck）[17]的信息交换与融合层。不同于其他的多模态融合方法，注意力瓶颈可以引导模态浓缩并交换对另一个模态而言必要的信息，融合音脸间对伪造检测而言更有效的信息。本文将最终的音脸融合特征作为视频的音脸协同性表征，并利用此表征进行伪造检测。

# 1 相关工作

# 1. 1 深度伪造

深度伪造在学术界和工业界已经成为研究热门。现有的伪造算法均基于图像，利用图像-图像转换的过程完成伪造人脸的合成，并合成伪造视频。早期的换脸算法需要为每一对互换的人脸训练编码器-解码器网络[18]，不能实现任意人脸互换。随着技术发展，出现可以实现高质量的任意人脸互换算法，如 Korshunova 等[3]将换脸视作图像风格迁移的过程，实现了任意人脸互换；Nirkin 等[4]提出FSGAN，利用插值引导的姿态转换网络、身份交换网络和人脸修复网络，解决了姿态差异过大带给换脸过程的困难并能降低换脸区域与背景区域的边界差异；Li 等[19]提出 FaceShifter，利用级联 AAD 块在多个特征级别内集成身份和人脸属性，生成逼真的人脸。除了人脸交换外，有的深度伪造方法尝试利用音频驱动视频中说话人的唇形，Jamaludin 等[20]提出Speech2Vid 模型，利用音频驱动静止人脸合成说话人视频；Prajwal 等[6]提出 Wav2Lip，利用了唇形同步鉴别器，监督网络合成具有良好的语音唇形同步的说话人视频；Zhou 等[21]提出 PC-AVS 模型，在利用音频驱动唇形的基础上，实现对说话人的姿态控制。伪造算法虽然不断更新，可以实现合成越来越逼真的面部，但其不可避免地会对音频与人脸的协同性造成破坏：换脸算法会改变说话人的身份，因此对音脸的身份一致性造成破坏；而伪造方法均是将视频切分为帧后，逐帧生成伪造人脸，伪造人脸的运动在时域上难以维持和真实视频一样的连贯性，因此伪造方法也会对人脸与语音的运动同步性造成破坏。

# 1. 2 伪造检测技术

深度伪造检测任务通常被看作是一个二分类任务（真或假）。主流的伪造检测方法基于视觉单模态信息，通过检测伪造伪影进行伪造检测[10]。孙鹏等[22]提出利用伪造方法后处理过程带来的拼接部位色彩偏移量不一致进行伪造取证，张怡暄等[23]通过伪造方法引入的时域帧间差异进行人脸篡改视频检测，Masi 等[24]提出双分支网络，同时提取颜色域和频域的伪影来进行伪造检测。Nguyen 等[8]将胶囊网络引入至伪造检测任务中。SSTNet[25]通过空间、隐写分析和时间特征检测编辑过的人脸。Qian等[26]提出F3Net，利用图像频率感知、图像分解分量和局部频率统计特征进行伪造检测。基于视觉单模态的方法具有一定的效果，但忽视了伪造方法给图像引入的伪造特征是不同的：1） 从合成人脸融合至原图时，经过的后处理过程不同，因此伪造图像拥有不同的拼接痕迹；2） 生成器会给图像带来独特的频域变化，被称为指纹[27]，不同的伪造方法会使用不同结构的生成网络，因此带给图像的指纹也不同。所以基于视觉单模态的方法往往会陷入过拟合的困境：其通过训练集无法学习如何提取具有良好泛化性的特征，不仅在测试集上会展现出受限的性能，在面对未知的伪造方法时性能更是会严重下降。

为了解决上述缺陷，有学者尝试利用音频信息更好地发掘视觉模态的篡改。如Chugh 等[12]利用小时间段内的模态不和谐分数判别视觉模态是否存在伪造，Mittal 等[14]提出了利用音频与人脸的情感线索特征进行伪造检测，因为正常视频中的音频与人脸具有相同的情感偏见，因此利用音频与人脸的情感差异线索可以辅助伪造检测，但由于人类表达情感方式是多样的，这种情感差异并不存在于所有的伪造视频中。Cheng 等[13]提出了 VFD，通过视频中语音与人脸的身份相似性判别视频是否经过篡改。该方法仅利用了音频与人脸单一方面的匹配特性，忽视了音频信息同样可以反映人脸运动是自然连贯的，因此对音频信息的利用并不充分。同时，部分深度伪造视频并不会对音脸的身份一致性造成破坏，如用同一个人的音频驱动说话面部的合成，这种情况下该方法会失效。上述方法的共同缺陷在于，他们对音频与人脸特征单独进行特征提取，并利用向量的距离判别真假，但人类对模态信息的感知是同时进行的[16]，因而忽视了利用模态间的互补信息。

还有一些学者尝试进行多模态深度伪造检测，同时检测视频中是否存在一个或两个模态的篡改。Zhou 等[28]提出了联合视听篡改检测，利用视觉和听觉的内在同步流检测是否存在视觉或听觉模态的篡改。Ilyas 等[29]提出 AVFakeNet，利用 SwinTrans⁃former 结构[30]检测视频流或语音流是否经过伪造。Cai 等[31]提出基于边界感知时域检测的多模态伪造检测方法，并构建了一个包含视觉与听觉模态篡改的 数 据 集 Localized Audio Visual DeepFake 进 行 实验。Yang 等[32]提出了 AVoiD-DF，探索时间与空间上的视听不一致性进行多模态深度伪造检测，并且构建了大规模多模态伪造数据集 DefakeAVmiT 进行实验，取得了很好的效果。但这类方法更多的是将伪造检测任务扩充为多模态检测任务，并没有更深入地探究如何利用音频模态信息增强视觉模态的篡改检测性能。同时，这些工作没有探究如何利用网络空间中存在着的大量未标注的自然视频，让模型学习到更丰富的音脸内在表示。

从以上方法得到启发，本文提出通过音脸间的协同性进行伪造检测。音脸协同性在音脸身份一致性上进行了扩展，加入了音频与人脸的运动同步性，更加充分地利用了音频信息。为了引导网络学习到伪造视频对音脸协同性的破坏，设计了音脸协同对比学习策略，在真实的音视频数据集上构建正负样本对模型进行预训练，增强了模型的性能和泛化能力。最后，本文基于注意力瓶颈机制，构建了多模态模型SFformer，与通常的多模态融合方式不同，基于注意力瓶颈机制的多模态交融能够使得音脸间更加有效地交换必要的互补信息，避免冗余信息的干扰，因而增强了模型的检测性能。

# 2 音脸协同驱动的伪造检测

# 2. 1 方法依据

本文利用音脸协同性进行伪造检测，主要基于以下两点：1） 音脸协同性存在于自然视频中，并且可以定义为身份一致性和运动同步性；2） 伪造视频与真实视频的音脸协同性存在显著差异。

音脸协同性可以从多个先前研究中得到验证。人类的感知和神经学研究发现，可以从一个人的声音勾勒出一个人的外表，反之亦然[33]。因此，自然视频中的音脸存在着身份一致性。2018 年，Nagrani等[34]提出了音脸跨模态匹配的任务，通过多路分类器，判断输入的多个人脸哪一个最可能与某个音频片段来源于相同身份。随后，不断有研究致力于音脸匹配的任务[35-36]，不过通常做法是，将人脸与语音嵌入特征空间中，通过向量距离判别与语音最接近的人脸图像。自动唇读任务（Lip-Reading）、语音识别任务（Speech-Recognition）分别将视频中的视觉和语音信息转变为文字输出[37-38]，因此可以推断，视频中的人脸运动与语音内容存在着同步性，Chung等[39]的唇形同步研究更是验证了这一点。在 Zhou等[40]的说话人脸合成研究中，将人脸和语音信息解耦合为身份特征向量和语音内容向量，再将身份特征向量与另一段音频的语音内容向量重新耦合，生成与这段音频同步的说话人脸，因此可以说明，这两种特性存在于同一种特征空间中，属于相互耦合的状态。综上所述，将音脸协同性定义为身份上一致性与运动上同步性有充足的理论依据（图1）。

![](images/071d4b27e87de2d9e1f94464518f43607fb11c8bd0cb5a71390a09e429f4820f.jpg)  
图1　音脸协同性  
Fig. 1　Speech-face synergy  
图2　真伪视频音脸运动同步性  
Fig. 2　Sync between speeches and faces in real/fake videos

因此，可以从身份一致性和运动同步性分别验证伪造视频的协同性与真实视频有明显差异。文献[13]已经用实验验证了伪造方法会破坏真实视频的音脸身份一致性，因此只需进一步验证伪造方法会破坏真实视频中音脸间的运动同步性差异即可。本文使用 SyncNet[39]结构进行了实验，SyncNet[39]是一个唇形同步鉴别网络，可以鉴别语音内容与唇形运动是否同步。随机抽取一定数量的真实视频和伪造视频，并用SyncNet 计算音脸运动同步度，结果如图 2 所示。从实验结果可以看出，伪造视频的音脸同步性明显弱于真实视频。再结合文献[13]的结论，说明伪造视频的音脸协同性差于真实视频，这为利用音脸的协同特性进行伪造检测提供了基础。

# 2. 2 SFformer 结构

为了对视频的音脸协同性进行建模，本文提出了如图 3 所示的 SFformer 结构，由四部分组成：数

60 realfake50 40 L  
量  
数 30201000.1 0.2 0.3 0.4 0.50.55同步性

据预处理层、特征提取层、信息交换与融合层以及分类层。SFformer 基于多模态融合中的中期融合思想构建[16]，输入为一串人脸帧序列和对应的音频段，首先在数据预处理层中，将输入转变为符合Transformer 结构输入的样式，随后在特征提取层中，人脸模态和音频模态将分别进行特征处理。紧接着，在信息交换与融合层中，音脸模态信息会进行模态信息交流与融合，最终给出模态各自的特征以及音脸融合特征，该音脸融合特征即代表视频的音脸协同性表征。最后在分类层中对音脸融合特征通过多层感知机进行二分类，以判断视频是否经过篡改。

![](images/a845346e2a81357b43c354598174669be12b501c977ac6b5c29e5fd5e43946bd.jpg)  
图 3　SFformer（Speech-Face Transformer）结构Fig. 3　Structure of SFformer（Speech-Face Transformer）

# 2. 2. 1 数据预处理

通常的伪造检测任务中，对数据集中视频的预处理方式是将其切分为视频帧，以图片为单位进行伪造检测。单张人脸图片也足以反映人脸身份信息，因此在文献[13]中也使用单张图片作为基本单位。但单张人脸图片无法反映出人脸的运动特性，因此本文采用与文献[6，39]相同的做法，将视频以5帧一组切分，以连续的 5 张视频帧作为视觉模态输入和检测的单位。同时，截取相应的音频片段作为听觉模态输入。随后，对其进行预处理过程，以符合模型的输入形式。

本文将这5 张视频帧通过人脸检测算法截取人脸区域并缩放为相同大小的人脸图片，作为模型视觉模态的输入。而对于输入语音片段，则按照其他音脸任务的通常做法[6，20-21，39]，将其转变为梅尔语谱图（Mel Spectrum）的形式。语谱图（Spectrum）是声音波形的频率表示形式，其在频率上是线性分布的，而人耳对声音的感受则是对数变化的，因此梅尔语谱图将语谱图的频率刻度进行了对数化，以更符合人耳的感受模式，更加适合音脸任务。

由于Transformer 结构需要一组具有相同维度的向量作为输入，称为tokens，因此将人脸图像和语谱图映射为维度为dim 的tokens，映射流程如图4 所示。对于宽和高为 $H$ 与 $W$ 的人脸图像，用输入通道数与图像通道数相同、输出通道数为 $\dim$ ，卷积核大小和步长均为 $\boldsymbol { \mathscr { p } }$ 的卷积对图像进行一次卷积操作，即将一张图像转换为 ${ \frac { H } { P } } \times { \frac { W } { P } }$ 个大小为 $\dim$ 的tokens。为了保留输入序列的时序信息，原始的Transformer 结构会一次性处理所有输入的tokens，并为输入序列添加余弦位置编码。然而，这种方式并不适用于图像这种高度结构化的数据。为此， $\mathrm { V i T ^ { [ 4 1 ] } }$ （Visiual Transformer）使用了一组可学习的随机参数与tokens 相加作为位置编码。而由于SFformer 的输入是一段视频帧，需要保留帧之间的时序信息，因此除了在图像内随机化一组可学习参数外，还要将来自第i帧的tokens与tokens维度相同、全为整数i的向量相加，以保留帧之间的位置信息（i以将这一组连续帧的起始帧视为第0 帧得到）。按照与ViT[41]相同的方式，随机初始化一个 CLS_Token 与这些来自图像块的tokens 合并作为特征提取层的输入，这个输入代表了视觉模态的整体特征。

梅尔语谱图的两个维度分别为时间和频率，为了保留时域上的特征，将其按照不同的频率分段，拉平为一维向量，并经过线性映射变换后，加入随机位置编码转变为输入的 tokens。同样地，随机初始 化 一 个 CLS_Token，与 来 自 语 谱 图 的 tokens 合并，用其代表听觉模态的整体特征。

![](images/29ad1b9a23d223e7eefc12315f3db3354231fb4a943bfe6c3ad9007838e63ef3.jpg)  
图 4　映射结构与 Transformer 块结构Fig. 4　Structures of projection and Transformer block

# 2. 2. 2 基于 Transformer 的特征提取

在网络的前N 层中，使用Transformer 块进行人脸和语音的特征提取。Transformer 块结构如图 4所示，由层归一化（Layer Norm）、多头自注意力（Multi-Head Self-Attention，MSA）、多 层 感 知 机（Multilayer Perceptron，MLP）和跳跃连接（ResdiualConnection）组成，前向传播的过程可形式化表示如下：

$$
Z _ { i } ^ { * } = \operatorname { M S A } \left( \operatorname { L N } \left( Z _ { i - 1 } \right) \right) + Z _ { i - 1 } , i = 0 , 1 , \cdots , N ,
$$

$$
Z _ { i } = \mathrm { M L P } \left( \mathrm { L N } \left( Z _ { i } ^ { * } \right) \right) + Z _ { i } ^ { * } , i = 0 , 1 , \cdots , N
$$

其中， $Z _ { i }$ 为经过 $i - 1$ 层 Transformer 块后的 tokens，$\boldsymbol { Z } _ { i } ^ { * }$ 为经过残差结构得到的中间结果。因此，在信息交换与融合层前，模型的整个前向传播过程如下：

$$
Z _ { \mathrm { f a c e s } } = \mathrm { c o n v } ( \mathrm { f a c e s } ) \oplus E ,
$$

$$
Z _ { \mathrm { s p e e c h e s } } = \mathrm { l i n e a r } ( \mathrm { m e l s } ) \oplus E ,
$$

$$
\begin{array} { r } { Z _ { \mathrm { f a c e s } } ^ { \mathrm { 0 } } = \left[ \mathrm { C L S } _ { \mathrm { f a c e s } } \middle | \middle | Z _ { \mathrm { f a c e s } } \right] , } \end{array}
$$

$$
\begin{array} { r } { Z _ { \mathrm { s p e e c h e s } } ^ { \mathrm { 0 } } = [ \mathrm { C L S } _ { \mathrm { s p e e c h e s } } ]  Z _ { \mathrm { s p e e c h e s } }  ] , } \end{array}
$$

$\begin{array} { r } { Z _ { \mathrm { f a c e s } } ^ { i } = \mathrm { T r a n s f o r m e r } ( Z _ { \mathrm { f a c e s } } ^ { i - 1 } ; \theta _ { \mathrm { f a c e s } } ^ { i } ) , i = 1 , 2 , \cdots , N , } \end{array}$ $Z _ { \mathrm { s p e e c h e s } } ^ { i } = \mathrm { T r a n s f o r m e r } ( Z _ { \mathrm { s p e e c h e s } } ^ { i - 1 } ; \theta _ { \mathrm { s p e e c h e s } } ^ { i } ) _ { i }$ ,$i = 1 , 2 , \cdots , N$ 1其中， $\bigoplus$ 表示逐元素相加， $E$ 表示位置编码。faces 代表输入人脸帧，mels 为对应的梅尔语谱图， $Z _ { \mathrm { f a c e s } }$ 代表视觉模态的 tokens， $Z _ { \mathrm { s p e e c h e s } }$ 代表听觉模态的tokens， $\theta$ 代表Transformer 块的参数，|| 表示将 tokens 合并。

# 2. 2. 3 基于注意力瓶颈的信息交融

在网络的后 $M$ 层中，本文引入了注意力瓶颈（Attention Bottleneck）机制[17]，用以实现音脸模态的信息交换与融合。通常的多模态 Transformer 会将所有模态的 tokens 进行合并，放入 Transformer 块中进行学习。但这样的做法存在缺陷，并且并不适合伪造检测任务。原因在于：1) 多头自注意力机制会对所有tokens 两两进行计算，而模态间存在许多冗余的信息，因此将所有tokens 一起合并，会导致网络难以学习到重要的特征；2) 模态间的表示、大小均不同，由于模型输入的视觉图像大小远大于语谱图的大小，因此来自视觉模态的tokens 数量也远多于来自听觉模态的tokens 数量，将其一起合并，会导致网络忽略来自听觉模态的信息，并且会干扰视觉模态的特征提取；3) 伪造检测任务不同于一般的多模态分类任务，所提出的策略期望利用音频增强视觉模态的篡改检测性能，但音频模态在本质上并没有区分，所以用所有的听觉模态tokens 进行信息交换与融合会干扰网络学习到与视觉模态伪造相关的特征。因此本文使用注意力瓶颈机制进行信息交融的过程。

在模型初始化时，随机生成 $K$ 个与输入 tokens维度一致的 bottleneck tokens，但其在网络的特征提取 层 不 参 与 训 练 。 bottleneck tokens 在 音 脸 模 态 完成单独的特征提取后，分别与音脸模态tokens 拼接，送入后续的 Transformer 块中：

$$
\begin{array} { r } { \big [ \ Z _ { \mathrm { f a c e s } } ^ { i } \rVert Z _ { \mathrm { b o t t l e s } } ^ { i ^ { * } } \big ] { = } \operatorname { T r a n s f o r m e r } ( \ \big [ \ Z _ { \mathrm { f a c e s } } ^ { i - 1 } = } \end{array}
$$

$$
\lVert Z _ { \mathrm { b o t t l e s } } ^ { i - 1 } \rVert ; \theta _ { \mathrm { f a c e s } } ^ { i } ) , i = N + 1 , N + 2 , \cdots , K ,
$$

$$
1 , N + 2 , \cdots , K
$$

其中， $Z _ { \mathrm { b o t t l e s } }$ 代 表 bottleneck tokens。 引 入 bottlenecktokens 后，音脸模态间的信息交流与融合将只会通过bottleneck tokens 进行。此时语音和人脸模态的 tokens不再是并行地进入Transformer 块中进行处理，而是以串行的方式：首先将语音 tokens 与 bottleneck tokens 拼接 ，送入 Transformer 块中 ，更新一次人脸 tokens 和bottleneck tokens，然后再将 bottleneck tokens 与语音tokens 拼接送入 Transformer 块中，更新一次语音 tokens和 bottleneck tokens。bottleneck tokens 的数量被设置为远少于音脸模态tokens 的数量，此时跨模态的注意力必须通过这些瓶颈进行，因此会迫使模态浓缩自己的信息，并只共享必要的信息、减少冗余信息的流动，因此可以在降低计算量的同时提升多模态特征融合能力。

bottleneck tokens 不 仅 能 够 实 现 音 脸 模 态 信 息的交流，也实现了音脸模态信息的融合。在前向传播过程中，由于其不仅学习到了人脸模态信息，而且学习到了听觉模态信息，因此在 bottleneck tokens中实现了音脸模态信息的融合。在模型的最后，通过平均池化层获取视频的音脸融合特征表示，这个特征向量代表了音脸协同性表征，将其送入多层感知机中获取二分类结果：

$$
F _ { \mathrm { f u s i o n } } { = } \mathrm { A V G } \left( Z _ { \mathrm { b o t t l e s } } \right)
$$

$$
{ \mathrm { r e s u l t } } = \mathrm { M L P } \left( { F _ { \mathrm { f u s i o n } } } \right)
$$

其中， $\mathrm { A V G } \left( \ast \right)$ 表示平均池化， $F _ { \mathrm { f u s i o n } }$ 代表音脸融合特征， $\mathrm { M L P } \left( \ast \right)$ 代表多层感知机，result 为分类结果。

# 2. 3　音脸协同对比学习

对比学习（Contrastive Learning）是一种机器学习方法，旨在通过学习相似和非相似的样本之间的差异来学习数据的表示。对比学习使用正样本和负样本两类样本，并将其嵌入到特征空间中，使用一个损失函数度量相似性和非相似性。该损失函数使相似的样本在特征空间中距离更近，非相似的样本在特征空间中距离更远。对比学习常用于无监督学习，其优势在于：1） 可以利用未经过标注的数据进行学习；2） 可以学习到更加通用的特征表示，提高模型的泛化能力。

对于伪造检测任务而言，由于视觉内容差异不明显，构建正负样本较为困难。而音频信息的引入，使得这一过程变得容易起来。因此，本文设计了一种音脸协同对比学习策略，利用规模较大的真实视频数据集构建样本，模拟伪造方法对视频的音脸协同性的破坏并对模型进行预训练，使模型提取更具泛化能力的特征；同时，也能够让模型拥有更好的初始参数分布，相较于直接在伪造数据集上训练，它获得更优秀的性能。

# 2. 3. 1 正负样本选取

由于本文将音脸协同性定义为身份特征一致性与运动特征同步性，因此正负样本的选取基于这两方面考虑。对于一组选定的人脸帧，如图5 所示，将其对应的语音片段作为锚（Anchor），人脸帧作为正样本（Positive Samples）。为了模拟深度伪造方法对身份一致性的破坏，从来自不同身份演讲者的视频中选取一组人脸帧，作为一类负样本；为了模拟深度伪造方法对人脸面部运动与语音内容同步性的破坏，在相同视频中，移动 $0 . 2 { \sim } 0 . 5 \ \mathrm { s }$ ，选择一组人脸帧作为另一类负样本。

![](images/42e1e6e38918a640e94df7407d8384ed3b95119d48a6c181903ef15c73cf8a1a.jpg)  
图5　正负样本选取  
Fig. 5　Positive and negative samples

# 2. 3. 2　损失函数设计

在生成正负样本后，需要构建对比损失对模型进行预训练。由于本文所构建的 SFformer 最终会给出三个特征：语音特征、人脸特征以及音脸融合特征，因此，可以利用传统对比损失，让SFformer 学习将正样本对嵌入相近的特征向量中，将负样本对嵌入较远的特征向量中。由于注意力瓶颈的作用，协同性信息会随注意力计算过程，流动至音脸融合特征中，因此最后对音脸融合特征使用交叉熵损失进行二分类。对比损失为InfoNCE 损失[42]和约束损失之和。InfoNCE 损失计算如下：

$$
\begin{array} { r } { L _ { \mathrm { m f o N c E } } = - \log \big [ \frac { \mathrm { e } ^ { \mathrm { d i s } ( { \boldsymbol { v } } , f ^ { + } ) / \tau } } { \mathrm { e } ^ { \mathrm { d i s } ( { \boldsymbol { v } } , f ^ { + } ) } + \displaystyle \sum _ { i = 1 } ^ { N } \mathrm { e } ^ { \mathrm { d i s } ( { \boldsymbol { v } } , f _ { i } ^ { - } ) } } \big ] } \end{array}
$$

其中， $\boldsymbol { v }$ 为语音特征向量，作为锚 $\cdot f ^ { + }$ 为正样本得到的人脸特征向量， ${ f _ { i } } ^ { - }$ 为负样本得到的人脸特征向量，下标 $i$ 表示负样本标号； $\tau$ 为温度系数，其为一个超参数； $\mathrm { d i s } \left( \ast , \ast \right)$ 表示计算向量距离，这里使用余弦相似度计算。

直接使用 InfoNCE 损失会导致网络只针对面部运动特征进行优化。由于很容易获取到同一个人说不同话的样本，但是获取不同人说相同话的样本却难以获得，而预期是想让网络同时针对身份特征和面部运动特征进行学习，输入不同身份的负样本中的面部运动也是不同的。因此本文设计了另外一个度量损失用以约束。假设 $N$ 个负样本中与正样本身份相同的有 $j$ 个，那么约束损失如下：

$$
\begin{array} { c } { { \displaystyle { \cal L } _ { \mathrm { s } } = \log \big [ 1 + \sum _ { i = 0 } ^ { N - j } \exp ( \operatorname { d i s } ( f ^ { T } , f _ { i } ^ { - } ) - } } \\ { { \operatorname { d i s } ( f ^ { T } , \displaystyle { \frac { 1 } { j } } \sum _ { k = 0 } ^ { j } f _ { k } ^ { + } ) ) \big ] } } \end{array}
$$

其中， $\boldsymbol { \mathscr { f } } ^ { T }$ 是与语音片段相匹配的视频帧的特征向量 $\mathbf { \nabla } \mathcal { I } _ { i } ^ { - }$ 为负样本中身份与 $f ^ { T }$ 不相同的样本的特征向量 $\mathbf { \Delta } , f _ { k } ^ { + }$ 为与身份相同的样本的特征向量。此损失函数意图在于不让网络忽视对身份特征的学习。综上，预训练阶段总体的对比损失计算如下：

$$
L _ { \mathrm { c o n } } { = } L _ { \mathrm { I n f o N C E } } { + } \lambda _ { 1 } L _ { \mathrm { s } }
$$

其中， $\lambda _ { 1 }$ 为超参数。对比损失让网络学习到了提取音脸协同性相关特征，通过信息交融层后，音脸融合特征中包含了协同性信息，因此可以让网络直接通过音脸融合特征判别输入的人脸帧与语音是否匹配，最终的预训练阶段的音脸协同损失函数为二分类交叉熵损失与对比损失的叠加：

$$
L _ { \mathrm { p r e } } = L _ { \mathrm { c l s } } + \lambda _ { 2 } L _ { \mathrm { c o n } }
$$

其中， $L _ { \mathrm { p r e } }$ 代表预训练损失， $L _ { \mathrm { c l s } }$ 代表二分类交叉熵损失， $\lambda _ { 2 }$ 为超参数。

在迁移至深度伪造数据集上进行训练时，此时的输入变为人脸帧与其对应的语音切片，一段语音切片只对应一组相应的人脸帧，因此不能直接使用公式（6）作为正式训练的对比损失函数。本文使用修改过后的度量损失作为正式训练阶段的损失函数 ，称 为 音 脸 协 同 对 比 损 失（Voice-Face SynergyLoss，VFSL）。定义如下的样本距离：

$$
\left\{ \begin{array} { l l } { V _ { \mathrm { p o s } } = d ( f _ { \mathrm { s p e e c h e s } } ^ { \mathrm { r e a l } } , f _ { \mathrm { f a c e s } } ^ { \mathrm { r e a l } } ) } \\ { V _ { \mathrm { n e g } } = d ( f _ { \mathrm { s p e e c h e s } } ^ { \mathrm { f a k e } } , f _ { \mathrm { f a c e s } } ^ { \mathrm { f a k e } } ) } \end{array} \right.
$$

其中， $V _ { \mathrm { p o s } }$ 代表真实视频的音脸协同相似度， $V _ { \mathrm { n e g } }$ 代表伪造视频的音脸协同相似度，VFSL 的损失如下：

$$
L _ { \mathrm { { v F S L } } } = - \sum P \log \left( { \mathrm { s o f t m a x } } \left( \left[ V _ { \mathrm { p o s } } , V _ { \mathrm { n e g } } ^ { i } \right] \right) \right)
$$

其中， $P$ 为长度与样本数量相等的一维向量，第0 位为 1，其余为 0，该损失会使得真实视频的音脸协同相似度增大，而伪造视频的音脸协同相似度变小。与预训练阶段类似，正式阶段的总体损失同样为二分类交叉熵损失叠加上述的音脸协同对比损失：

$$
{ \cal L } = { \cal L } _ { \mathrm { c l s } } + \lambda _ { 3 } { \cal L } _ { \mathrm { v F S L } }
$$

其中的 $L _ { \mathrm { c l s } }$ 为二分类交叉熵损失， $\lambda _ { 3 }$ 为超参数。

# 3 实验结果与分析

# 3. 1 数据集

在预训练阶段，本文使用了两种数据集训练网络。

1）VoxCeleb2 数据集[43]。VoxCeleb2 数据集是一个从YouTube 视频中收集的通用视听数据集，包含了来自6 000 多名演讲者的100 多万个演讲视频。

2）LRS2(Lip Reading Sentence2)数 据 集[44]。 该数据集包含了从 BBC 电视节目中收集的数千个说话人视频。

预训练阶段主要使用 VoxCeleb2 数据集，对于每名演讲者，随机选取10 s 的片段进行作为预训练数据集，由于LRS2 数据集中没有标注视频身份，因此在预训练阶段从LRS2 数据集中获取数据作为负样本的补充。训练集、验证集、测试集大小比例为8∶1∶1。

在正式训练和测试阶段，使用了以下两种深度伪造数据集：

1）FakeAVCeleb 数据集[45]。该数据集是一个视频多模态深度伪造数据集，于 2021 年公开，使用了 4 种不同的伪造方法，从 500 个真实视频中生成了 19 500 个伪造视频。同时该视频具有高度的种族性别平衡比例以消除歧视问题。由于该数据集的真实视频数量远小于伪造视频数量，为了解决数据不平衡问题，从 VoxCeleb2 数据集中选取了相同身份说话人的 7 500 个视频，将真实视频数量扩充到 8 000 个。然后将 300 位说话人的真实视频和相应的伪造视频作为训练集，将 100 位说话人的真实视频和相应的伪造视频作为验证集，剩余的视频作为测试集。在模型的预训练、正式训练、验证和测试阶段均不包含来自相同身份演讲者的视频。

2） DeepfakeTIMIT 数 据 集[46]。 该 数 据 集 于2018 年公开，包含两个子集，即 $6 4 \times 6 4$ 分辨率大小的 LQ 集以及 $1 2 8 \times 1 2 8$ 大小的 HQ 集，每个子集包括16 对看起来相似的受试者的10 个换脸视频。从VidTIMIT 数据集[47]上下载了真实视频进行实验。

为了防止信息泄露，实验的预训练阶段和训练阶段中，训练集、验证集、测试集均不包含来自相同身份的视频。

# 3. 2　实验环境及参数设置

本文在pytorch1. 9 环境下搭建网络并且进行实验，训练过程中使用默认参数的 Adam 优化器[48]对参数进行优化。在预训练阶段，为了达到最好的训练效果，设定初始的学习率为 $1 . 0 \times 1 0 ^ { - 4 }$ ，在训练100 个 epoch 后将学习率降低为 $1 . 0 \times 1 0 ^ { - 5 }$ 。在微调阶段，设定全程的学习率为 $1 . 0 \times 1 0 ^ { - 5 }$ 。与文献[13]相同，网络参数用均值为0，标准差为0. 02 的随机正态分布初始化，并且将公式（5）中的温度系数设为0. 1。损失函数的超参数中，考虑到交叉熵损失函数为主要损失函数，对比损失函数为辅助函数，经过实验后，将损失函数的超参数设为 0. 1 时取得最优的结果，因此损失函数的超参数设定为 0. 1。参照文献[13]的经验，每种类型的负样本数量均为 2。与其他伪造检测工作相同[10，13]，输入视频帧通过人脸检测算法提取面部区域后，统一调整大小为$2 2 4 \times 2 2 4$ ，而输入的语音切片根据文献[6]的经验，用 $8 0 \times 1 6$ 大小的梅尔频谱图表示。模型在两块3090 显卡上以批量大小为64 进行训练。

模型设置方面，综合考虑文献[17]的经验和任务实际，为达到最优的效果，经过实验后，设置输入tokens 的维度为1 024。对于输入的人脸帧，经过输入通道为15、输出通道为1 024、卷积核大小为28 的卷积变换为维度为1 024 的输入tokens，并与通过标准正态分布生成的CLS_Token 进行拼接，因此输入视觉tokens 的总数为65。对于输入的语音样本，在频率维度上以10 为步长进行分块，通过全连接层后变为长度为1 024 的tokens，并与通过标准正态分布生成的 CLS_Token 进行拼接，因此输入的听觉 tokens 总数为 11。多头自注意力机制中设置了 16 个注意力头，网络的前六层为特征提取层，后六层为信息交换和融合层。信息交换和融合层中，通过均值为 0、标准差为 0. 02 的高斯分布生成了两个bottlenecks token。

# 3. 3 实验结果

使 用 ACC(Accuracy，准 确 率) 和 AUC（AreaUnder Curve）作为评判指标。

与多个先前的优秀研究工作进行了比较，这些方法可以分为利用单模态视觉信息检测和利用音频信息辅助的多模态检测两类。

单模态的方法：该方法仅利用视觉信息检测视频 帧 中 的 伪 影 ，对 比 的 工 作 包 括 Meso-4[11]、MesoInception $- 4 ^ { [ 1 1 ] }$ 、Xception[10]、Capsule[8]、 $\mathrm { F ^ { 3 } { - } N e t ^ { [ 2 6 ] } }$

和 ViT[41]。

多模态检测的方法：该方法引入了音频作为辅助信息进行伪造检测，对比的方法有：EmotionalForensics[14]、 MesoInception-4_MM[49]、 Efficient⁃Net_ $\mathrm { M M } ^ { [ 4 9 ] }$ 、VGG16_MM[49]、VFD[13]和 MDS[12]。

# 3. 3. 1 对比实验结果

表 1 展示了在 FakeAVCeleb 和 DeepfakeTIMIT数据集上的结果。

可以看出，SFSD 没有在伪造数据集上进行正式训练，仅经过预训练的情况下，在FakeAVCeleb 数据集上准确率便达到了 $7 2 . 1 2 \%$ ，AUC 达到了 $74 . 5 7 \%$ ，超过了基准方法Meso $^ { - 4 }$ ，说明仅经过预训练后，模型就具有了一定的伪造检测能力，验证了所提出的音脸协同对比学习策略的有效性。在迁移至伪造数据集上进行训练后，在FakeAVCeleb 数据集上准确率达到了 $89 . 5 1 \%$ ，AUC 达到了 $9 2 . 4 8 \%$ ，高于所有基于视觉单模态的方法。对比使用了音频信息的工作中表现最好的方法VFD，SFSD准确率提升了3. 91个百分点，AUC 提升了5. 8 个百分点，验证了SFSD 方法的可行性。在DeepfakeTIMIT 数据集上，SFSD 的AUC 值均高于 Emotional Forensics，其中在 LQ 集上提升了 4. 94个百分点，在 HQ 集上提升了 3. 6 个百分点。DeepfakeTIMIT 数据集提出时间较早，伪造方法造成的伪造痕迹较重，因此所有方法的AUC 均达到了不错的结果。而FakeAVCeleb 数据集较新，使用的伪造方法更为先进，伪造痕迹并不明显，而SFSD 达到了最高的准确率和AUC 值。

# 3. 3. 2 泛化能力测试

FakeAVCeleb 中包含了两种视频深度伪造方法：一种为换脸，使用了FaceSwap 和FSGAN；另一种为语音驱动说话人脸，使用了Wav2Lip。本文将其按此分为两种子集，一种仅使用了换脸算法，另一种则使用了 Wav2Lip。将模型在单个子集上进行训练，然后直接迁移至某一个子集上进行测试，结果如表2 所示。

从 表 2 的 实 验 结 果 可 以 看 出 ，SFSD 在FakeAVCeleb 的两个子集上同样达到了最佳的准确率，分别为89. $58 \%$ 和 $8 8 . 7 4 \%$ 。训练集为换脸集时，所有方法在Wav2Lip 集上的准确率都低于换脸集，这是由于Wav2Lip 集仅仅对图像的唇部运动进行了篡改，相较于对整个面部进行篡改的换脸算法而言伪造痕迹较小。泛化性方面，所提出的 SFSD不仅迁移至另一子集测试后准确率仍旧最高，并且准确率下降幅度小于其他方法，从换脸集迁移至Wav2Lip 集 时 下 降 了 13. 57 个 百 分 点 ，而 从

表1　性能比较Table 1 Performance comparison  

<html><body><table><tr><td rowspan="2">方案</td><td rowspan="2">模态</td><td rowspan="2">FakeAVCeleb上 ACC/%</td><td rowspan="2">FakeAVCeleb上 AUC/%</td><td colspan="2">DeepfakeTIMIT上AUC/%</td></tr><tr><td>HQ</td><td>LQ</td></tr><tr><td>Meso-4</td><td>视觉</td><td>65.32</td><td>67.57</td><td>62.10</td><td>63.25</td></tr><tr><td>MesoInception-4</td><td>视觉</td><td>75.82</td><td>68.45</td><td>78.45</td><td>79.12</td></tr><tr><td>Xception</td><td>视觉</td><td>82.12</td><td>83.49</td><td>96.78</td><td>97.90</td></tr><tr><td>Capsule</td><td>视觉</td><td>76.19</td><td>79.54</td><td>81.60</td><td>82.40</td></tr><tr><td>F3-Net</td><td>视觉</td><td>84.58</td><td>86.45</td><td>97.50</td><td>98.39</td></tr><tr><td>ViT</td><td>视觉</td><td>81.49</td><td>82.53</td><td>96.42</td><td>97.24</td></tr><tr><td>MesoInception-4_MM</td><td>视觉十听觉</td><td>74.87</td><td>77.86</td><td>77. 54</td><td>78.34</td></tr><tr><td>EfficientNet_MM</td><td>视觉十听觉</td><td>79.40</td><td>78.56</td><td>94.24</td><td>96.75</td></tr><tr><td>VGG16_MM</td><td>视觉十听觉</td><td>74.12</td><td>75.67</td><td>84.52</td><td>86.56</td></tr><tr><td>MDS</td><td>视觉十听觉</td><td>84.86</td><td>86.70</td><td>96.54</td><td>97.54</td></tr><tr><td>BA-TFD</td><td>视觉十听觉</td><td>83.57</td><td>86.65</td><td>95.16</td><td>96.38</td></tr><tr><td>Emotional Forensics</td><td>视觉十听觉</td><td></td><td></td><td>94.90</td><td>96.30</td></tr><tr><td>VFD</td><td>视觉十听觉</td><td>85.60</td><td>86.68</td><td>99.82</td><td>99.95</td></tr><tr><td>SFSD1(本文，未经迁移)</td><td>视觉十听觉</td><td>72. 12</td><td>74.57</td><td>78.38</td><td>79.10</td></tr><tr><td>SFSD(本文,预训练十迁移)</td><td>视觉十听觉</td><td>89.51</td><td>92.48</td><td>99.84</td><td>99.90</td></tr></table></body></html>

注：“—”表示无此数据，粗体数值表示最优结果。

# 表2　泛化能力测试

Table 2 Generalized ability testing   

<html><body><table><tr><td rowspan="2">方案</td><td rowspan="2">训练集</td><td colspan="2">测试集ACC/%</td></tr><tr><td>换脸集</td><td>Wav2Lip集</td></tr><tr><td>Xception</td><td>换脸集</td><td>83.52</td><td>66.46</td></tr><tr><td>F-Net</td><td>换脸集</td><td>85.65</td><td>68.26</td></tr><tr><td>ViT</td><td>换脸集</td><td>82.64</td><td>63.58</td></tr><tr><td>VFD</td><td>换脸集</td><td>86.26</td><td>69.42</td></tr><tr><td>SFSD(本文)</td><td>换脸集</td><td>89.58</td><td>76.01</td></tr><tr><td>Xception</td><td>Wav2Lip集</td><td>67.56</td><td>81.56</td></tr><tr><td>F3-Net</td><td>Wav2Lip集</td><td>69.82</td><td>83.49</td></tr><tr><td>ViT</td><td>Wav2Lip集</td><td>65.48</td><td>80.56</td></tr><tr><td>VFD</td><td>Wav2Lip集</td><td>70.58</td><td>82.59</td></tr><tr><td>SFSD(本文)</td><td>Wav2Lip集</td><td>77.35</td><td>88.74</td></tr></table></body></html>

Wav2Lip 集迁移至换脸集则下降了 11. 39 个百分点。同样使用了语音信息的VFD，准确率下降幅度大于 SFSD，说明其泛化能力弱于 SFSD。该实验表明，本文所提出的算法对音频信息利用更加充分，并且基于注意力瓶颈层的SFformer 信息利用能力更强，有更好的特征提取与交融能力。

# 3. 3. 3 鲁棒性测试

视频在传播的过程中，常常会受到各种因素的干扰，导致原始视频画面失真，给伪造检测带来困难。为了进一步验证基于音脸协同特征进行伪造检测的优越性，本文进行了如下的鲁棒性测试。将在原始数据集上训练好的模型，迁移至加入了JPEG 压缩和高斯模糊的测试集上进行测试，用以验证模型的抗干扰能力，结果如表3 所示。

表3　鲁棒性测试Table 3　Robustness testing  
%   

<html><body><table><tr><td>方案</td><td>原始准确率</td><td>JPEG压缩后 准确率</td><td>高斯模糊后 准确率</td></tr><tr><td>Xception</td><td>82.12</td><td>66.68</td><td>63.56</td></tr><tr><td>F³-Net</td><td>84.58</td><td>71. 32</td><td>69.47</td></tr><tr><td>ViT</td><td>81.49</td><td>64.96</td><td>62.54</td></tr><tr><td>VFD</td><td>85.60</td><td>72.16</td><td>70.24</td></tr><tr><td>SFSD(本文)</td><td>89.51</td><td>76.43</td><td>74. 68</td></tr></table></body></html>

注：粗体数值表示最优结果。

本文在测试阶段使用了 Albumentations 库分别对测试集进行了强度为60 的JPEG 压缩增强和高斯模糊增强，但是在训练阶段并未引入相应的数据增强方法，而是直接使用在原始数据集上训练完毕的模型直接迁移测试。从测试结果可以看出，本文所提出的 SFSD 在加入干扰后准确率仍保持最高，并且下降幅度最小，VFD 准确率下降幅度大于本文所提算法，说明了SFSD 抗干扰能力强于其他方法，具有更强的鲁棒性。

# 3. 3. 4 消融实验

本节对模型进行了一系列消融实验以验证所提各项策略的有效性。

本文所提出的SFformer 进行音脸融合的方式，在多模态领域中属于中期融合（middle fuse），即对模态特征进行单独的特征提取后，再对特征进行融合。在多模态领域中也存在着早期融合（early fuse）和后期融合（late fuse）的特征融合方式，因此本文同时构建了基于早期融合的 SFformer 和基于后期融合的SFformer 进行了对比实验。在早期融合中，不单独进行特征提取，而是将所有的模态tokens 一起合并输入到后续 Transformer 块中；在后期融合中，音脸模态分别进行特征提取后，直接将特征向量拼接作为音脸融合特征。对比结果如表4 所示。

Table 4　Accuracy of different fusion methods   

<html><body><table><tr><td>方案</td><td>训练方式</td><td>融合方式</td><td>准确率/%</td></tr><tr><td>SFformer-EF</td><td>仅预训练</td><td>早期</td><td>51.67</td></tr><tr><td>SFformer-MF</td><td>仅预训练</td><td>中期</td><td>72.12</td></tr><tr><td>SFformer-LF</td><td>仅预训练</td><td>后期</td><td>60.54</td></tr><tr><td>SFformer-EF</td><td>未预训练</td><td>早期</td><td>81.32</td></tr><tr><td>SFformer-MF</td><td>未预训练</td><td>中期</td><td>86.68</td></tr><tr><td>SFformer-LF</td><td>未预训练</td><td>后期</td><td>81.46</td></tr><tr><td>SFformer-EF</td><td>预训练十迁移</td><td>早期</td><td>82.68</td></tr><tr><td>SFformer-MF</td><td>预训练十迁移</td><td>中期</td><td>89.52</td></tr><tr><td>SFformer-LF</td><td>预训练十迁移</td><td>后期</td><td>81.46</td></tr></table></body></html>

注：EF、MF 和LF 分别代表早期融合、中期融合和后期融合，粗体数值表示最优结果。

由表 4 中可知，在基于早期融合的 SFformer上 ，预训练策略并没有起到作用，准确率仅为51. $67 \%$ ，原因在于预训练阶段，利用对比学习的方式生成的正负样本，本质上均为真实内容，而早期融合的方式在一开始混合了来自两个模态的信息，视觉模态的tokens 输入远多于听觉模态，导致网络只关注了来自视觉模态的内容，因而预训练策略失败。因此同样地，迁移至伪造数据集后，基于早期融合的SFformer 也并没有取得较好的效果，准确率为 $8 2 . 6 8 \%$ ，仅比基于纯视觉信息的 ViT 高出 1. 03个百分点，表明模型并没有很好地利用音频信息。而在基于后期融合的SFformer 上，预训练策略取得了一定的效果，可以在经过预训练的情况下准确率达到 $60 . 5 4 \%$ ，不过基于后期融合的方法仍存在缺陷，即音脸模态的特征提取中没有利用到彼此的互补信息，并且不同模态的输入大小和表示不同，给特征融合带来困难：语音信息通过语谱图的形式输入，是一个频率矩阵，而视觉信息以图像的形式输入，是一个像素矩阵，对应0. 2 s 的视频段而言，输入的图像矩阵大小为 $1 5 { \times } 2 2 4 { \times } 2 2 4$ ，而语谱图的大小仅为 $1 { \times } 8 0 { \times } 1 6$ ，相较而言，视觉内容的表示更为稠密，而听觉内容更加稀疏，将这两个模态的特征向量直接拼接送入分类器，会让分类器偏向于某一个模态的信息，而对另一个模态的信息没有加以利用，这就给跨模态的信息交流带来了困难，因而准确率较低。而基于中期融合的SFformer 中，模态特征先进行了单独提取，而后半部分利用注意力瓶颈进行音脸信息的交融，避免了模态间冗余信息的干扰，充分利用了模态间的互补信息，取得了最优的效果。此外，经过预训练的 SFformer 相较于未经过预训练、直接在伪造数据集上开始训练的 SFformer，准确率提升了2. 84 个百分点，同时验证了所提出的音脸协同对比学习策略能够有效地提升模型性能。

为了验证所提出的对比损失的有效性，分别对对比学习中常用的对比损失：三元组损失、N-Pairs损失、InfoNCE 损失进行了实验，结果如表 5 所示（表5 中均为仅在真实数据集上预训练，随后直接迁移至伪造数据集测试的结果）。

表4　不同融合方式准确率  
表5　不同对比损失准确率  
Table 5　Accuracy of different contrastive losses   

<html><body><table><tr><td>对比损失</td><td>准确率/%</td></tr><tr><td>三元组损失</td><td>65.18</td></tr><tr><td>N-Pairs损失</td><td>69.64</td></tr><tr><td>InfoNCE损失</td><td>70.06</td></tr><tr><td>InfoNCE损失十约束损失(本文）</td><td>72. 12</td></tr></table></body></html>

注：粗体数值表示最优结果。

从表5 可以看出，三元组损失的效果最差，准确率仅为 $6 5 . 1 8 \%$ 。这是因为三元组损失中仅能使用一个正样本和一个负样本，负样本数量过少，网络未能充分训练。N-Pairs 损失和 InfoNCE 损失中均采用多个负样本，其性能比较接近，准确率都比三元组损失高。由于2. 3. 2 节中提到的负样本选取问题，单纯的InfoNCE 损失效果差于本文提出的叠加了约束损失的效果，本文构建的损失函数准确率相较于 InfoNCE 损失提升了 2. 06 个百分点。该实验表明本文提出的对比损失的有效性。

最后，本文探究了信息交融层中，注意力瓶颈的数量对模型的影响，结果如图6 所示。

实验所用模型均没有经过预训练，从图 6 可以看出，bottlenecks 数量在 $1 { \sim } 4$ 时，模型的准确率都达到了 $85 \%$ 以上，在 bottlenecks 数量为 2 时最高，但是在数量超过5 后，模型的准确率显著降低，这是因为在 bottlenecks 数 量 远 少 于 输 入 的 音 脸 模 态 tokens时，可以通过bottlenecks 减少冗余信息的交换，迫使模态浓缩必要的信息，因而提升模型性能。当 bot⁃tlenecks 数量超过 4 后，由于输入的听觉模态 tokens数量为11，此时通过随机化生成的 bottlenecks 数量过多，不仅不能起到信息交换的作用，多余的bottlenecks 还会被当作噪声，干扰网络的训练。

![](images/9605a2c217a3ef31591c1ad93a4b70816a50509c1d884188382c0bc4737ef912.jpg)  
图6　注意力瓶颈数量对性能影响 Fig. 6　The impact of the number of attention bottlenecks on performance

# 4　 结 语

本文从深度伪造方法对音脸协同性破坏的角度出发，设计了一种音脸协同驱动的深度伪造检测方案（SFSD）。为了解决基于视觉单模态的检测模型检测性能受限和泛化能力差的问题，提出利用音脸的协同特性进行伪造检测，并从理论与实验上验证了伪造方法会破坏音脸的协同性。为了实现对音脸互补信息的充分利用，提出了基于中期融合和注意力瓶颈的SFformer 网络，该网络首先对音脸模态单独进行特征提取，随后利用注意力瓶颈进行音脸信息的交流与融合过程。为了增强模型的性能和泛化能力，设计了一种基于音脸协同对比学习的预训练方法，可以在完全真实的视频数据集上构建样本，模拟伪造方法对音脸协同性的破坏，并对网络进行预训练，增强其性能和泛化能力。在伪造数据集上进行的大量实验表明，SFSD 在仅经过预训练的情况下准确率可以达到 $7 2 . 1 2 \%$ ，验证了预训练策略的有效性，经过迁移后准确率可以达到$89 . 5 1 \%$ ，超越了其他的伪造检测工作。泛化实验、鲁棒性测试表明，基于音脸协同性设计的 SFSD 算法有优秀的泛化能力和抗干扰能力。最后，消融实验验证了所提出策略的有效性。

伪造方法虽然会引入不同的伪造特征，但均会对自然视频的自然语义产生破坏，所提出的音脸协同性仅是一种初探，未来可以从更多方向考虑如何让模型学习自然视频与伪造视频的高级语义差异。此外，所提出的方法需要用音频输入，然而互联网中存在着许多没有音频的视频，在这种情况下，使用了音频信息的伪造检测方法将会失效。因此未来可以考虑知识蒸馏策略，让模型利用音频信息学习到的视觉篡改特征，通过蒸馏策略指导纯视觉模型更好地识别视觉篡改。

# 参考文献：

［1］ GOODFELLOW I， POUGET-ABADIE J， MIRZA M，et al. Generative adversarial networks［J］. Communi⁃ cations of the ACM，2020，63（11）：139-144. DOI：10. 1145/3422622.   
［2］ KINGMA D P，WELLING M. Auto-encoding variational Bayes［EB/OL］. 2013：arXiv：1312.6114. http：//arxiv.org/ abs/1312.6114.   
［3］ KORSHUNOVA I，SHI W Z，DAMBRE J，et al. Fast faceswap using convolutional neural networks［DB/OL］.［2023- 02-02］. https：//openaccess. thecvf. com/content_ICCV_ 2017/papers/Korshunova_Fast_Face-Swap_Using_ICCV_ 2017_paper.pdf.   
［4］ NIRKIN Y，KELLER Y，HASSNER T. FSGAN：Subject agnostic face swapping and reenactmen［DB/OL］.［2023-02- 02］. https：//openaccess.thecvf.com/content_ICCV_2019/ papers/Nirkin_FSGAN_Subject_Agnostic_Face_Swapping_ and_Reenactment_ICCV_2019_paper.pdf.   
［5］ KOUJAN M R，DOUKAS M C，ROUSSOS A，et al. Head2Head：Video-based neural head synthesis［C］//2020 15th IEEE International Conference on Automatic Face and Gesture Recognition（FG 2020）. New York：IEEE Press， 2020：16-23. DOI：10.1109/FG47880.2020.00048.   
［6］ PRAJWAL K R，MUKHOPADHYAY R，NAMBOODIRI V P，et al. A lip sync expert is all you need for speech to lip generation in the wild［DB/OL］.［2023-02-02］. https：//dl. acm.org/doi/pdf/10.1145/3394171.3413532.   
［7］ PEROV I，GAO D H，CHERVONIY N，et al. Deep⁃ FaceLab：Integrated，flexible and extensible face-swapping framework［EB/OL］. 2020：arXiv：2005.05535. http：// arxiv.org/abs/2005.05535.   
［8］ NGUYEN H H，YAMAGISHI J，ECHIZEN I. Capsuleforensics：Using capsule networks to detect forged images and videos［C］//2019 IEEE International Conference on Acoustics，Speech and Signal Processing（ICASSP）. New York：IEEE Press，2019：2307-2311. DOI：10.1109/ ICASSP.2019.8682602.   
［9］ LI Y Z，LYU S W. Exposing DeepFake videos by detecting face warping artifacts［EB/OL］. 2018：arXiv：1811.00656. http：//arxiv.org/abs/1811.00656.   
［10］ RÖSSLER A，COZZOLINO D，VERDOLIVA L，et al. FaceForensics $^ { + + }$ ：Learning to detect manipulated facial images［DB/OL］.［2023-01-12］. https：//openaccess.thecvf. com/content_ICCV_2019/papers/Rossler_FaceForensics_ Learning_to_Detect_Manipulated_Facial_Images_ICCV_ 2019_paper.pdf.   
［11］ AFCHAR D， NOZICK V， YAMAGISHI J， et al. MesoNet：A compact facial video forgery detection network ［C］//2018 IEEE International Workshop on Information Forensics and Security（WIFS）. New York：IEEE Press， 2018：1-7. DOI：10.1109/WIFS.2018.8630761.   
［12］ CHUGH K，GUPTA P，DHALL A，et al. Not made for each other- audio-visual dissonance-based deepfake detec tion and localization［DB/OL］.［2023-02-03］. https：//dl. acm.org/doi/pdf/10.1145/3394171.3413700.   
［13］ CHENG H，GUO Y Y，WANG T Y，et al. Voice-face homogeneity tells deepfake［EB/OL］. 2022：arXiv：2203. 02195. http：//arxiv.org/abs/2203.02195.   
［14］ MITTAL T，BHATTACHARYA U，CHANDRA R，et al. Emotions don’t lie：An audio-visual deepfake detection method using affective cues［DB/OL］. ［2023-02-01］. https：//dl.acm.org/doi/pdf/10.1145/3394171.3413570.   
［15］ VASWANI A， SHAZEER N， PARMAR N， et al. Attention is all you need［DB/OL］.［2024-10-11］. https：// proceedings. neurips. cc/paper/2017/file/3f5ee243547dee9 1fbd053c1c4a845aa-Paper.pdf.   
［16］ XU P，ZHU X T，CLIFTON D A. Multimodal learning with transformers：A survey［J］. IEEE Transactions on Pattern Analysis and Machine Intelligence，2023，45（10）： 12113-12132. DOI：10.1109/TPAMI.2023.3275156.   
［17］ NAGRANI A，YANG S，ARNAB A，et al. Attention bottlenecks for multimodal fusion［EB/OL］. 2021：arXiv： 2107.00135. http：//arxiv.org/abs/2107.00135.   
［18］ NGUYEN T T，NGUYEN Q V H，NGUYEN D T，et al. Deep learning for deepfakes creation and detection：A survey［J］. Computer Vision and Image Understanding， 2022，223：103525. DOI：10.1016/j.cviu.2022.103525.   
［19］ LI L Z，BAO J M，YANG H，et al. Advancing high fidelity identity swapping for forgery detection［C］//2020 IEEE/ CVF Conference on Computer Vision and Pattern Recog⁃ nition（CVPR）. New York：IEEE Press，2020：5074-5083. DOI：10.1109/CVPR42600.2020.00512.   
［20］ JAMALUDIN A，CHUNG J S，ZISSERMAN A. You said that：Synthesising talking faces from audio［J］. Inter⁃ national Journal of Computer Vision，2019，127（11）： 1767-1779. DOI：10.1007/s11263-019-01150-y.   
［21］ ZHOU H，SUN Y S，WU W，et al. Pose-controllable 185 talking face generation by implicitly modularized audiovisual representation［C］//2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition （CVPR）. New York：IEEE Press，2021：4176-4186. DOI：10. 1109/CVPR46437.2021.00416.   
［22］ 孙鹏， 郎宇博， 巩家昌， 等. 拼接篡改伪造图像的色彩偏 移量不一致取证方法［J］. 计算机辅助设计与图形学学 报，2017，29（8）：1408-1415. SUN P，LANG Y B，GONG J C，et al. Authentication method for splicing manipulation with inconsistencies in color shift［J］. Journal of Computer-Aided Design & Computer Graphics，2017，29（8）：1408-1415（Ch）.   
［23］ 张怡暄， 李根， 曹纭， 等. 基于帧间差异的人脸篡改视频 检测方法［J］. 信息安全学报，2020，5（2）：49-72. DOI： 10.19363/J.cnki.cn10-1380/tn.2020.02.05. ZHANG Y X，LI G，CAO Y，et al. A method for detecting human-face-tampered videos based on interframe difference ［J］. Journal of Cyber Security，2020，5（2）：49-72. DOI： 10.19363/J.cnki.cn10-1380/tn.2020.02.05（Ch）.   
24］ MASI I，KILLEKAR A，MASCARENHAS R M，et al. Two-branch recurrent network for isolating deepfakes in videos［M］// Computer Vision - ECCV 2020. Cham： Springer International Publishing，2020：667-684. DOI： 10.1007/978-3-030-58571-6_39.   
［25］ WU X，XIE Z，GAO Y T，et al. SSTNet：Detecting manipulated faces through spatial，steganalysis and temporal features ［C］//2020 IEEE International Conference on Acoustics，Speech and Signal Processing（ICASSP）. New York：IEEE Press，2020：2952-2956. DOI：10.1109/ ICASSP40776.2020.9053969.   
［26］ QIAN Y Y，YIN G J，SHENG L，et al. Thinking in frequency：Face forgery detection by mining frequency-aware clues［C］//European Conference on Computer Vision. Cham：Springer，2020：86-103.10.1007/978-3-030-58610- 2_6.   
［27］ WANG S Y，WANG O，ZHANG R，et al. CNN-generated images are surprisingly easy to spot for now［DB/OL］.［2023- 01-05］. https：//openaccess. thecvf. com/content_CVPR_ 2020/papers/Wang_CNN-Generated_Images_Are_Surpri singly_Easy_to_Spot_for_Now_CVPR_2020_paper.pdf.   
［28］ ZHOU Y P，LIM S N. Joint audio-visual deepfake detection ［C］//2021 IEEE/CVF International Conference on Com ⁃ puter Vision （ICCV）. New York：IEEE Press，2021： 14780-14789. DOI：10.1109/ICCV48922.2021.01453.   
［29］ ILYAS H，JAVED A，MALIK K M. AVFakeNet：A unified end-to-end Dense Swin Transformer deep learning model for audio-visual  deepfakes detection［J］. Applied Soft Computing，2023，136：110124. DOI：10.1016/j. asoc.2023.110124.   
［30］ LIU Z，LIN Y T，CAO Y，et al. Swin Transformer： Hierarchical vision transformer using shifted windows ［C］//2021 IEEE/CVF International Conference on Com ⁃ puter Vision （ICCV）. New York：IEEE Press，2021： 9992-10002. DOI：10.1109/ICCV48922.2021.00986.   
［31］ CAI Z X，STEFANOV K，DHALL A，et al. Do you really mean that？ content driven audio-visual deepfake dataset and multimodal method for temporal forgery localization［C］// 2022 International Conference on Digital Image Computing： Techniques and Applications（DICTA）. New York：IEEE Press，2022：1-10. DOI：10.1109/DICTA56598.2022. 10034605.   
［32］ YANG W Y，ZHOU X Y，CHEN Z K，et al. AVoiD-DF： Audio-visual joint learning for detecting deepfake［J］. IEEE Transactions on Information Forensics and Security，2023， 18：2015-2029. DOI：10.1109/TIFS.2023.3262148.   
［33］ KAMACHI M，HILL H，LANDER K，et al. Putting the face to the voice：Matching identity across modality ［J］. Current Biology，2003，13（19）：1709-1714. DOI： 10.1016/j.cub.2003.09.005.   
［34］ NAGRANI A，ALBANIE S，ZISSERMAN A. Seeing voices and hearing faces：Cross-modal biometric matching ［C］//2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. New York：IEEE Press，2018： 8427-8436. DOI：10.1109/CVPR.2018.00879.   
［35］ WANG R，LIU X，CHEUNG Y M，et al. Learning discriminative joint embeddings for efficient face and voice association ［DB/OL］. ［2023-04-02］. https：//dl. acm. org/doi/pdf/10.1145/3397271.3401302.   
［36］ ZHU B Q，XU K L，WANG C J，et al. Unsupervised voice-face representation learning by cross-modal proto⁃ type contrast［DB/OL］. ［2023-03-12］. https：//arxiv. org/pdf/2204.14057.   
［37］ MALIK M， MALIK M K， MEHMOOD K， et al. Automatic speech recognition：A survey［J］. Multimedia Tools and Applications，2021，80（6）：9411-9457. DOI：10. 1007/s11042-020-10073-7.   
［38］ CHUNG J S，SENIOR A，VINYALS O，et al. Lip reading sentences in the wild［DB/OL］.［2023-03-14］. https：// openaccess.thecvf.com/content_cvpr_2017/papers/Chung_ Lip_Reading_Sentences_CVPR_2017_paper.pdf.   
［39］ CHUNG J S，ZISSERMAN A. Out of time：Automated lip sync in the wild［DB/OL］.［2023-01-23］. https：//link. springer.com/chapter/10.1007/978-3-319-54427-4_19.   
［40］ ZHOU H，LIU Y，LIU Z W，et al. Talking face generation by adversarially disentangled audio-visual representation［J］. Proceedings of the AAAI Conference on Artificial Intelli⁃ gence，2019，33（1）：9299-9306. DOI：10.1609/aaai.v33i01. 33019299.   
［41］ DOSOVITSKIY A，BEYER L，KOLESNIKOV A，et al. An image is worth 16x16 words：Transformers for image recognition at scale［EB/OL］. 2020：arXiv：2010.11929. http：//arxiv.org/abs/2010.11929.   
［42］ HE K M，FAN H Q，WU Y X，et al. Momentum con⁃ trast for unsupervised visual representation learning［DB/ OL］.［2023-02-06］. https：//openaccess.thecvf.com/con⁃ tent_CVPR_2020/papers/He_Momentum_Contrast_for_ Unsupervised_Visual_Representation_Learning_CVPR_2 020_paper.pdf.   
［43］ CHUNG J S，NAGRANI A，ZISSERMAN A. VoxCeleb2： Deep speaker recognition［DB/OL］.［2023-02-03］.https：// arxiv.org/pdf/1806.05622.   
［44］ AFOURAS T，CHUNG J S，SENIOR A，et al. Deep audio-visual speech recognition［J］. IEEE Transactions on Pattern Analysis and Machine Intelligence， 2022， 44 （12）：8717-8727. DOI：10.1109/TPAMI.2018.2889052.   
［45］ KHALID H，TARIQ S，KIM M，et al. FakeAVCeleb： A novel audio-video multimodal deepfake dataset［EB/ OL］. 2021： arXiv： 2108.05080. http：//arxiv. org/abs/ 2108.05080.   
［46］ KORSHUNOV P，MARCEL S，KORSHUNOV P，et al. DeepFakes：A new threat to face recognition？ assess⁃ ment and detection［EB/OL］. 2018：arXiv：1812.08685. http：//arxiv.org/abs/1812.08685.   
［47］ SANDERSON C，LOVELL B C. Multi-region probabilistic histograms for robust and scalable identity inference［C］// International Conference on Biometrics. Heidelberg： Springer，2009：199-208. DOI：10.1007/978-3-642-01793- 3_21.   
［48］ KINGMA D P，BA J，HAMMAD M M. Adam：A method for stochastic optimization［EB/OL］. 2014：arXiv：1412. 6980. http：//arxiv.org/abs/1412.6980.   
［49］ KHALID H，KIM M，TARIQ S，et al. Evaluation of an audio-video multimodal deepfake dataset using unimodal and multimodal detectors［DB/OL］.［2023-02-10］. https：//dl. acm.org/doi/pdf/10.1145/3476099.3484315.