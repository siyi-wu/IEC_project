计算机科学与探索   
Journal of Frontiers of Computer Science and Technology   
ISSN 1673-9418,CN 11-5602/TP

# 《计算机科学与探索》网络首发论文

题目：  
作者：  
网络首发日期：  
引用格式：基于预训练的人脸图像伪造算法识别模型  
丁博文，芦天亮，彭舒凡，王珑皓  
2025-06-16  
丁博文，芦天亮，彭舒凡，王珑皓．基于预训练的人脸图像伪造算法识别模型[J/OL]．计算机科学与探索.  
https://link.cnki.net/urlid/11.5602.tp.20250616.0907.002

![](images/2d7784edba6ec70f7a61248f9eb7ee375001992d021caf712b18596ede8583b2.jpg)

网络首发：在编辑部工作流程中，稿件从录用到出版要经历录用定稿、排版定稿、整期汇编定稿等阶段。录用定稿指内容已经确定，且通过同行评议、主编终审同意刊用的稿件。排版定稿指录用定稿按照期刊特定版式（包括网络呈现版式）排版后的稿件，可暂不确定出版年、卷、期和页码。整期汇编定稿指出版年、卷、期、页码均已确定的印刷或数字出版的整期汇编稿件。录用定稿网络首发稿件内容必须符合《出版管理条例》和《期刊出版管理规定》的有关规定；学术研究成果具有创新性、科学性和先进性，符合编辑部对刊文的录用要求，不存在学术不端行为及其他侵权行为；稿件内容应基本符合国家有关书刊编辑、出版的技术标准，正确使用和统一规范语言文字、符号、数字、外文字母、法定计量单位及地图标注等。为确保录用定稿网络首发的严肃性，录用定稿一经发布，不得修改论文题目、作者、机构名称和学术内容，只可基于编辑规范进行少量文字的修改。

出版确认：纸质期刊编辑部通过与《中国学术期刊（光盘版）》电子杂志社有限公司签约，在《中国学术期刊（网络版）》出版传播平台上创办与纸质期刊内容一致的网络版，以单篇或整期出版形式，在印刷出版之前刊发论文的录用定稿、排版定稿、整期汇编定稿。因为《中国学术期刊（网络版）》是国家新闻出版广电总局批准的网络连续型出版物（ISSN 2096-4188，CN 11-6037/Z），所以签约期刊的网络版上网络首发论文视为正式出版。

# 基于预训练的人脸图像伪造算法识别模型

丁博文，芦天亮+，彭舒凡，王珑皓中国人民公安大学 信息网络安全学院，北京 100038$^ +$ 通信作者 E-mail: lutianliang@ppsuc.edu.cn

摘要：随着深度学习的发展，人脸伪造手段逐渐增多，现有的深伪人脸检测模型多集中于真假二分类阶段，泛化性较差，难以适应如今不断迭代的伪造算法，并且对受到不同清晰度和噪声等干扰的人脸图像研究不够深入。因此，为解决上述问题，提出了一种基于预训练的多分类人脸图像伪造算法识别方法。具体而言，整个模型分为预训练和主体训练两大模块。在预训练阶段，设计了基于掩码策略改进的 PR-MAE(Perceptionand Reconstruction-MAE)，深入挖掘掩码特征来构造感知损失，增强模型全局的感知学习能力；构建了UDenseNet 训练网络，利用 U 型化的稠密块提升模型对细节的捕捉能力，以适应更加复杂的任务；加入多噪声融合模块 MNF(Multi-Noise Fusion)，以动态注入策略提高模型对多类噪声的抵抗能力，提升模型鲁棒性。在主体训练过程，引入 CLIP(Contrastive Language-Image Pretraining)补充学习模块，与预训练后的UDenseNet 进行对比学习，以进一步提升模型归类的准确率。实验结果表明，在最新的 MCFF(Multi-Classification Fake Faces)和 DF40 多分类深伪人脸数据集上准确率(Accuracy, ACC)指标达到 $8 9 . 1 2 \%$ 和 $9 0 . 2 5 \%$ 在低清晰度图像中 ACC 平均值达到 $8 7 . 2 2 \% ,$ 在各类噪声下检测率达到 $8 3 . 3 6 \%$ ，泛化性测试中指标最高也能达到 $90 \%$ 以上。

关键词：伪造算法识别；人脸伪造；深度学习文献标志码：A 中图分类号：TP 391.4

# Recognition Model of Face Image Forgery Algorithm Based on Pre-training

DING Bowen,  LU Tianliang+,  PENG Shufan,  WANG Longhao

College of Information Network Security, People's Public Security University of China, Beijing 100038, China

Abstract: With the development of deep learning, face forgery methods are gradually increasing. The existing deepfake face detection models are mostly concentrated on the true or false binary classification, with poor generalization, which is difficult to adapt to the current iterative forgery algorithms. What’s more, the researches on the face images with different sharpness and noise interference are not deep enough. Therefore, in order to solve the above problems, this paper proposes a low quality face image forging algorithm recognition method based on pre-training model. Specifically, the entire model is divided into two major modules: pre-training and main training.In the pre-training stage, Perception and Reconstruction-MAE (PR-MAE) based on mask strategy is designed to further excavate mask features to construct perceptual loss and enhance the global perceptual learning ability of the model. And the UDenseNet training network is constructed to improve the ability to capture details by U-shaped dense blocks, in order to adapt to more complex tasks. Meanwhile, the Multi-Noise Fusion (MNF) module is added to improve the model's resistance to multiple types of noise by dynamic injection strategy and enhance the robustness. In the main training process, CLIP supplementary learning module is introduced to perform the comparative learning with the pre-trained UDenseNet to improve the accuracy of the model. The experimental results show that the ACC index of this method can reach $8 9 . 1 2 \%$ and $9 0 . 2 5 \%$ on the latest MCFF and DF40 classification datasets, the average ACC index can reach $8 7 . 2 2 \%$ in the tests of the low definition images, $8 3 . 3 6 \%$ in the tests of the various kinds of noise and the index can reach more than $90 \%$ at the highest in the generalization test.

Key words: Forgery algorithm recognition; Face forgery; Deep learning

人脸深度伪造是一种基于深度学习对有关人脸的图像、视频、音频等进行编辑、篡改、生成或操纵的技术，俗称“换脸术”。近年来，随着科学技术的发展与信息化的普及，深伪技术的实现门槛逐渐降低，甚至出现了“Zao”、“Face App”和“Sora”等封装好的换脸应用，普通群众也可以使用“一键换脸”。尽管该技术在文娱产业方面有所应用，但因其仿真程度高、欺骗性强，易被不法分子用于财产诈骗、传播谣言、侵犯公民权益等违法犯罪活动中。因此，对人脸深度伪造图像的检测具有重要的研究意义。

传统的深伪检测手段主要在经典的数据集上进行训练测试实现真伪二分类的辨析，如 Li 等[1]在$\mathrm { F F ^ { + + } }$ (FaceForensics $+ + { \dot { } }$ ) 和 Celeb-DF 数 据 集 上 的AUC(Area Under the Curve)指标达到 $9 9 . 9 5 \%$ 和 $9 9 . 9 8 \%$ 但是泛化性和鲁棒性不足，跨数据集对比实验结果最高达到 $8 2 . 3 9 \% ^ { [ 2 ] }$ ，鲁棒性实验平均结果为 $8 9 . 9 1 \% ^ { [ 3 ] }$ ，对伪造图像的生成原理的分析不够深入，缺少伪造算法识别检测模型的相关研究，导致在新的伪造方法出现时无法及时辨析。为更好地适应如今多样化伪造手段，目前识别伪造算法的研究也开始逐渐增多[4,5]，在各类多分类数据集 $\mathrm { F F ^ { + + } }$ 、ForgeryNet 和 DF40 上测试的伪造算法多分类 ACC 最高为 $9 2 . 4 1 \% ^ { [ 6 ] }$ 、 $8 5 . 8 9 \% [ 6 ]$ 和$8 9 . 8 \% ^ { [ 7 ] }$ 。然而在实际场景中人脸图像更多是无标签状态，对模型的泛化能力要求较高 。此外，现实网络中的图像因为压缩、噪声、传输受损等原因导致大量干扰性人脸图像的产生，会很大程度上降低模型检测的准确性[9]。

因此，为解决上述问题，本文拟采用预训练模型来应对人脸图像的伪造算法识别任务，主要工作如下：

（1）提出了一种基于掩码策略改进的PR-MAE 方法。利用 MAE 掩码策略的随机遮蔽让模型关注数据的整体结构，减少对特定特征的依赖，提升泛化能力。同时，掩码策略增加了训练数据的多样性，使模型能更好地应对未见过的伪造手法。为进一步提升掩码遮蔽后的检测准确率，本文深挖掩码和原图之间特征关系，提出新的感知损失并融合重构损失以增强模型的归类效果。

（2）设计了一种基于 U 型化改进的 DenseNet 训练网络 UDenseNet。为适应多分类人脸图像检测的更加精细化复杂化的任务，本文以能够实现特征高效复用的稠密块为基础，利用进行 U 型化改进，在满足 PR-MAE 的图像尺寸要求下，减少参数数量，降低计算复杂度，并且能够保留低层特征来捕捉细节信息。

（3）设计了一个多噪声融合模块 MNF。为提升模型对噪声的抵抗能力，在预训练中引入多噪声融合模块 MNF，选取六种常见的图像噪声类型，使用动态注入策略，单类噪声的强度以线性方式稳定增长，多类噪声以方差权重进行融合，让模型在与训练阶段适应多变的噪声环境。

（4）设计了 CLIP 补充学习模块。在继承预训练完成的 UDenseNet 后，加入卷积神经网络结构，对提取的特征进行精细化处理，同时使用 CLIP 对比学习进行补充，利用向量差值计算对比损失微调模型的偏向，从而提升模型整体的检测准确率。

# 1  相关工作

# 1.1 深伪图像伪造算法识别技术

对深度伪造的人脸图像的伪造算法进行识别是在深伪检测的基石上的细化发展，不仅能够完成人脸图像真假二分类的检测，而且可以提供伪造方法的补充性信息，增强证据溯源能力，提升深伪鉴定的司法认可度。He[10]在建立庞大的人脸视频和图像数据集的基础上，进行了伪造分类和伪造定位任务，利用三种经典模型 Xception、GramNet、F3-Net 在 ForgeryNet 数据集上完成 16 种方法的深伪图像分类检测（真实和15 种伪造方法），平均准确率达到 $58 . 4 7 \%$ ，极大地推动了深伪图像检测的更加细化地发展。赵[6]等延续上述数据集并依托 $\mathrm { F F ^ { + + } }$ 数据集构建多分支系统，结合二分类和多分类两种检测文采用可动态扩展的增量学习框架来提升模型的检测准确率的同时，缩小资源占用和时间成本。Arshed 等[11]针对扩散模型和 GAN 伪造方法进行多分类检测（真实和 4 种伪造方法），评估视觉转换器(Vision Transformers, ViTs)在检测多类深度伪造图像方面的可行性，并与传统的基于 CNN 的模型进行比较分析，来应对不断更新的深伪技术。Guarnera 等[12]基于收集的 9 种不同 GAN 网络架构和4 种扩散模型生成的真实图像和虚假图像，提出了一种分层的多级检测思想，第一级将真实图像与 AI 生成的图像区分开，第二级区分了由 GAN 和 DM 生成的图像，第三级用于识别出生成合成数据的特定 GAN和 DM 架构，从而实现了较高的准确性和鲁棒性。Bhattacharyya 等[13]基于扩散模型伪造方法提出了一种动量难度提升策略，以应对训练数据异质性带来的额外困难，根据学习难度动态分配适当的样本权重，增强了模型对困难样本的适应性。深伪人脸图像的伪造算法识别技术在满足真假人脸分辨的条件下，依托更多更加全面的数据集完成更具有挑战性的分类任务。

# 1.2 预训练模型

深度学习中的预训练(Pre-training)是指在训练一个深度学习模型前，预先在一个较大的数据集上进行训练学习一些通用的特征表示，更好地泛化利用先前的知识，从而达到提高模型在相关任务上的性能，加快训练效率的效果，减少数据依赖提升模型的泛化能力。Khan[14]等评估了八种监督深度学习架构和两种基于 Transformer 的模型，并使用两种自监督策略(DINO和 CLIP)进行预训练，在四个不同的深度伪造检测基准进行测试，结果表明使用预训练后的模型准确率比监督学习效果更佳。Mouna[15]等使用 CIFAKE 数据集，在 三 个 预 训 练 的 卷 积神 经 网 络 模 型 (VGG16 、MobileNet 和 InceptionV4)上完成测试，验证了 CNN预训练模型在识别人工智能(Artificial Intelligence, AI)生成虚假图像的优越性。Tan[16]等分析了 CLIP 检测能力的底层机制，引入类别通用提示构建 C2P-CLIP 模型，将类别概念注入图像编码器中，从而增强检测性能，在不额外添加参数的条件下提升了检测的准确率。Khan[17]等继续探索了预训练视觉-语言模型(Vision

Language Models, VLMs)与最新适应方法结合在通用深度伪造检测中的有效性，与仅依赖 CLIP 的视觉部分而忽略其文本组件的先前研究不同，结合 CLIP 的图像和文本组件的迁移学习策略更具有优越性。预训练模型策略能够在下游任务资源受限的情况下提升整体的检测效率，同时也能保护人脸训练数据，维护视觉信息的完整性。

# 2  本文方法

# 2.1 整体结构

由于真实场景中的数据集大多处于无标签状态，伪造方法种类也不断增多，而且图像在网络传输过程中易受到尺寸、噪声等干扰，这些因素减弱了模型的性能，降低了模型检测的准确率[18,19]。而预训练模型能够提前完成对特征的大量学习，并具备良好的迁移能力，即使新任务数据受限也能表现出较好的效果，故而近年来被引入图像领域，应用于人脸图像检测相关研究[20,21]。为针对性地提升算法的抗干扰能力和泛化性，本文提出了一种基于预训练模型的人脸图像伪造算法识别模型。如图 1 所示，模型主要由预训练和主体训练两大部分组成。在预训练阶段，（1）在掩码策略的基础上改进形成 PR-MAE 方法，利用掩码特征构建感知损失来调和掩码图像和原始图像之间的重构损失，从而让模型更深入地挖掘特征，学习复原预测能力。（2）对训练网络升级构建 UDenseNet，基于稠密块将其进行 U 型化改进，利用稠密网络高效复用和梯度的直接传递的特点，来减少参数数量，降低计算复杂度，并且为保证图像输入与输出端的尺寸相同，使用 U 型网络进行解码。（3）为提升模型对噪声的抵抗能力，在预训练中引入多噪声融合模块 MNF，选取六种常见的图像噪声类型，使用动态注入策略，单噪声强度线性增长，多噪声权重融合。在主体训练阶段，使用双流结构，一方面，继承预训练完成的 UDenseNet，并引入系列卷积操作得到向量 Vector1；另一方面，使用即插即用 CLIP 对比学习得到向量 Vector2，利用两个向量计算对比损失后微调模型的偏向，从而提升模型整体的检测准确率。

![](images/e6fb105755dc1dff1809d94c6efbcb6e36ae088fd56057f258dc76c9d0d8be47.jpg)  
图 1  整体架构

# 2.2 基于掩码策略改进的 PR-MAE

MAE 掩码策略通过随机设置掩码能够提升模型的泛化能力，同时掩码恢复过程与低清晰度图像向原图的转化过程相似，有助于提升模型对模糊图片的学习。为适应人脸伪造算法识别检测的细化要求，更加深入地挖掘图像特征，本文提出基于掩码策略改进的PR-MAE 模块，利用 MAE 掩码策略作为预训练的重要部分，深挖掩码和原图之间特征关系，提出感知损失并与重构损失进行融合，提升模型的归类效果，具体过程如图 2 所示。

![](images/dd45c8d1ff0d8c519ddf55297d86d20305198d807d77b794edcb223a64396880.jpg)  
Fig.1  The whole structure   
图 2  PR-MAE 结构示意图  
Fig.2 PR-MAE structure

根据掩码比例参数和输入的图像大小等比例自适应确定需要掩码块的数量，并随机选择一些位置，通过遍历所有被选中的掩码块的位置，将对应位置元素设置为 0 表示该块被遮盖，从而生成一个掩码矩阵，达到遮盖图像部分内容的效果。掩码操作常用于训练深度学习模型时的数据增强，以提高模型的泛化能力。为扩展到整个全局图像，将掩码矩阵与一个形状大小为 patch 的全 1 矩阵进行克罗内克(Kronecker)积操作，使得掩码矩阵中的每个元素扩展成一个 patch 大小的块。最后将原始图像和张量转化后的 mask 进行逐元素乘法，形成掩码图像，如公式(1)。

$$
\phi \big ( r \big ) = r \odot T \big ( m \times P \big ( 1 \big ) \big )
$$

其中r 为掩码图形矩阵， $r$ 为原始图像， $T$ 表示Tensor 转化， $m$ 为随机掩码矩阵， $P ( 1 )$ 为形状大小为 patch 的全 1 矩阵。

在获得掩码图像后，原始的 MAE 策略仅将掩码图 像 与 原 始 图 像 进 行 比 较 得 到 的 重 构 损 失(Reconstruction Loss)来作为整个模型损失的判定根据整个过程侧重点在于被遮盖后剩余图像特征的学习。本文在此基础上加入遮盖部分的特征学习，利用中间过程中生成的掩码，深挖其中蕴含的特征信息，构建新的感知损失(Perceptual Loss)。抽取掩码中的掩码特征与原始人脸图像的原始特征，比较其差异性，增加掩码基于人脸图像本身的位置特征，提升图像对掩码的感知力，为模型检测提供更加丰富的特征信息。具体如公式(2)，通过原始特征和掩码特征进行对比学习获取这两组特征之间的相似度，并据此计算出感知损失，能够更细节地剖析图像之间的差异。将生成的掩码图像和原始人脸图像之间进行比较得到重构损失后，与赋值权重后的感知损失相加得到总损失，并让总损失值尽可能地小，具体过程如(3)(4)。最终的目标是即使在部分被遮盖的情况下，使得来自同一伪造类别图像的特征比来自不同类别图像的特征更加相似。

$$
\begin{array} { r } { L _ { r } = \theta \Big ( f _ { m a s k } , f _ { i m g } \Big ) } \end{array}
$$

$$
\begin{array} { r } { L _ { \scriptscriptstyle R } = \theta \big ( \phi ( r ) , r \big ) } \end{array}
$$

$$
L = L _ { \scriptscriptstyle R } + \lambda L _ { \scriptscriptstyle P }
$$

其中 $L _ { \scriptscriptstyle P }$ 为感知损失， $\theta$ 表示相似度计算， mask 为掩码特征， $f _ { i m g }$ 为原始图像特征， 为重构损失， r

为掩码图形矩阵，r 为原始图像，L 表示总损失，表示权重。

# 2.3 U 型化改进的 DenseNet 训练网络

MAE 原论文[22]中采取的是 ViT(Transfomer En-coder)结构作为主体网络，整体结构较为简单，为适应多分类人脸图像检测的更加精细化复杂化的任务，本文选取 U 型化改进后的 DenseNet 网络作为主体训练网络。DenseNet[23]是一种密集连接卷积神经网络，每一层都直接与后续层相连接，实现了特征的高效复用和梯度的直接传递，减少了参数数量，降低了计算复杂度。同时，为了满足掩码策略中输入和输出端图像大小的一致性，本文对进入稠密块后的图像尺寸进行修正，利用 UNet[24]的思想对图像放缩至原尺寸，具体结构如图 3 所示。在编码阶段使用稠密块，将每一层的输出与后续层相连，实现特征重用，每个块内部层与层之间密集连接，块与块之间通过卷积和池化操作连接以用于压缩特征图尺寸。这种密集连接方式能够有效缓解梯度消失问题，还能促进特征在网络中的传播，使得网络在减少参数量的同时能够学习到更丰富、更复杂的特征表示[25,26]。再通过上采样和卷积操作，层层递增放大，在解码的同时实现尺寸修正，以满足预训练的图像大小要求。最后，加入一个残差连接完成输出。

# 2.4  多噪声融合模块 MNF

现实场景中的人脸图像在传输过程会受到不同噪声的干扰[27]，降低了模型检测的准确率。为了提高预训练模型在噪声环境下的抵抗能力，本文提出了一种多噪声融合模块 MNF，旨在通过在预训练过程中引入多种噪声类型，增强模型的鲁棒性。研究者们针对预训练模型的鲁棒性进行了大量研究，主要包括数据增强、模型正则化、单噪声注入等方法[28,29]。然而，这些方法在提高模型鲁棒性方面存在单类噪声的局限性，本文提出的多融合噪声模块 MNF 旨在克服这些局限性，进一步提升预训练模型在噪声环境下的性能。

![](images/cec83124d0ec13be30780dc524100a74adcfd116955f0ed7882f1cd9dc8193bc.jpg)  
图 3  UDenseNet 结构示意图  
Fig.3  UDenseNet structure

噪声类型选择：根据不同应用场景，选择合适的噪声类型进行融合。本文依据图像研究中常见的噪声种类，分别选取了高斯噪声、伽马噪声、瑞利噪声、泊松噪声、乘性噪声、椒盐噪声等多种常见噪声类型。

噪声注入方法：噪声注入策略是指在预训练过程中，将多种噪声按照一定比例注入到原始图像中，使模型在学习过程中适应不同类型的噪声。传统的噪声注入策略通常是静态的，在训练过程中噪声类型和强度不会发生改变。动态噪声注入策略则是根据模型的训练进度和性能动态调整增加的噪声类型和强度，可以增强模型对输入变化的适应性，从而提高模型的稳定性。本文设计线性组合噪声动态注入，对于单个噪声采用线性强度增长，对于多种噪声使用权重组合策略。在预训练模型的初始阶段，注入较低强度的噪声，帮助模型快速适应噪声环境，随着训练的逐步深入，增加噪声的强度和复杂性，让模型学习更加复杂的特征，提升特征提取能力和抗干扰能力。对于加入的 $n$ 类不同的噪声，每种噪声 i 的线性增长如公式(5)(6)(7)。

其中 n 是第 $i$ 种噪声在第 $t$ 轮训练时的强度， $n _ { i _ { 0 } }$ 是第 $i$ 种噪声的初始强度， $n _ { i _ { \mathrm { m a x } } }$ 是第 $i$ 种噪声的最大强度， $t$ 是当前的训练轮数， $T$ 是总的训练轮数。

再将 $n$ 类噪声根据每种噪声权重 $\omega _ { i }$ 组合起来，形成组合噪声的总强度 N   ：

$$
N ( t ) = \sum _ { i = 1 } ^ { n } \omega _ { i } \cdot n _ { i } ( t )
$$

权重的选择基于每个噪声源的方差进行分配，方差小的噪声源具有更大的权重，具体如公式(7)。

$$
\omega _ { i } = \frac { 1 / \sigma _ { i } ^ { 2 } } { \displaystyle \sum _ { j = 1 } ^ { n } 1 / \sigma _ { j } ^ { 2 } }
$$

其中 $\sigma _ { i } ^ { 2 }$ 表示第 $i$ 个噪声源的方差， $\sigma _ { j } ^ { 2 }$ 表示第 $j$ 个噪声源的方差。

通过这种动态调整的过程，使模型不仅在干净的数据上表现出色，而且在面对现实环境中的含噪图像时，也能保持较高的准确性和鲁棒性。

# 2.5 CLIP 补充学习模块

预训练结束后的测试主体模型在继承 UDenseNet的基础上，进一步融入了对比学习机制，以提升图像分类性能。利用预训练PR-MAE 提取图像的深层特征，捕捉图像信息，并引入 CLIP 对比学习，通过构建正负样本对，强化模型对相似性和差异性的辨识能力[30]，从而增强特征的判别性，具体过程如图 4 所示。

训练评估的基础模型在 UDensesNet 的基础上，为适配整体需求，加入卷积神经网络结构，对提取的特征进行精细化处理，以更好地适应分类任务的需求。通过两次卷积和池化，特征图展平和线性映射，并使用 ReLU 作为激活函数来构造分类器，完成分类任务。此外，对比学习 CLIP 模型已经在大量图像-文本对上进行了预训练，学习到了丰富的跨模态知识[31,32]，本文利用其即插即用的特性应用到实验中，通过使用CLIP 可以充分利用这些预训练知识，为下游的图像分类任务，提供强大的表征能力。首先，利用已完成预训练的 CLIP 模型处理图像，通过 AutoProcessor 和CLIPVisionModelWithProjection 来提取图像的特征表示，同时为了适应模型输入使用双线性插值将图像尺寸调整为 $2 2 4 \mathrm { x } 2 2 4$ 。CLIP 在预训练后，可以直接进行zero-shot 推理，无需微调即可在新的任务上进行推理，拥有很好的迁移能力和泛化能力。其次，基于对比学习构造对比损失。在对特征向量进行归一化操作后，计算相似度矩阵，并据此计算正样本上的平均对数似然。因为要最大化正样本的对数似然，相当于最小化其负值，故将其负值作为损失 loss，最后对所有锚点的损失取平均，得到对比损失值。训练时，本文将传统的分类损失和对比损失加权求和，作为最终的损失函数来指导分类器进行权重调整。

图 4  CLIP 补充学习模块  
![](images/39ca0e56826190e485cf94e7a83451532c0017e82b8ecb855cc242b1ff54e758.jpg)  
Fig.4  CLIP supplementary learning module

# 3  实验与分析

# 3.1 实验设置与评价指标

本文的实验环境为 64 位 Linux 操作系统 Ubuntu16.04.6，GPU 为 NVIDIA A100-PCIE-40GB。代码语言为 python，版本为 3.9.12，深度学习模型在 Pytorch 框架下实现。使用 Adam 优化器用于更新模型参数，学习率设置为 0.001。在预处理阶段，对图像加入随机旋转转换和随机水平翻转转换来进行数据增强，旋转角度范围是-5 度到 5 度，使得模型能够处理不同方向的图像并学习到左右对称的特征[33]。

为评估模型对人脸图像的检测能力，本文选择经典的准确率(Accuracy)、检测精度(Precision)、召回率(Recall)、F1-score 作为实验的评价指标。此外，ROC曲线下面积 AUC 数值常被用作深伪人脸图像检测的二分类评估指标，但本文研究的是针对不同伪造算法进行识别，无法单独用 AUC 进行计算。因此，将各个伪造方法类别能否识别当作二分类，计算出相应的AUC，再将每个类别单独的 AUC 累计加和取平均，最终得到宏平均 Macro-AUC。相比于依据数量而进行权重加和，均值计算能够平等地考虑到所有类别，减弱数据不平衡所带来的影响。具体公式如(8)，其中n为分类任务所需的类别数， $i$ 为对应的类别。

$$
M a c r o - A U C = \frac { 1 } { n } \sum _ { i } ^ { n } A U C _ { i }
$$

# 3.2 数据集

随着人脸深度伪造检测领域的快速发展，为适应不同检测的多样化需求，除了真假两类的数据集外还逐渐产生了多分类数据集，如 ForgeryNet[10]、 $\mathrm { D F } 4 0 ^ { [ 7 ] }$ 、

MCFF[34]等。因为人脸图像伪造算法的识别需要依托于多分类数据集，而目前通用的 Celeb-DF[35]、DFDC[36]等检测数据集里面只有 Fake&Real 两种不同的标签，无法满足伪造算法识别研究的需求。下面两个新创建的数据集是可通过公开网络渠道获取的较高质量数据集，因此本文主要基于这两个数据集完成相应的实验测试和验证。

MCFF 数据集是从生成原理入手，基于经典的自编码器和 GAN 网络、新兴的扩散模型、大模型技术以及技术融合方法等，通过收集和复现等方法共整理272599 张真实人脸图像和 144425 张虚假人脸图像，包含19 种分类标签（18 种伪造方法和1 种真实标签）。其中真实图像来源于 $\mathrm { F F ^ { + + } } ^ { [ 3 7 ] }$ 和 CelebA[38]中的真实样本，虚假图像则通过收集整理和论文复现等方式进行制作，图像涵盖了种族、年龄、性别、表情、光照等丰富内容。

DF40 数据集是由北京大学和腾讯 Youtu 实验室合作开发于 2024 年正式发布，推动了多分类数据集革新性的发展。该数据集包含 40 种伪造方法，从面部交换、面部交换、整体合成、面部编辑四个维度构建了一个高度多样化和大规模的深度伪造检测数据集，将$\mathrm { F F ^ { + + } }$ 和 Celeb-DF 数据集作为原始数据并在此基础上应用各类伪造算法完成虚假图像的制作。

由于数据集提供的人脸图像都只有一种模式，图像质量状态并未发生改变，所以本文将降低原始数据集图像的清晰度或在其中加入不同种类参数各异的噪声，来更好地检测模型的抗干扰能力。

# 3.3 参数实验

本文先分别从两个数据集各抽取 5000 张人脸图像，针对掩码策略中的掩码比例进行参数实验。从0.1 到0.9，以 0.1 为单位依次进行遍历来探寻参数的最佳取值，其中参数值越小表示被遮挡的面积越少，反之则遮盖更多的面积。具体的实验结果如表 1，从中可以看到随着遮盖比例的增加，刚开始检测的准确率有得到一定的提升，这是因为适当的掩码可以提高模型的适应性，但当遮盖面积过大时则会遮盖住过多的信息，模型获取到的信息受到了较大的局限性，从而抑制了模型的学习能力。图5 更加直观体现出随着遮盖比例变化时准确率的变化浮动，从趋势来看，在本实验中遮盖比例取值为0.4时，达到模型检测率的峰值，故在预训练中设置掩码遮盖比例为 $40 \%$ 来完成后续实验。

表 1  不同遮盖比例下的准确率  
Table 1  ACC under different hiding ratios

图 5  遮盖比例与准确率的关系  
![](images/fa1d22dd5ee149b2d0e8631703db7180932f2646d6b56b9e6aeb5a054012f64f.jpg)  
Fig.5  The relationship between masking ratio and accuracy

# 3.4 数据集内的实验与分析

# 3.4.1 域内多分类实验

本文依托于 MCFF 和 DF40 两个数据集进行域内人脸图像伪造算法多分类实验，每类伪造方法抽取2000 张评估模型对各类伪造算法的识别情况。为体现模型的泛化性能力，在预训练学习过程中采用的是整体的混合无标签数据集，在主体训练时对 MCFF 和 DF40分别进行训练和检测，得到结果如图 6 和图 7 所示。整体而言，模型对DF40 的检测效果普遍较好，除了对DiT和 SiT 两种伪造图像检测的指标仅达到 $50 \%$ 左右，其余 17 种的识别能力都在 $90 \%$ 左右，Precision 最高可达$9 9 . 4 8 \%$ ，Recall 最高可达 $100 \%$ ，F1-score 最高可达$9 9 . 7 1 \%$ 。此外，模型也能完成对 MCFF 数据集的整体检测，过半数的伪造方法检测指标能够达到 $90 \%$ 左右，但 个 别 效 果 较 差 于 DF40 数 据 集 ， 对DenoisingDiffusionGAN、LamaDeep、ProjectedGAN、

StarGAN 四类伪造图像的检测指标较低，precision 值甚至低于 $50 \%$ 。除了体现出模型对域内各个伪造数据集的检测能力，表格也反映出 MCFF 数据集中的伪造图像分类难度较大，更具有检测的挑战性。

F1-scoreRecallPrecision StytleGAN StarGAN attGAN StableDiffusion ProjectedGAN SFHal 三 Palette NeuLaTadluape LM 一 Gansformer-FFHQ FaceSythetics 一 FaceSwap Face2Face   
Diffusion-StyleGAN2.-..   
DenoisingDiffusionGAN Deepfakes CIPS-FFHQ 0 20 40 60 80 100 数值(%)

![](images/fc0272a81a579f70764aea9e2a6eb6f3c1aa279fe29851ce8bec727197ed755a.jpg)  
图 6  模型对 MCFF 数据集的识别结果  
Fig.6  Recognition results of MCFF data set by the model   
图 7  模型对 DF40 数据集的识别结果  
Fig.7  Recognition results of DF40 data set by the model   
图 8  模型对 MCFF 检测的混淆矩阵  
Fig.8  The confusion matrix of the model for MCFF detection   
图 9  模型对 DF40 检测的混淆矩阵  
Fig.9 The confusion matrix of the model for DF40 detection 3.4.2 与主流模型的比较

为了更直观地评估模型对伪造算法的识别效果，本文采用混淆矩阵进行分析[39]，具体如图 8 和 9 所示。在两个矩阵中，颜色越深表示对应数值越大。从图中可以看出，对角线上的颜色明显较深，这表明模型在分类识别伪造算法方面表现出色，具有较高的准确性和可靠性。但仍有部分类别的非对角线地方颜色较为明显，说明对该类别的检测存在一定的上升空间。

Normalized Confusion Matrix CLIPS-FFHQ 1.0 Damoxing Deepfakes DenoisingDiffusionGAN 0.8 Diffusion-StyleGAN2-FFHQ+ADA Face2Face FaceSyteewtics 0.6 Gansformer-FFHQ LamaDeep NeurualTextures 0.4 Palette ProjectedGAN SFHQ StableDiffusion 0.2 StarGAN StyleGAN attGAN 0.0 AAA SGGG U 0 G L

Normalized ConfusionMatrix 1.0 CollabDiff DiT MRAA Midjourney 0.8 SiT VQGAN blendface ddim 0.6   
Gr facedancer   
facevid2vid fsgan 0.4 hyperreenact inswap pirender 0.2 sadtalker simswap tpsm uniface Pj 0.0 PredictedLabel

为更好地验证模型的检测效果，本实验选取 6 种深度学习图像检测领域的经典算法，以及 3 种多分类图像识别的先进模型与本文的模型进行对比。将 10 种模型算法分别在 MCFF 和 DF40 数据集上进行测试，用 ACC 和 Macro-AUC 作为检测指标，具体的实验结果如表 2 所示。通过比较可以看出本文的模型具备更加优越的准确率，在MCFF 上的ACC 值达到 $8 9 . 1 2 \%$ ，比次高的模型提高 $1 . 4 \%$ ，在 DF40 上的 ACC 值达到$9 0 . 2 5 \%$ ，相比提高 $1 . 5 3 \%$ 。再从评估正负样本排序能力的 Macro-AUC 指标来看，在两个数据集的检测结果数值上分别是 $9 9 . 5 6 \%$ 和 $9 9 . 6 1 \%$ ，对比其他模型均有$0 . 0 1 \%$ 到 $0 . 2 5 \%$ 的提高。

表 2  模型比较Table 2  Model comparison  

<html><body><table><tr><td rowspan="2">数据来源</td><td colspan="2">MCFF</td><td colspan="2">DF40</td></tr><tr><td>ACC(%)</td><td>MUCr%)</td><td>ACC(%)</td><td>Aucr%</td></tr><tr><td>CNN</td><td>69.62</td><td>96.04</td><td>79.55</td><td>97.58</td></tr><tr><td>Vgg19</td><td>72.98</td><td>98.16</td><td>88.72</td><td>99.20</td></tr><tr><td>Efficientnet</td><td>28.27</td><td>82.97</td><td>39.06</td><td>88.36</td></tr><tr><td>Resnet50</td><td>83.36</td><td>99.12</td><td>70.96</td><td>96.02</td></tr><tr><td>ViT</td><td>84.65</td><td>99.30</td><td>86.61</td><td>99.21</td></tr><tr><td>Xception</td><td>46.18</td><td>92.19</td><td>54.71</td><td>94.27</td></tr><tr><td>F3- Net[40,41]</td><td>81.90</td><td>98.21</td><td>83.50</td><td>98.28</td></tr><tr><td>For- geryNet[10]</td><td>87.72</td><td>99.55</td><td>87.89</td><td>99.34</td></tr><tr><td>Zhao[6]</td><td>74.84</td><td>95.83</td><td>76.86</td><td>98.24</td></tr><tr><td>Our net</td><td>89.12</td><td>99.56</td><td>90.25</td><td>99.61</td></tr></table></body></html>

# 3.5 鲁棒性测试

通过对 MCFF 和 DF40 的合并数据集中约 3.8 万

张人脸图像降低清晰度、添加各类噪声等方式构造干扰性人脸图像，模拟现实场景，来完成对模型的鲁棒性测试。

# 3.5.1 针对图像清晰度的测试

图像清晰度主要与像素大小和颜色明暗有关，而在真实场景中图像常以缩略图、模糊图、压缩图等形式进行传播。因此，本文为更好地模拟现实生活中人脸图像清晰度的测试，采取图像压缩(JPEG/PNG Com-pression)、图像剪裁(Resized)、色彩对比(Color contrast)、色彩饱和(Color saturation)四个手段对人脸图像进行处理。实验中选取 MCFF 和 DF40 中的部分数据进行测试，并在预训练阶段采取混合去标签数据，更贴合实际网络的无标注人脸图像。

表 3  模型在不同清晰度下的检测效果  
Table 3  The detection effect of the model under different resolution   

<html><body><table><tr><td>Quality reduction method</td><td colspan="5">Parameter value and Accuracy(%)</td><td>Average (%)</td></tr><tr><td>JPEG/PNG Compres- sion</td><td>10/9</td><td>30/7 50/5</td><td>70/3</td><td>90/1</td><td>100/0</td><td></td></tr><tr><td rowspan="2">Resized</td><td>77.21</td><td>84.85</td><td>85.10 50</td><td>88.67</td><td>91.25 91.64</td><td>86.45</td></tr><tr><td>10</td><td>30</td><td>70</td><td>90</td><td>100</td><td></td></tr><tr><td rowspan="2">Color Contrast</td><td>84.34</td><td>87.60 90.41</td><td>89.85</td><td>90.19</td><td>91.64</td><td>89.01</td></tr><tr><td>0.1</td><td>0.3</td><td>0.5 0.7</td><td>0.9</td><td>1</td><td></td></tr><tr><td rowspan="2"></td><td>81.85</td><td>83.84 84.34</td><td>88.92</td><td>90.40</td><td>91.64</td><td>86.83</td></tr><tr><td>0.1</td><td>0.3</td><td>0.5 0.7</td><td>0.9</td><td>1</td><td></td></tr><tr><td>Color Saturation</td><td>81.99</td><td>83.79</td><td>84.06</td><td>86.93</td><td>91.20 91.64</td><td>86.60</td></tr></table></body></html>

模型在不同清晰度下的检测实验结果如表 3 所示，数据显示在不对数据进行任何预处理的情况下，模型的整体检测准确率为 $91 . 6 4 \%$ 。在图像压缩方面，少量压缩使准确率微降至 $8 8 . 6 7 \%$ ，压缩数值达到 30/7 时，准确率降至 $84 . 8 5 \%$ ，而大量压缩则使准确率下降至

$7 7 . 2 1 \%$ ，虽无法满足高精度检测需求，但仍能大致识别人脸伪造。图像剪裁至 $30 \%$ 尺寸比例时，准确率仍高达 $8 7 . 6 0 \%$ ，最小尺寸时仅下降 $7 . 3 0 \%$ 。色彩对比参数在 1 到 0.7 间时结果稳定，0.5 到 0.1 间恢复平稳。色彩饱和度在 0.9 到 0.7 间微小变化时检测能力大幅下降，后稳定在 $83 \%$ 。总体而言，模型在人脸图像清晰度鲁棒性检测上表现稳定，清晰度下降时准确率降幅基本控制在 $10 \%$ 内，显示出模型在极端条件下的良好鲁棒性。

# 3.5.2 针对噪声图像的测试

伪造图像在制作和传播过程中会不可避免地受到噪声干扰，不同的制作方式和传播路径会导致图像上增加的噪声类别也各有不同。大多数的深伪图像论文仅将噪声干扰性检测作为小实验而进行，对视觉噪声的种类选取不够多元，和参数设置范围不够精细。因此，本文在 MCFF 和 DF40 混合实验数据集中加入高斯噪声(Gaussian noise)、伽马噪声(gamma noise)、瑞利噪声(Rayleigh noise)、泊松噪声(Poisson noise)、乘性噪声(Multiplicative Noise)、椒盐噪声(Salt & Peppernoise)等 6 种单类噪声和 5 种混合噪声进行模型的鲁棒性测试。

表 4  模型在不同类别噪声和强度下的检测效果  
Table 4  The detection effect of the model under different types of noise and intensity   

<html><body><table><tr><td colspan="2">Noise Type</td><td colspan="3">Parameter value and Accuracy(%) > 15 20 25</td><td colspan="2">Average (%)</td></tr><tr><td>Gaussian noise</td><td colspan="4">5 10 91.59 90.15</td><td>30</td><td colspan="2">87.45</td></tr><tr><td></td><td colspan="4">89.90 Alpha=0.5</td><td>84.18 82.49 Alpha=2</td><td></td><td></td></tr><tr><td>gamma</td><td colspan="4">1 5 10 一 90.94 87.21 85.02 90.24</td><td>5 86.70</td><td>10 83.67</td><td>87.30</td></tr><tr><td>Rayleigh noise</td><td colspan="4">5 10 15 87.71 84.85 84.68</td><td>25 83.00</td><td>30 81.82</td><td>84.29</td></tr><tr><td>Poisson noise</td><td colspan="4">λ=average brightness 87.40</td><td></td><td></td><td>87.40</td></tr><tr><td>Multiplicative Noise</td><td colspan="5">0.1 0.3 0.5 0.7 0.9</td><td>1</td><td></td></tr><tr><td></td><td colspan="4">80.81 85.35 87.21 88.72 0.1</td><td>89.73</td><td>91.64</td><td>87.24</td></tr><tr><td rowspan="2">salt & pepper</td><td colspan="4">0.01 0.03 0.05 83.50</td><td>0.15</td><td>0.2</td><td></td></tr><tr><td colspan="4">84.68 83.84</td><td>77.95 59.82</td><td>9.09</td><td>66.48</td></tr></table></body></html>

从表 4 所呈现的单类噪声干扰测试结果中可以观察到，除了椒盐噪声在极端条件下导致检测准确率出现断崖式下降外，其余五类噪声干扰实验均能够将检测准确率维持在 $80 \%$ 以上，展现出模型在噪声干扰下仍具备稳定的较高检测能力。具体来看，在高斯噪声干扰条件下，模型在中等干扰强度下仍保持 $89 . 9 0 \%$ 的较高准确率，仅下降 $1 . 7 4 \%$ ；随着噪声强度增加，准确率虽有所下降，但每间隔强度增加导致的准确率下降幅度控制在 $2 \%$ 以内。在伽马噪声干扰方面，当Alpha值为 0.5 时，准确率间隔下降幅度分别为 $3 . 2 3 \%$ 和

$2 . 1 9 \%$ ，表现优于 Alpha 值为 2 时的 $2 . 7 4 \%$ 和 $3 . 0 3 \%$ 下降，表明模型对伽马噪声具有较强的抵抗性。面对瑞利噪声干扰，模型在噪声强度初始增加时准确率出现一定下降，但在参数 10 至 25 区间内展现出显著的稳定性。在泊松噪声干扰下，当参数取平均亮度时，检测准确率仍高达 $87 . 4 0 \%$ ，凸显了模型对泊松噪声的优异抗干扰能力。对于乘性噪声干扰，模型在噪声干扰中期仍能保持 $8 7 . 2 1 \%$ 的检测率，随后虽有所下降，但最终准确率仍控制在 $80 \%$ 以上的水平。然而，在椒盐噪声干扰条件下，模型在噪声添加初期即出现 $6 . 9 6 \%$ 的较大幅度衰减，后在 0.01 到 0.1 参数区间内保持约$80 \%$ 的相对稳定准确率；但当面临极端强烈的噪声干扰时，准确率发生断崖式下降，此时图像已遭受严重污染，导致模型无法有效识别。综上所述，该模型在多种噪声干扰下均表现出较强的鲁棒性和稳定性，仅在极端椒盐噪声干扰下出现显著性能下降。

在多类噪声融合测试中，我们采用了基于常见频率情况的逐步融合方案，即以高斯噪声为起点，逐步融入泊松噪声、中等强度的乘性噪声、椒盐噪声、伽马噪声和瑞利噪声。从表 5 所展示的融合噪声测试结果可以看出，随着噪声种类的增加，检测准确率呈现递减趋势，且下降幅度逐渐扩大，这一现象证实了多种噪声对模型产生了显著的干扰效应。尽管如此，整体准确率仍基本控制在 $80 \%$ 左右。在融合少量噪声种类的情况下，模型展现出较强的抗干扰能力，准确率仍保持在 $8 7 . 0 4 \%$ 和 $8 4 . 3 4 \%$ 的较高水平。然而，当面临多类噪声融合的复杂场景时，准确率下降至 $7 6 . 7 7 \%$ 和$7 4 . 7 5 \%$ ，表明模型的检测能力虽有所衰减，但依然能够满足对虚假人脸伪造算法的识别需求，显示出模型在复杂噪声环境下的稳健性能。

表 5  融合噪声检测Table 5  Fusion noise detection  

<html><body><table><tr><td colspan="6">Noise 7</td><td rowspan="2">ACC (%)</td></tr><tr><td>Gaussian noise</td><td>Poisson noise</td><td>Multiplicative noise</td><td>Salt & Pepper noise</td><td>Gamma noise</td><td>Rayleigh noise</td></tr><tr><td>15</td><td>1</td><td></td><td></td><td>一</td><td>1</td><td>89.90</td></tr><tr><td>15</td><td>√</td><td></td><td></td><td></td><td>1</td><td>87.04</td></tr><tr><td>15</td><td>√</td><td>0.5</td><td></td><td></td><td>1</td><td>84.34</td></tr><tr><td>15</td><td>√</td><td>0.5</td><td>0.05</td><td></td><td></td><td>80.13</td></tr><tr><td>15</td><td>√</td><td>0.5 0.05</td><td></td><td>5</td><td>1</td><td>76.77</td></tr><tr><td>15</td><td></td><td>0.5</td><td>0.05</td><td>5</td><td>15</td><td>74.75</td></tr></table></body></html>

# 3.6 泛化性测试

本文进一步探讨了模型的泛化能力，旨在评估其在面对未知伪造方法和数据集时的检测性能。为此，选取了四种深伪图像检测领域常用的算法结构与本文模型进行对比实验，并选择了三种多分类数据集和三种常用的深伪检测二分类数据集作为实验数据。实验设计中，使用其中一个数据集进行模型训练，并在其余两个数据集上进行测试以评估泛化性能。真假分类任务的跨数据集测试实验结果如表 6 所示，模型在DF(Deepfake Detection) 和 DFDC(Deepfake DetectionChallenge)训练后进行的泛化性测试结果均有所提升，相比次优数据提升了约 $3 \% - 7 \%$ ，在 Celeb-DF 上训练后对 DF 的测试效果有小浮动的下降，但从整体效果来看，本文所提出的模型更优。

多分类任务的泛化性测试实验结果如表 7 所示，本文所提出的模型在 MCFF 数据集上训练后，在 $\mathrm { F F ^ { + + } }$ 和 DF40 数据集上分别达到了 $9 4 . 5 1 \%$ 和 $8 8 . 0 4 \%$ 的较高泛化性能；同样，在 DF40 数据集上训练后，模型在 MCFF 和 $\mathrm { F F ^ { + + } }$ 数据集上也展现了 $9 0 . 1 8 \%$ 和 $8 5 . 9 7 \%$ 的优异泛化性。然而，值得注意的是，在 $\mathrm { F F ^ { + + } }$ 数据集上训练后的效果相对不佳，这主要与 $\mathrm { F F ^ { + + } }$ 数据集本身的特性有关，因其仅包含 4 种伪造方法，而 MCFF 和DF40 数据集的伪造方法种类是其 4 倍，导致识别难度增加。由此可以推测，从低维伪造算法向高维伪造算法的预测较为困难，而由高维向低维或同等级维度的预测则相对容易。因此，为了提高模型对伪造方法变化的适应性和泛化能力，未来应更多地收集各类伪造方法以充实数据集，从而提升模型的全面检测性能。

表 6  跨数据集测试(二分类)  
Table 6  Cross-dataset testing (binary classification)   
表 7  泛化性测试结果(多分类)  

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">Trainset</td><td colspan="2">Testset</td><td rowspan="2">Trainset</td><td colspan="2">Testset</td><td rowspan="2">Trainset</td><td colspan="2">Testset</td></tr><tr><td>Celeb-DF</td><td>DFDC</td><td>DF</td><td></td><td>Celeb-DF</td><td>DF</td></tr><tr><td>Xception</td><td rowspan="5"></td><td>48.31</td><td>46.25</td><td rowspan="5"></td><td>55.29</td><td>50.81</td><td></td><td>53.44</td><td>49.08</td></tr><tr><td>ViT</td><td>67.83</td><td>68.78</td><td>70.32</td><td>62.37</td><td></td><td>66.15</td><td>60.97</td></tr><tr><td>F3-net[40,41]</td><td>71.42</td><td>69.24</td><td>76.65</td><td>70.76</td><td>DFDC</td><td>70.32</td><td>72.56</td></tr><tr><td>Chen[42]</td><td>78.6</td><td>75.6</td><td>90.0</td><td>67.7</td><td></td><td>74.1</td><td>77.8</td></tr><tr><td>Our net</td><td>83.33</td><td>82.58</td><td>86.62</td><td>77.27</td><td></td><td>79.74</td><td>80.81</td></tr></table></body></html>

Table 7  Generalized test resultsGeneralization test results (multiple classifications)   

<html><body><table><tr><td rowspan="2">Method</td><td rowspan="2">Trainset</td><td colspan="4">Testset</td><td rowspan="2"></td><td colspan="2">Testset</td></tr><tr><td>MCFF</td><td>Trainset DF40</td><td>FF++</td><td>Trainset DF40</td><td>FF++</td><td>MCFF</td></tr><tr><td>Xception</td><td></td><td>38.89 42.32</td><td></td><td>72.78</td><td>68.69</td><td></td><td>70.96</td><td>63.89</td></tr><tr><td>ViT</td><td></td><td>51.05</td><td>55.16</td><td>84.85</td><td>74.65</td><td></td><td>83.33</td><td>70.71</td></tr><tr><td>F3-net[40,41]</td><td>FF++</td><td>58.59</td><td>60.69 MCFF</td><td>89.46</td><td>81.31</td><td>DF40</td><td>84.09</td><td>78.79</td></tr><tr><td>Forgerynet[10]</td><td></td><td>66.41 70.45</td><td></td><td>88.11</td><td>82.58</td><td></td><td>86.11</td><td>80.05</td></tr><tr><td>Our net</td><td></td><td>72.51 76.77</td><td></td><td>94.51</td><td>88.04</td><td></td><td>90.18</td><td>85.97</td></tr></table></body></html>

# 3.7 消融实验

为验证每个模块对模型抗干扰能力的提升，通过拆解消除各模块来完成消融实验，结果如表 8 所示（本小节的 PR-MAE 特指感知机制的增加部分，以单独体现出其贡献）。当去掉上游的预训练任务时模型的检测率下降了 $1 4 . 0 3 \%$ ，体现出预训练阶段对整体提升泛化性和抗干扰性有着较大的帮助，可以在正式识别前加强模型的对伪造算法的感知力。当仅去掉预训练阶段中的感知机制和噪声 MNF 模块，准确率降至 $8 6 . 3 6 \%$ 和 $8 5 . 9 8 \%$ ，说明对图像噪声和干扰的提前学习增强了对现实场景中受到噪声污染的抵抗能力。当舍弃UDenseNet，还原 ViT 结构时，准确率降至 $84 . 8 4 \%$ ，体现出网络结构创新的意义。当仅舍去对比学习模块时，准确率减少了 $4 . 6 2 \%$ ，说明对比学习模块能够进一步增强模型的细节学习能力。若是只增加掩码策略中的感知机制、UDenseNet 或是 MNF 模块，模型的性能会分别下降至 $80 . 4 5 \%$ 、 $8 2 . 7 8 \%$ 、 $8 1 . 4 8 \%$ ，下降幅度小于 CLIP 补充学习模块，表现出三个结构对整体模型识别能力的提升均具有一定的贡献性，其中UDenseNet 网络结构的提升贡献度略优。

# 表 8  消融实验结果

Table 8  Results of ablation experiment   

<html><body><table><tr><td colspan="4">Modules</td><td rowspan="2">ACC (%)</td></tr><tr><td>PR-MAE(感知机制)</td><td>UDenseNet</td><td>MNF</td><td>CLIP 补充学习模块</td></tr><tr><td>1</td><td>√</td><td>√</td><td>√</td><td>86.36</td></tr><tr><td>√</td><td></td><td>√</td><td>√</td><td>84.83</td></tr><tr><td>√</td><td>√</td><td>1</td><td>√</td><td>85.98</td></tr><tr><td>√</td><td>√</td><td>√</td><td></td><td>87.02</td></tr><tr><td>√</td><td>√</td><td>√</td><td></td><td>91.64</td></tr><tr><td>√</td><td></td><td></td><td></td><td>80.45</td></tr><tr><td></td><td>√</td><td>一</td><td></td><td>82.78</td></tr><tr><td>1</td><td></td><td></td><td></td><td>81.48</td></tr><tr><td>一</td><td></td><td>士</td><td>√</td><td>77.61</td></tr></table></body></html>

![](images/8ae238ba3355e6f15ab0ea6b60bb5bba759193bacb1a50c1b94ee39081665bfa.jpg)  
图 10  t-sne 示意图  
Fig.10  Schematic diagram of t-SNE

此 外 ， 本 文 利 用 t-SNE(T-Distributed StochasticNeighbor Embedding)对分类过程进行可视化展示，以便更加清晰地体现出各模块对分类效果的提升，具体t-SNE 示意图如图 10 所示。蓝色的分类效果是原先的MAE 和 ViT 结构的基础分类，仅分出一部分类别，右侧大部分未形成聚类效果。黄色的图片展现的是将ViT 结构替换成 UDenseNet 之后的效果，可以看出大部分已完成分类，但左侧仍有小部分点混在一起。绿色的图片加入了 PR-MAE 的感知机制部分，各类别更加紧凑，左侧也逐渐呈现聚类趋势。橙色图片在绿色的基础上加入了 MNF 模块，各类别在各自紧凑的同时类与类之间开始分离。红色图片是本文模型的整体效果，左下部分的橙色、蓝色、红色三类更加分离，聚类之间的区愈加明显。

# 4  结束语

为解决深伪人脸伪造算法识别方法面对低质量人脸图像时在抵抗性和泛化性差等问题，本文设计了一种基于预训练模型的人脸伪造算法识别模型，有效提升了检测效能。模型主要分为预训练和主体训练两部分：在预训练阶段，对掩码策略进行改进形成 PR-MAE 方法，进一步挖掘掩码特征，构造感知损失，从而提升模型对整体图像的学习能力；训练网络采取 U 型化改进的稠密块，构建 UDenseNet 网络模型完成对图像的学习训练和复原预测，增强了模型对残缺图像的细节捕捉；添加多噪声融合模块 MNF，提出以动态注入的方式，提高模型对噪声图像的学习和适应，进而提升模型的抵抗能力。在主体训练阶段，继承完成预训练的 UDenseNet后通过卷积操作提取特征，并与即插即用 CLIP 补充模块进行对比学习，以提高模型识别的准确率。

本文所提出的方法在域内的多分类识别精度有了进一步的提升，在面对不同清晰度和各类噪声干扰下的低质量人脸图象是仍保持较高的准确率，并且在跨库检测的泛化性方面较现有的方法有更好的提升。但是模型离高精度的标准仍有一定的距离，并且预训练方法在时间成本能耗上有所消耗，不利于实时的快速检测。未来， 将对如何不牺牲模型性能的条件下进行模型优化，来适应实际的市场应用需求。

# 参考文献：

[1] LI X, ZHOU H, ZHAO M. Transformer-based cascade networks with spatial and channel reconstruction convolution for deepfake detection[J]. Math. Biosci. Eng, 2024, 21(3): 4142-4164.   
[2] CAO J, MA C, YAO T, et al. End-to-end reconstructionclassification learning for face forgery detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 4113-4122.   
[3] LAI Y, YU Z, YANG J, et al. Gm-df: generalized multiscenario deepfake detection[J]. arXiv preprint arXiv: 2406.20078, 2024.   
[4] XIE H, LIU J, CHEN Z, et al. A multi-attack adaptive fake face detection network based on global feature normalization[J]. Electronics, 2024, 13(23): 4615.   
[5] NGUYEN H H, FANG F, YAMAGISHI J, et al. Multi-task learning for detecting and segmenting manipulated facial images and videos[C]//2019 IEEE 10th International Conference on Biometrics Theory, Applications and Systems (BTAS). IEEE, 2019: 1-8.   
[6] 赵泽军，范振峰，丁博，等.基于增量学习的深度人脸 伪造检测[J].数据与计算发展前沿,2023,5(06):42-57. ZHAO Z J, FAN Z F, DING B, et al. Deepfake detection based on incremental learning[J].Frontiers of Data & Computing,2023,5(06):42-57.   
[7] YAN Z, YAO T, CHEN S, et al. Df40: toward next- generation deepfake detection[J]. arXiv preprint arXiv: 2406.13495, 2024.   
[8] YAN Z, LUO Y, LYU S, et al. Transcending forgery specificity with latent space augmentation for generalizable deepfake detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024: 8984-8994.   
[9] LE B M, WOO S S. Quality-agnostic deepfake detection with intra-model collaborative learning[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 22378-22389.   
[10] HE Y, GAN B, CHEN S, et al. Forgerynet: a versatile benchmark for comprehensive forgery analysis[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 4360-4369   
[11] ARSHED M A, MUMTAZ S, IBRAHIM M, et al. Multiclass AI-generated deepfake face detection using patchwise deep learning model[J]. Computers, 2024, 13(1): 31.   
[12] GUARNERA L, GIUDICE O, BATTIATO S. Mastering deepfake detection: a cutting-edge approach to distinguish gan and diffusion-model images[J]. ACM Transactions on Multimedia Computing, Communications and Applications, 2024,20(11):1-24.   
[13] BHATTACHARYYA C, WANG H, ZHANG F, et al. Diffusion deepfake[J]. arXiv preprint arXiv:2404.01579, 2024.   
[14] Khan S A, Dang-Nguyen D T. Deepfake detection: analyzing model generalization across architectures, datasets, and pre-training paradigms[J]. IEEE Access, 2023, 12: 1880-1908.   
[15] MOUNA L E L, TOURAD M C, NANNE M F, et al. AIgenerated fake image detection using pre-trained CNN models[C]//International Conference on Artificial Intelligence and its Applications in the Age of Digital Transformation. Cham: Springer Nature Switzerland, 2024: 207-219.   
[16] TAN C, TAO R, LIU H, et al. C2P-CLIP: injecting category common prompt in CLIP to enhance generaliza- tion in deepfake detection[J]. arXiv preprint arXiv:2408. 09647, 2024.   
[17] KHAN S A, DANG-NGUYEN D T. CLIPping the deception: adapting vision-language models for universal deepfake detection[C]//Proceedings of the 2024 International Conference on Multimedia Retrieval. 2024: 1006-1015.   
[18] QIAO T, XIE S, CHEN Y, et al. Fully unsupervised deepfake video detection via enhanced contrastive learning[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024,46(7):4654-4668.   
[19] WASEEM S, ABU S A R B S, AHMED B A. Attentionguided supervised contrastive learning for deepfake detection[C]//2024 IEEE 8th International Conference on Signal and Image Processing Applications (ICSIPA). IEEE, 2024: 1-6.   
[20] DING N, TANG Y, FU Z, et al. GPT4IMAGE: can large pre-trained models help vision models on perception tasks?[J]. arXiv preprint arXiv:2306.00693, 2023.   
[21] MA X, WEI Z, JIN Y, et al. Masked pre-training enables universal zero-shot denoiser[C]//The Thirty-eighth Annual Conference on Neural Information Processing Systems. 2024.   
[22] HE K, CHEN X, XIE S, et al. Masked autoencoders are scalable vision learners[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 16000-16009.   
[23] HUANG G, LIU Z, Van Der Maaten L, et al. Densely connected convolutional networks[C]//Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2017: 4700-4708.   
[24] RONNEBERGER O, Fischer P, Brox T. U-net: convolutional networks for biomedical image segmentation[C]// Medical Image Computing and Computer-assisted Intervention–MICCAI 2015: 18th International Con- ference, Munich, Germany, October 5-9, 2015, pro- ceedings, part III 18. Springer international publishing, 2015: 234-241.   
[25] HE Q, PENG C, LIU D, et al. GazeForensics: DeepFake detection via gaze-guided spatial inconsistency learning[J]. Neural Networks, 2024, 180: 106636.   
[26] TIAN C, LUO Z, SHI G, et al. Frequency-aware attentional feature fusion for deepfake detection[C]// ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2023: 1-5.   
[27] 蔺海荣,段晨星,邓晓衡,等.双忆阻类脑混沌神经网络及 其在 IoMT 数据隐私保护中应用[J/OL].电子与信息学 报 ,2025[2025-06- 10].http://kns.cnki.net/kcms/detail/11.4494.TN.20250427.1433.002.html. LIN H R, DUAN X C, DENG X H,et al.Dual-Memri stor Brain-Like chaotic neural network and its applica tion in IoMT data privacy protection[J/OL].Journal of Electronics & Information Technology,2025[2025-06-1 0].http://kns.cnki.net/kcms/detail/11.4494.TN.20250427.1 433.002..html.   
[28] WANG Y, YU K, CHEN C, et al. Dynamic graph learning with content-guided spatial-frequency relation reasoning for deepfake detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 7278-7287.   
[29] 张文祥,王夏黎,王欣仪,等.一种强化伪造区域关注的深 度伪造人脸检测方法[J].图学学报,2025,46(01):47-58. ZHANG W X, WANG X L, WANG X Y, et al.A deepfake face detection method that enhances focus on forgery regions[J/OL].Journal of Graphics,,2025,46(01):47-58.   
[30] ZHOU Y，FAN B，ATREY P K, et al. Exposing deepfakes using dual-channel network with multi-axis attention and frequency analysis[C]//Proceedings of the 2023 ACM Workshop on Information Hiding and Multimedia Security. 2023: 169-174.   
[31] ZHENG J S, ZHOU Y C, HU X Y, et al. Deepfake detection with combined unsupervised-supervised contrastive learning[C]//2024 IEEE International Conference on Image Processing (ICIP). IEEE, 2024: 787-793.   
[32] MOON K H, OK S Y, LEE S H. SupCon-MPL-DP: supervised contrastive learning with meta pseudo labels for deepfake image detection[J]. Applied Sciences, 2024, 14(8): 3249.   
[33] XIA R, LIU D, LI J, et al. Mmnet: multi-collaboration and multi-supervision network for sequential deepfake detection[J]. IEEE Transactions on Information Forensics and Security, 2024.   
[34] 丁博文,芦天亮,彭舒凡,等.基于多分类数据集的人脸伪 造算法识别模型[J/OL].计算机科学, 2024 [2025-06-10]. http://kns.cnki.net/kcms/detail/50.1075.TP.20241101.1335.008.html. DING B W, LU T L, PENG S F,et al.Face forgery a lgorithm recognition model based on multi-classificati on dataSet[J/OL].Compute Science,2024[2025-06-10].ht tp://kns.cnki.net/kcms/detail/50.1075.TP.20241101.1335.0 08.html.   
[35] LI Y, YANG X, SUN P, et al. Celeb-DF: a large-scale challenging dataset for deepfake forensics[C]//Pro- ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020: 3207-3216.   
[36] DOLHANSKY B, BITTON J, PFLAUM B, et al. The deepfake detection challenge(dfdc) dataset[J]. ArXiv: 2006.07397, 2020.   
[37] ROSSLER A, COZZOLINO D, VERDOLIVA L, et al. Faceforensics $^ { + + }$ : learning to detect manipulated facial images[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 1-11.   
[38] LIU Z, LUO P, WANG X, et al. Deep learning face attributes in the wild[C]//Proceedings of the IEEE International Conference on Computer Vision. 2015: 3730-3738.   
[39] JU Y, JIA S, CAI J, et al. Glff: global and local feature fusion for ai-synthesized image detection[J]. IEEE Transactions on Multimedia, 2023.   
[40] KARRAS T, LAINE S, AILA T. A style-based generator architecture for generative adversarial networks[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 4401-4410.   
[41] QIAN Y Y, YIN G J, SHENG L, et al. Thinking in frequency: Face forgery detection by mining frequencyaware clues[J]ECCV, 2020.   
[42] 陈咏豪,蔡满春,张溢文,等.基于参数高效微调及双流网 络的人脸伪造检测[J].计算机工程与应用,2025,61(10): 288-298. CHEN Y H, CAI M C, ZHANG Y W, et al.Face forgery detection based on parameter-efficient fine-tuning and dual-stream network[J/OL].Computer Engineering and Applications,2025,61(10):288-298. 丁博文（2001—），女，辽宁沈阳人，硕士研究生，CCF 学生会员，主要研究方向为深度伪造检测、网络安全等。   
DING Bowen, born in 2001, M.S. candidate. Her research interests include deep forgery detection, cyber security, etc.

![](images/a35e747a37751a679785e2d017c7a01ee610c02a62b42caf24c81b2ddeb1e1ce.jpg)

![](images/7a8662b04cfab657a81bfb314925f25b199884639fd501554a5601965cb63887.jpg)

芦天亮（1985—），男，河北保定人，博士，教授，主要研究方向为网络安全、人工智能等。LU Tianliang, born in 1985, Ph.D., professor, Ph.D. supervisor. His research interests include cyber security, artificial int elligence,etc.

![](images/620a0dfa8b3f846f4fbd31c42afea33959112a69f58d224dc7ba1498eadf1b71.jpg)

彭舒凡（1998—），男，江苏无锡人，硕士研究生，主要研究方向为信息网络安全等。  
PENG Shufan, born in which 1998, Ph.D. candidate. His research interests include Information cyber security, etc.王珑皓（1998—），男，浙江义乌人人，硕士研究生，CCF 学生会员，主要研究方向为图像深度伪造检验技术等。  
WANG Longhao, born in 1998, M.S. candidate. His research interests include Image deep forgery inspection technology, etc.

![](images/8cebbc7816104136ddba885bf486f59ef29046043c4c188dbde13c17afb19850.jpg)