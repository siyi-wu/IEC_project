# 基于频域和空域多特征融合的深度伪造检测方法

董佳乐 1，邓正杰 1,2，李喜艳 1，王诗韵 1(1. 海南师范大学信息科学技术学院，海南 海口 571127；2. 广西图像图形与智能处理重点实验室，广西 南宁 541004)

摘 要：在当今社会，面部伪造技术的迅速发展对社会安全构成了巨大挑战，尤其是在深度学习技术被广泛应用于生成逼真的伪造视频的背景下。这些高质量的伪造内容不仅威胁到个人隐私，还可能被用于不法活动。面对这一挑战，传统的基于单一特征的伪造检测方法已经难以满足检测需求。因此，提出了一种基于频域和空域多特征融合的深度伪造检测方法，以提高面部伪造内容的检测准确率和泛化能力。并将频域动态划分为3 个频带来提取在空域中无法挖掘的伪造伪影；对空域使用 EfficientNet_b4 网络和 Transformer 架构多尺度划分图像块来计算不同块的差异、根据上下图像块之间的一致性信息来进行检测以及捕获更精细的伪造特征信息；最后使用查询-键-值机制的融合块，将上述中的频域和空域的方法进行融合，从而更全面地挖掘到 2 个域中的特征信息，提升伪造检测的准确性和迁移性。大量的实验结果显示该方法有效，其性能明显优于传统深度伪造检测方法。

关 键 词：深度伪造检测；EfficientNet_b4 网络；频域特征；空域特征；特征融合中 图 分 类 号：TP 391.41 DOI：10.11996/JG.j.2095-302X.2025010104文献标识码：A 文 章 编 号：2095-302X(2025)01-0104-10

# Deepfake detection method based on multi-feature fusion of frequency domain and spatial domain

DONG Jiale1, DENG Zhengjie1,2, LI Xiyan1, WANG Shiyun1 (1. College of Information Science and Technology, Hainan Normal University, Haikou Hainan 571127, China; 2. Guangxi Key Laboratory of Image Processing and Intelligent Analysis, Nanning Guangxi 541004, China)

Abstract: In today’s society, the rapid advancement of facial forgery technology has posed a substantial challenge to social security, especially in the context where deep learning techniques have been widely employed to generate realistic fake videos. These high-quality forged contents not only threaten personal privacy but can also be utilized for illegal activities. Faced with this challenge, traditional forgery detection methods based on single features have become inadequate to meet detection demands. To address this issue, a deepfake detection method based on multi-feature fusion in both frequency and spatial domains was proposed to enhance the detection accuracy and generalization capability for facial forgeries. The frequency domain was dynamically divided into three bands to extract forgery artifacts that cannot be mined in the spatial domain. The spatial domain employed the EfficientNet_b4 network and Transformer architecture to segment image blocks at multiple scales, calculate differences between different blocks, perform detection based on consistency information between upper and lower image blocks, and capture more detailed forgery feature information. Finally, a fusion block using a query-key-value mechanism integrated the methods from the frequency and spatial domains, thereby more comprehensively mining feature information from both domains to enhance the accuracy and transferability of forgery detection. Extensive experimental results confirmed the effectiveness of the proposed method, demonstrating significantly superior performance compared to traditional deepfake detection methods.

Keywords: deepfake detection; EfficientNet_b4 network; frequency domain features; spatial domain features; feature fusion

随着采用先进的机器学习技术的视频伪造技术的出现，其因具有欺骗和操纵观众的潜力，已被人们重点关注。特别是由深度伪造(deepfake)方法制作的伪造视频，以逼真的方式伪造场景，导致人们产生误解、混淆和不信任。因此，应对深度伪造视频的影响，保护公众免受虚假信息的伤害，已成为确保多媒体内容的真实性和可信度的关键任务。

目前，现有的伪造检测方法[1-2]大多检测准确率不高，分析原因[3]可知，使用如 JPEG 的压缩方法，在空域中很难检测出图像的伪造伪影；通过频域中的分析可对图像的伪影进行检测，主要是将频域划分为固定的频带，但是划分的频带可能并非适用于所有图像。针对这些不足，本文提出对每张图像的频带进行动态划分并提取信息。同时，因为在空域中，人脸伪造区域通常大小不同，所以可对不同人脸图像在不同尺度上进行特征提取，例如，使用 Transformer 网络模型[4]，同时根据上下文之间的一致性信息进行检测。另一方面，如果仅依赖于单一类型的特征(如仅依赖空域[5]或频域[6]的特征)，则可能很难充分捕获复杂伪造内容的多维特性，导致检测性能在面对高质量伪造内容时受限。针对上述面部伪造检测所面临的挑战，提出了一种基于频域和空域多特征融合的深度伪造检测方法。本研究在以下几个方面做出了贡献：

1) 提出了结合离散余弦变换(discrete cosinetransform，DCT)技术的伪造人脸检测方法，对图像频带进行动态划分，检测图像的伪造伪影的特征信息，从而提高检测的准确率；

2) 提出了采用图像多尺度特征的伪造人脸检测方法，以 EfficientNet_b4 网络[3]为基础网络模型，引入 Transformer[4]架构，对人脸进行不同尺度的划分，一方面，计算不同块的差异，根据上下图像块之间的一致性信息进行检测；另一方面，捕获更精细的伪造特征信息；

3) 设计了一种使用查询-键-值机制的融合块，将频域和空域的方法进行融合，从而更全面地挖掘到 2 个域中的特征信息，提升伪造检测的准确性和迁移性。

# 1  相关工作

伪造方法可以从源图像中提取一个人的脸部特征，并将其合成到目标图像或视频中，从而创造出一种逼真的虚假效果。其主要包括 2 类方法：基于生成对抗网络(generative adversarial networks，GANs)[7]和变分自动编码器(variational autoencoders，VAEs)[8]的方法。GANs 是一种基于生成器和判别器博弈的深度学习模型，在人脸交换任务中，通过训练一个生成器网络和一个判别器网络来实现原始人脸和目标人脸的交换。VAEs 是一种基于自编码器(autoencoder，AE)的深度学习模型，通过学习输入数据的潜在分布结构以实现图像生成或重建。在人脸生成伪造中，VAEs 通常采用卷积变分自编码器(convolutional VAE，CVAE)来处理图像。对于将原始人脸和目标人脸的交换过程主要用于编码器和解码器。

面对日益增多的伪造人脸视频，学术界提出了大量的检测方法。目前的检测方法可分为 2 大类：基于频域特征提取和基于空域特征提取。

频域分析是信号处理中常用的一种技术。其已广泛应用于图像分类、纹理分类和超分辨率等方面。例如，文献[6]使用输入图像的 DFT 提取频域信息，然后根据不同频带的振幅进行平均。此外，结合伪造图像特征的相关理论和传统的图像信号处理的滤波器也可以用于细化和挖掘潜在的伪造信息。一些研究利用高通法[9]和 Gabor 滤波器以捕捉图像中的边缘或纹理信息，特别是那些高频部分。当应用高通滤波时，原图与仿制图之间在光谱特征上的差异变得非常明显。但这项研究中所使用的滤波器一般是预先定义好的，具有特定的设置，并且通常不能自适应地捕获虚假特征。因此，本研究采用可学习的 DCT 技术进行低、中、高 3 种频带动态划分来提取频域伪影线索。

基于空域特征提取的方法，前人已经提出了几种不同的用于深度伪造检测任务的技术。其中一些方法利用在合成过程中产生的伪造图像(如颜色或纹理)作为识别的线索。具体来说，通过提取的颜色空间特征来区分虚假图像和真实图像。3D-CS[5]将面部分解为多个纹理组合并结合浅层卷积神经网络(convolutional neural networks，CNN)进行伪造检测。例如，GAN 指纹识别技术引入了一个深度篡改鉴别器来发现特定的伪造模式。AUNet[10]提取人脸动作单元，生成自己的伪造文件用于操作预测，然后使用变换器进行伪造检测。然而，这些算法大多只使用空间信息，因此检测在颜色空间中的细微篡改不敏感。

# 2 基于频域和空域多特征融合的深度伪造检测方法

# 2.1 方法思路

本文目的是检测隐藏在伪造图像中的细微伪影，并提高伪造检测的准确率和迁移性。本方法主要由频域特征提取模块、空域特征提取模块以及特征融合模块共同组成。首先，使用 CNN 提取人脸图像特征，将特征信息放入频域分支和空域分支进行更细致的特征提取，其中，频域分支通过将图像的 DCT 转换到频域中，经过卷积操作将频域划分为低、中、高 3 个频段来充分提取频域中的细微伪影；空域分支利用堆叠的多尺度变换器和自注意力机制相结合提取每个图像补丁的伪影信息；最后通过自注意力机制将 2 个域中的信息进行多特征交叉融合来进一步提取更多的伪造伪影信息，同时实现二分类和伪造区域的热力图。本方法的整体检测框架如图 1 所示。

![](images/b8b1ec397a0d277a230813f621f958e977a9e3d689d297a33949f2b3dd317c0d.jpg)  
图 1  本文检测方法框架图  
Fig. 1  Framework diagram of the detection method in this paper

其中，由于将原始图像直接输入到 2 个提取模块中可能会丢失一些低级和中级的局部特征信息，而通过 EfficientNet_B4 网络进行初始特征提取，可以提取出具有更高级别语义信息的特征图[3]，然后再利用 2 个模块进行全局范围的特征提取，从而获得更具判别能力的特征表示。所以，首先使用卷积操作提取输入图像的特征，然后将其输入到频域特征提取模块和空域提取模块用于伪造线索检测。

# 2.2 初始特征提取

EfficientNet 网络是 2019 年由谷歌团队提出的一种基于复合缩放的方法，可用于设计高效且准确的 CNN。其中复合缩放旨缩放模型的高度、深度和分辨率 3 个维度，通过巧妙地对这 3 个维度均衡缩放来实现更好的模型效果和模型效率。已有实验研究表明，EfficientNet 在参数量和计算复杂度方面具有优势，同时在保持准确性的同时提供了高效的模型架构，也在计算机视觉任务中展现了出色的性能。在图像分类上取得了较高的准确率[11]。考虑到计算资源的消耗与识别精度的平衡，本项工作采纳了 EfficientNet_B4 作为抽取人脸图像首要特性的方法。网络结构如图 2 所示。其中，网络以 Stem为主干，其包含 7 个 Block。Module 1 是每个子Block 的起点，Module 2 用于除第一个模块外的所有主要模块的第一个子 Block 的起点，Module 3 作为跳跃连接的子 Block，最后以 Final layers 结束。

其中， $( \times N )$ 表示括号内的模块重复次数。每个模块包含的结构，如图 3 所示。Stem 是网络的起始部分，用于对输入数据进行初始处理和特征提取。在网络中，Stem 包含卷积层、归一化层和激活函数，用于对输入图像进行初步特征提取和尺寸调整，为后续模块提供特征表示。Final Layers 是网络的最后一部分，包括卷积层、归一化层和激活函数。这些层用于从前面模块提取的特征映射到最终的输出类别或预测结果。Module 1：包含一系列堆叠的重复结构单元，用于学习低级别的特征。这些结构单元通常包括深度可分离卷积和激活函数等操作。Module 2：在 Module 1 之后，其包含更多的重复结构单元，用于学习更高级别的特征。这些结构单元包括更深的卷积层。Module 3：包含全局平均池化、缩放和卷积层，旨在整合前几个模块学到的特征，并进一步提取高级别的语义特征。

![](images/54bfcf81195ba84d9a82e88e0700d7dd2b1ea47bb9ce2237fe3a978477cc8dc3.jpg)  
Fig. 2  Structure of the EfficientNet_B4 network model

![](images/05f32245163868be81d1d1a5010803e5f5c8d93423fc2b58a73b7e5a9eead492.jpg)  
图 2  EfficientNet_B4 网络模型结构  
图 3  EfficientNet_B4 网络模型中模块结构  
Fig. 3  Module structure within the EfficientNet_B4 network model

# 2.3 频域特征提取

LIANG 等[12]的研究成果显示，频率信息包含不可见的伪造伪影，利用 DCT 提取的频域信息来辅助空域特征，有利于捕捉伪造人脸中的伪造伪影。参考该思路，本文设计了 DCT 频谱划分以进一步提高伪造检测准确率。图 4 为在 DCT 频域中的人脸图像显示对比结果。在空域纹理中无法看出的图像之间的差异，在经过频域后再转换回到空域，可以明显看出图像的差别。在空域中很难检测出图像的伪造伪影，可以考虑通过频域中的分析来补充。

![](images/43895d4c6bf9e1ff50efe85811e0a2db9569876d5975b7f8c0f34594106b7506.jpg)  
图4  真伪图像分析示例((a)真实图像；(b)\~(c)伪造图像，其中像素点亮度提高了 0.3)  
Fig. 4  Example of real vs fake image analysis ((a) Real image; (b)\~(c) Forged images, where the brightness of pixels is increased by 0.3)

具体来说，首先将得到的特征信息沿着空间维度应用 DCT 得到频谱，然后为了充分全面地提取到图像频谱的信息，使用卷积操作将频带自适应地划分为低、中、高 3 个频段，再计算出与 3 个频段$\left\{ f _ { \mathrm { b a s e } } ^ { i } \right\} _ { i = 1 } ^ { 3 }$ 器，即一个矩阵，频谱中对应位置的值在该频段范围内，则为 1，否则为 0。这些基本滤波器实现频率选择，其中矩阵中值为 1 的元素表示该频率被“通过”，而值为 0 则表示该频率被“阻止”，进行了自适应选择以进一步关注二进制基本滤波器以外的频率。为了加强自适应性，为这些基本滤波器，配上 3 个和输入图像尺寸相同的可学习的滤波器 $\left\{ f _ { \mathrm { w } } ^ { i } \right\} _ { i = 1 } ^ { 3 }$ 。其中，初始的可学习滤波器的值是随机生成的张量，随后将这些张量转换成模型的可学习参数，在模型训练过程中，计算这些参数的梯度，通过反向传播算法更新其值，最后将 2 种滤波器进行相加得到组合频率滤波器为

$$
f ^ { i } = f _ { \mathrm { b a s e } } ^ { i } ( x ) + \sigma \Big ( f _ { w } ^ { i } ( x ) \Big ) , i = \{ 1 , \cdots , 3 \}
$$

$$
\sigma ( t ) = \frac { 1 - \exp ( - t ) } { 1 + \exp ( - t ) }
$$

式中： $\sigma$ 为函数，可将学习滤波器的值限制在-1 到1 之间的范围，进行了归一化处理。

由此，频域滤波 $F$ 是输入图像的 DCT 后的频

率和组合滤波器 $f$ 的点积，可表示为

$$
F ^ { i } = D ( x ) \odot f ^ { i }
$$

式中： $D ( x )$ 表示对图像 $x$ 进行离散余弦变换操作；  
$\odot$ 表示数乘操作。

对频域中图像进行平移或局部调整时，对应的频域表示不会发生显著变化。另外，在频域中，无法使用 CNN 进行特征提取。因此，为了方便人们视觉观察以及结合空域操作，进行逆离散余弦变换(inverse DCT，IDCT)，将频域信号转换回 RGB 颜色空间。因此，对于输入图像 $x$ ，分解后的图像分量为

$$
\boldsymbol { y } ^ { i } = \boldsymbol { D } ^ { - 1 } ( \boldsymbol { F } ^ { i } )
$$

虽然本文通过卷积操作将频带划分为了低、中、高 3 个频段并添加了可学习滤波器，但对于所分解的频域的每段频带可能并不会完全地将人脸中的伪影信息完全提取出来，为了得到更细致全面的特征信息，可将 3 个频段的信息在通道方向上进行重组融合，并放入到 CNN 中进行提取。

# 2.4 空域特征提取

如图 5 所示，不同的数据集或不同的伪造方法所生成的图像具有的伪造特征差异很大。一方面，伪造区域大小不同，需要对不同人脸图像在不同尺度上进行特征提取；另一方面，因为在一张图像上，并不是所有的区域都被伪造，所以可根据上下文之间的一致性信息进行检测。虽然传统的 CNN 也可以进行多尺度特征提取，但 Transformer 可以根据固定的输入的图像块的大小提取不同尺度的连续性信息。本文通过对图像进行不同尺寸划分，以此覆盖广泛的区域大小，从而实现对更微妙伪造迹象的识别。本小节将介绍使用多尺度的 Transformer作为空域特征提取部分的架构。整体结构如图 6所示。

为了提取不同尺度的连续信息，将输入的特征图划分为不同大小的空间图像块。其中，大的空间图像块用来提取明显的语义不连贯信息，而小的则用来提取细微的像素级不连贯信息。之后，进一步计算不同注意力头的图像块之间的自注意力。

具体来说，将通过卷积操作后的特征图输入到多尺度 Transformer 框架中，按照本文划分方式，将每个空间图像块通过线性投影操作展平为一维向量，为了使图像块之间的语义不产生差错，也要对每个一维向量添加位置编码，然后，将一维向量嵌入到查询嵌入 $\varrho$ 中，同理，分别得到键嵌入 $\pmb { K }$ 和值嵌入 $V$ ，再使用归一化、softmax 函数和矩阵乘法来计算上下文即不同图像块之间的相关性为

![](images/6e0b03772158a35bdaa594ef3da8f6f5a947129ce22eadc419af000f9681f520.jpg)  
图 5 ${ \mathrm { F F } } { + } { + }$ 图像的视觉伪影((a)眼部伪影；(b)面部伪影；(c)嘴部伪影)

$$
R = \mathrm { s o f t m a x } \left( { \frac { Q \times K ^ { \mathrm { T } } } { \sqrt { C } } } \right)
$$

式中： $c$ 为输入图像的通道数。

式(5)表示的是不同图像块的相关性，基础此，对所有相关图像块的关联度进行加权汇总，最终形成对查询图像块的高度综合和精确输出为

$$
A = \sum _ { j = 1 } ^ { N } R V
$$

式中： $N$ 为相关块数量。

然后，将查询图像块重塑回原图像的尺寸，将来自不同尺度划分的头部特征连接起来，可进一步通过一个二维残差块获得空域特征信息。

# 2.5 特征融合模块

目前，基于 2 个模态融合的方式大多是平行或是简单的相加，使用这 2 种方法的前提是 2 个模态的信息是平衡的。在伪造检测中，尽管希望 2 种模态提供的信息是平衡的，但实际上频域信息可能包含较多噪声。例如，在人脸图像的特定区域，如头发在频域中的代表性可能偏向高频成分。这表明，频域信息应被视为支持性的模态，辅助空间域信息以提高检测的准确性。

为了更好地将频域和空域 2 方面的特征信息挖掘出来，同时基于第 2.4 节中用到的 Transformer架构中查询-键-值的启发，本方法采用查询-键-值的方法来融合 2 个模态，将空域模态作为查询部分、频域模态作为键-值部分，该设计一方面能够突出空域的作用，另一方面也能更好地挖掘频域分布中的异常区域。

具体来说，首先将空域特征 $T _ { i }$ 和频域特征 $W _ { i }$ 通过一维卷积操作得到嵌入向量 $\boldsymbol { Q } _ { i }$ ， ${ \bf \delta } _ { { \bf { K } } _ { i } }$ ， $V _ { i }$ ，然后将其沿着空间维数展平得到各自的二维嵌入向量$\tilde { Q } _ { i } , \tilde { \cal K } _ { i } , \tilde { V } _ { i }$ ，进一步计算融合特征为

$$
\tilde { M } _ { i } = \mathrm { s o f t m a x } \Bigg ( \frac { \tilde { \bf Q } _ { i } \times \tilde { \cal K } _ { i } ^ { \mathrm { T } } } { \sqrt { C } } \Bigg ) \tilde { V } _ { i }
$$

最后，将频域模块、空域模块和特征融合模块叠加更新了 $D$ 次(本文取 $D { = } 4 )$ ，得到融合的特征信息 $M _ { i }$ ，再通过 CNN 对图像进行分类。为了进一步显示图像的伪造区域，通过解码器对人脸图像进行伪造定位。

![](images/376296d4868472a54f48fbef4cacb1d2ca379928ee598bd54dec9ac418c53b86.jpg)  
Fig. 5  Visual artifacts in $\mathrm { F F } { + } { + }$ images ((a) An eye artifact; (b) A facial artifact; (c) A mouth artifact)   
图 6  空域模块模型Fig. 6  Spatial domain module model

# 3 实验与结果分析

# 3.1 数据集介绍

本文使用 FaceForensics $^ { + + ( \mathrm { F F + + } ) }$ 视频数据集、Celeb-DeepFake (Celeb-DF)视频数据集和 DFF 新数据集进行了实验。FaceForensics $^ { + + }$ 视频数据集使用多种伪造算法生成虚假视频，包括人脸替换类型的DeepFake (DF)和 FaceSwap (FS)算法，以及表情篡改类型的 Face2Face (F2F)和 NeuralTextures (NT)算法。此外，这些数据集还包含未修改的原始视频集，总共包含了 1 000 个不同时长的短视频样本。Celeb-DeepFake 视频数据集包括人脸替换类型为DeepFake 算法，总共包含 1 100 多个时长不一致的短时频样本。DFF 新数据集包含 3 种不同的生成模型来合成深度伪造，以评估深度伪造检测方法区分人工智能生成和真实图像的能力。

FaceForensics $^ { + + }$ 数据集根据压缩程度的不同分为 RAW 高画质、C23 中画质和 C40 低画质 3 个级别。中画质的视频更贴近实际应用场景。因此，在本研究中，主要使用中画质的 4 类伪造视频，并进行实验。

在实验中，采用了间隔取帧的策略来提取视频中的图像帧。具体而言，每隔 10 帧提取 1 帧，每个视频共提取 32 张。通过这种方法，可以有效地减少数据集中的重复信息，同时加快了训练过程的速度。然后，根据 $8 : 1 : 1$ 的比例将提取的图像进行划分，分别形成训练集、验证集和测试集。

# 3.2 实验设置

1) 预处理。本文使用 Dlib 作为人脸检测和特征提取的工具，从视频帧中确定人脸在图像中的位置和边界框，提取人脸图像，并对人脸图像进行了尺寸的统一处理，将其调整为统一的 $3 2 0 \times 3 2 0$ 像素。对人脸图像进行初始特征提取，图像大小变为$8 0 \times 8 0$ 像素。

2) 环境设置。使用了 PyTorch 深度学习框架作为基础构建。采用了 i7-10700K 处理器和英伟达3090 显卡。CUDA 版本为 11.1。

3) 训练参数。在训练过程中，将训练批次设置为 16，设定了最大训练 epoch 为 20，使用 Adam进行优化，初始学习率为 0.000 1，学习率每 40 步下降 10 次。

4) 评估标准。利用 sigmoid 激活函数预测模型的判断结果，并以 0.50 作为识别真假的准则。选择了准确率(accuracy，ACC)和 ROC 曲线下方的面积(area under the roc curve，AUC)作为评估模型对分类判断正确与否的标准。

5) 实验方法的选用。为了展现本方法对不同数据集以及结果分析的准确率以及有效性，针对不同表需要呈现的作用及效果，分别选用了相关方法进行比较。

# 3.3 结果分析

# 3.3.1 实验结果

$\mathrm { F F ^ { + + } }$ 是一种广泛应用于各种深度假检测方法中的数据集。选用的比较方法包括：文献[13-17]。

本文分别测试了 $\mathrm { F F ^ { + + } }$ 中的 4 类数据集在模型上的检测性能，并在表中显示了对应的 ACC 值 $( \% )$ 。由表1 可见，本方法在FS，F2F 和NT 这3 个子数据集上都取得了更好的性能，表明了本方法在检测不同方面伪造结果上的有效性。但本方法在 DF 数据集中准确率偏低，经研究发现，文献[17]方法在空域上进行了全局和局部特征提取，同时在频域中进行了全局特征提取，在频域和空域特征融合过程中，将2 个域进行了简单相加。而本文策略为查询-键-值机制的融合块，更有利于特征信息的提取。本方法在其他 3 个子数据集的结果优于 DF，也说明这一点。文献[16]方法使用了 2 个网络模型在空域上进行特征处理，但缺乏对多维特征提取。本方法可能因未在空间域进行更深入的特征提取而导致这一结果。总之，在其他 3 个数据集的准确率方面，本方法与次优方法相比分别提升了 $1 . 6 2 \%$ ，$2 . 0 6 \%$ 和 $2 . 8 0 \%$ 。

<html><body><table><tr><td colspan="4">Table1 Accuracyresultsonthe FaceForensics++dataset/%</td></tr><tr><td>模型</td><td>DF</td><td>FS</td><td>F2F NT</td></tr><tr><td>文献[13]</td><td>93.40</td><td>92.93</td><td>93.51 79.37</td></tr><tr><td>文献[14]</td><td>91.65</td><td>87.03 90.73</td><td>60.57</td></tr><tr><td>文献[15]</td><td>94.50</td><td>84.50 80.30</td><td>74.00</td></tr><tr><td>文献[16]</td><td>97.30</td><td>94.20 81.80</td><td>79.40</td></tr><tr><td>文献[17]</td><td>97.80</td><td>93.40 88.70</td><td>84.20</td></tr><tr><td>本文模型</td><td>95.88</td><td>95.82</td><td>95.57 87.00</td></tr></table></body></html>

注：加粗数据为最优值；下划线数据为次优值。

当前常有人使用人工智能大模型生成的人脸图像，为了验证本文方法在区分人脸图像和真实图像的效果，在DFF 数据集上展开实验，并与RECCE方法[18]对比。表 2 结果表明，在 ACC 值及 AUC值方面，本方法均具有很大优势。

表 1  在 FaceForensics $^ { + + }$ 数据集上的准确率结果 $\%$   
表 2  在 DFF 数据集上的结果Table 2  Results on the DFF dataset  

<html><body><table><tr><td rowspan="2">模型</td><td colspan="2">Insight</td><td colspan="2">Text2img</td><td colspan="2">Inpainting</td></tr><tr><td>ACC/%</td><td>AUC</td><td>ACC/%</td><td>AUC</td><td>ACC/%</td><td>AUC</td></tr><tr><td>RECCE[18]</td><td>58.99</td><td>63.12</td><td>38.14</td><td>35.12</td><td>51.35</td><td>51.52</td></tr><tr><td>本文模型</td><td>90.21</td><td>96.33</td><td>96.50</td><td>99.32</td><td>92.81</td><td>97.98</td></tr></table></body></html>

注：加粗数据为最优值。

具体来说，RECCE 方法的核心为重建学习，即利用真实人脸图像训练一个重建网络，以能够学习到真实人脸的共性特征。当输入伪造人脸图像时，由于伪造人脸与真实人脸在数据分布上存在不一致，因此伪造人脸的重建误差更明显，这反映了潜在的伪造区域。在重建网络的基础上，进一步利用其隐性特征对真实与伪造人脸进行分类。这是一个很好的检测思路。相对来说，本文基于频域的方法能够捕捉到图像在频率上的细微变化，而这些变化往往在 RECCE 通过隐性特征进行分类无法进行精准检测。基于空域方法则侧重于图像的空间结构和纹理信息。伪造图像在空域上可能会出现一些不自然的纹理模式、颜色分布不均等问题。结合空域特征，可以进一步增强对伪造图像的辨别能力。2 种提取方法使特征提取更全面，这是本方法具有较大优势的主要原因；其次，本方法有更强的抗干扰能力。基于频域和空域的方法通常对噪声具有一定的鲁棒性，能够在噪声存在的情况下仍然准确地提取到有效的特征，从而保证检测的准确性。而RECCE 方法可能对噪声较为敏感，噪声的存在可能会影响重建网络的性能，进而影响检测结果。基于上述分析，说明本方法在检测人工智能生成图像的有效性。

# 3.3.2 频域模块的频带划分

本文对不同频带划分的方式进行了实验比较，从而选出适合本文检测方法的最佳方案，如表 3 所示。第一种划分方式(能量)：根据以往经验，频带数 $N { = } 3$ ，低频带 f1 基为整个频谱的前 1/16，中间频段 f2 基在频谱的 1/16 和 1/8 之间，高频频带 f3基为最后 7/8；第二种划分方式(信息熵)：将频谱按照能量划分，然后进行信息熵运算，以提取高通道的特征；第三种划分方式(平均)：对频谱进行平均划分；第四种划分方式(动态)：将变换后的频谱进行 CNN 操作，动态选取适合提取特征信息的频带划分方式，划分为低、中、高 3 种频带。见表 3\~4，通过比较发现，“动态”划分方式，相比其他 3 种方法都有所提升，取得了最好的性能。针对 4 类数据集，ACC 值分别提高了 $0 . 8 0 \%$ ，$0 . 5 6 \%$ ， $0 . 4 5 \%$ 和 $1 . 9 5 \%$ ，同时，AUC 值也高于其他 3 类划分方式，因此，本方法选用的划分方式为使用 CNN 动态划分频带。

# 3.3.3  频域模块空域模块中尺度划分设计

为了验证在空域中的 Transformer[19]在不同头部中使用多尺度图像块的有效性，本文用几种不同大小的单尺度模型，在 $\mathrm { F F ^ { + + } }$ 上进行了实验。表 5表明，在划分多尺度图像块上取得了最好的性能，即 ACC 评分比 $8 0 \times 8 0$ ， $4 0 { \times } 4 0$ ， $2 0 { \times } 2 0$ 和 $1 0 \times 1 0$ 单尺度变压器最多提高了 $3 . 9 8 \%$ ， $3 . 1 0 \%$ ， $1 . 3 9 \%$ 和$0 . 8 5 \%$ 。这证实了使用多尺度变压器确实是有效的。

表 3  对频域的不同划分方式实验准确率比较 $/ \%$ Table 3  Comparison of experimental accuracy ondifferent frequency domain partitioning methods/%  

<html><body><table><tr><td>划分方式</td><td>DF</td><td>FS</td><td>F2F</td><td>NT</td></tr><tr><td>能量</td><td>95.73</td><td>95.26</td><td>95.36</td><td>86.05</td></tr><tr><td>信息熵</td><td>95.08</td><td>95.27</td><td>95.29</td><td>85.61</td></tr><tr><td>平均</td><td>95.34</td><td>95.36</td><td>95.16</td><td>85.27</td></tr><tr><td>动态</td><td>95.88</td><td>95.82</td><td>95.57</td><td>87.00</td></tr></table></body></html>

注：加粗数据为最优值。

表 4  对频域的不同划分方式实验 AUC 值比较 $\%$ Table 4  Comparison of experimental AUC values ondifferent frequency domain partitioning methods/%  

<html><body><table><tr><td>划分方式</td><td>DF</td><td>FS</td><td>F2F</td><td>NT</td></tr><tr><td>能量</td><td>99.33</td><td>99.34</td><td>99.14</td><td>93.58</td></tr><tr><td>信息熵</td><td>99.30</td><td>99.20</td><td>99.30</td><td>93.28</td></tr><tr><td>平均</td><td>99.37</td><td>99.25</td><td>99.23</td><td>93.94</td></tr><tr><td>动态</td><td>99.39</td><td>99.34</td><td>99.33</td><td>94.66</td></tr></table></body></html>

注：加粗数据为最优值。

表 5  空域不同尺度划分方式实验准确率比较 $/ \%$ Table 5  Comparison of experimental accuracy ondifferent scale partitioning methods inthe spatial domain module/%  

<html><body><table><tr><td>块大小</td><td>DF</td><td>FS</td><td>F2F</td><td>NT</td></tr><tr><td>80×80</td><td>93.47</td><td>93.58</td><td>93.13</td><td>83.02</td></tr><tr><td>40×40</td><td>94.72</td><td>94.29</td><td>94.39</td><td>83.90</td></tr><tr><td>20×20</td><td>94.88</td><td>94.82</td><td>95.08</td><td>85.61</td></tr><tr><td>10×10</td><td>95.55</td><td>94.97</td><td>95.16</td><td>86.42</td></tr><tr><td>本文模型</td><td>95.88</td><td>95.82</td><td>95.57</td><td>87.00</td></tr></table></body></html>

注：加粗数据为最优值。

# 3.3.4  泛化能力分析

泛化能力是深度造假检测的核心。本文分别通过在 FaceForensis $^ { + + }$ 和 Celeb-DF 数据集上进行测试来评估本模型的泛化能力。为了更好地对比模型的泛化能力，本文选用了较为常用的比较泛化能力的方法，与当前主流的 6 类方法相比，不同模型在不同数据集上的检测结果见表 6。由表可知，在跨数据集方面的检测效果有时不够理想，但是本方法比大多数现有的方法具有更好的泛化效果。在 2 类数据集上 AUC 值可达到 97.19 和 73.81。

# 3.3.5 可解释性分析

为了进一步说明本方法在人脸区域伪造部分的可解释性，分别对在 FaceForensics $^ { + + }$ 中的 4 类数

# 表 6  不同方法的跨数据集 AUC 结果

Table 6  AUC results across different datasets for various methods   

<html><body><table><tr><td>方法</td><td>FaceForensis++</td><td>Celeb_DF</td></tr><tr><td>Two-stream[20]</td><td>70.10</td><td>53.83</td></tr><tr><td>Meso4[13]</td><td>84.70</td><td>54.80</td></tr><tr><td>DSP-FWA[21]</td><td>93.00</td><td>64.60</td></tr><tr><td>Capsule[22]</td><td>96.60</td><td>57.50</td></tr><tr><td>Two Branch[23]</td><td>93.18</td><td>73.41</td></tr><tr><td>SMIL[24]</td><td>96.80</td><td>56.30</td></tr><tr><td>本文模型</td><td>97.19</td><td>73.81</td></tr></table></body></html>

注：加粗数据为最优值。

据集分别进行了 gradcam 可视化分析。通过生成热力图来显示模型对输入图像的伪造区域，帮助理解模型在做出预测[25]时所依据的特征。

在图 7 中，列举了 FaceForensis $^ { + + }$ 数据集的4 个类别中的一个例子，图 7(a)为原图，图 7(b)为基线模型伪造图像可视化结果图，图 7(c)为本文模型伪造图像可视化结果图。从这些结果图中可以清晰地看出，与基线模型相比，本模型检出的伪造区域与基线模型的重合度很高。通过可视化结果可发现，伪造区域主要集中在鼻子和嘴巴附近。这一发现不仅证明了本模型能够更准确地定位伪造区域，还表明模型能够更有效地学习到图像中与伪造相关的特征。

![](images/63ad4b499fac4f0067dac273aae10d9409356a290830b80960c7ed2a1fc97ded.jpg)  
图 7  基线模型和本文模型可解释性分析结果((a)原图；(b)基线模型；(c)本文模型)  
Fig. 7  Interpretability analysis results of the baseline model and the model proposed in this paper ((a) Original image; (b) Baseline model; (c) Ours)

# 3.3.6 消融研究

该方法的频域特征提取模块用于提取在空域中很难检测出图像的伪造伪影的特征信息，空域特征提取模块旨在计算不同块的差异，根据上下图像块之间的一致性信息来进行检测，同时可以捕获更精细的伪造特征信息，特征融合模块可以更全面地挖掘到 2 个域中的特征信息。为了评估频域、空域以及两者融合模块的有效性，将其从整体模型中分离出来，并展示了 $\mathrm { F F ^ { + + } }$ 上的性能变化，还用简单的连接操作替换了特征融合模块，以验证使用查询-键-值机制在特征融合中的有效性。定量结果见表 7，其验证了各个模块的使用可以有效地提高本模型的检测性能。特别是，本文提出的特征融合模块对本方法进行了显著地改进，在 NT 数据集上 ACC性能增益为 $1 . 6 5 \%$ ，这主要得益于空域模块的互补信息。

表 7  消融实验结果 $\%$   
Table 7  Ablation experiment results/%   

<html><body><table><tr><td>模型</td><td>空域</td><td>频域</td><td>融合</td><td>DF</td><td>FS</td><td>F2F</td><td>NT</td></tr><tr><td>1</td><td>√</td><td></td><td></td><td>95.41</td><td>95.08</td><td>95.01</td><td>86.14</td></tr><tr><td>2</td><td>-</td><td>√</td><td></td><td>95.54</td><td>95.76</td><td>95.13</td><td>85.35</td></tr><tr><td>3</td><td>√</td><td>√</td><td></td><td>95.50</td><td>95.80</td><td>95.35</td><td>86.57</td></tr><tr><td>4</td><td>√</td><td>√</td><td>√</td><td>95.88</td><td>95.82</td><td>95.57</td><td>87.00</td></tr></table></body></html>

注：-表示不添加模块；√表示添加模块；加粗数据为最优值。

# 4  结束语

本文提出了一种基于频域和空域多特征融合的深度伪造检测方法，其中包含基于 DCT 技术的伪造人脸检测方法和基于多尺度的伪造人脸检测方法两者的特征融合，旨在检测空域中很难检出的图像的伪造伪影的特征信息，根据上下图像块之间的一致性信息来进行检测以及捕获空域中更精细的伪造特征信息。实验结果表明，与多种传统方法相比，本方法在多种数据集上取得了较好的检测性能以及迁移性。本方法的不足是在 DF 数据集中，检测准确率相对其他方法稍低，可能是由于本文空域的检测方法对这类数据适用性不高，后续将对其展开相应的深入研究。

# 参考文献 (References)

[1] WANG B, HUANG L Q, HUANG T Q, et al. Two-stream Xception structure based on feature fusion for DeepFake detection[J]. International Journal of Computational Intelligence Systems, 2023, 16(1): 134.   
[2] LIU D C, CHEN T, PENG C L, et al. Attention consistency refined masked frequency forgery representation for generalizing face forgery detection[EB/OL]. [2024-06-20]. https://arxiv.org/ abs/2307.11438.   
[3] LUAN T, LIANG G Q, PEI P F. Interpretable DeepFake detection based on frequency spatial transformer[J]. International Journal of Emerging Technologies and Advanced Applications, 2024, 1(2): 19-25.   
[4] COCCOMINI D A, MESSINA N, GENNARO C, et al. Combining EfficientNet and vision transformers for video deepfake detection[C]//The 21st International Conference on Image Analysis and Processing. Cham: Springer, 2022: 219-229.   
[5] ZHU X Y, FEI H Y, ZHANG B, et al. Face forgery detection by 3D decomposition and composition search[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023, 45(7): 8342-8357.   
[6] DURALL R, KEUPER M, PFREUNDT F J, et al. Unmasking DeepFakes with simple features[EB/OL]. [2024-06-20]. https://arxiv.org/abs/1911.00686.   
[7] GOODFELLOW I J, POUGET-ABADIE J, MIRZA M, et al. Generative adversarial nets[C]//The 27th International Conference on Neural Information Processing Systems. Cambridge: MIT Press, 2014: 2672-2680.   
[8] KINGMA D P. Auto-encoding variational Bayes[EB/OL]. [2024-06-20]. https://openreview.net/forum?id=33X9fd2-9FyZd.   
[9] WANG S Y, WANG O, ZHANG R, et al. CNN-generated images are surprisingly easy to spot...for now[C]//2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. New York: IEEE Press, 2020: 8692-8701.   
[10] BAI W M, LIU Y F, ZHANG Z P, et al. AUNet: learning relations between action units for face forgery detection[C]//2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition. New York: IEEE Press, 2023: 24709-24719.   
[11] TAN M X, LE Q. EfficientNet: rethinking model scaling for convolutional neural networks[EB/OL]. [2024-06-20]. https://proceedings.mlr.press/v97/tan19a.html.   
[12] LIANG W Y, WU Y F, WU J S, et al. FAClue: exploring frequency clues by adaptive frequency-attention for Deepfake detection[C]//2023 42nd Chinese Control Conference. New York: IEEE Press, 2023: 7621-7626.   
[13] AFCHAR D, NOZICK V, YAMAGISHI J, et al. MesoNet: a compact facial video forgery detection network[C]//2018 IEEE International Workshop on Information Forensics and Security. New York: IEEE Press, 2018: 1-7.   
[14] QIAN Y Y, YIN G J, SHENG L, et al. Thinking in frequency: face forgery detection by mining frequency-aware clues[C]//The 16th European Conference on Computer Vision. Cham: Springer, 2020: 86-103.   
[15] NIRKIN Y, WOLF L, KELLER Y, et al. DeepFake detection based on discrepancies between faces and their context[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022, 44(10): 6111-6121.   
[16] LIN K H, HAN W H, LI S D, et al. IR-Capsule: two-stream network for face forgery detection[J]. Cognitive Computation, 2023, 15(1): 13-22.   
[17] JIN X, WU N, JIANG Q, et al. A dual descriptor combined with frequency domain reconstruction learning for face forgery detection in deepfake videos[J]. Forensic Science International: Digital Investigation, 2024, 49: 301747.   
[18] SONG H X, HUANG S Y, DONG Y P, et al. Robustness and generalizability of deepfake detection: a study with diffusion models[EB/OL]. [2024-06-20]. https://arxiv.org/abs/2309.02218.   
[19] 翟永杰, 李佳蔚, 陈年昊, 等. 融合改进 Transformer 的车辆 部件检测方法[J]. 图学学报, 2024, 45(5): 930-940. ZHAI Y J, LI J W, CHEN N H, et al. The vehicle parts detection method enhanced with Transformer integration[J]. Journal of Graphics, 2024, 45(5): 930-940 (in Chinese).   
[20] ZHOU P, HAN X T, MORARIU V I, et al. Two-stream neural networks for tampered face detection[C]//2017 IEEE Conference on Computer Vision and Pattern Recognition Workshops. New York: IEEE Press, 2017: 1831-1839.   
[21] LI Y Z, LYU S W. Exposing DeepFake videos by detecting face warp artifacts[C]//IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. New York: IEEE Press, 2019: 46-52.   
[22] NGUYEN H H, YAMAGISHI J, ECHIZEN Capsule-forensics: using capsule networks to detect forged images and videos[C]//ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing. New York: IEEE Press, 2019: 2307-2311.   
[23] MASI I, KILLEKAR A, MASCARENHAS R M, et al. Two-branch recurrent network for isolating deepfakes in videos[C]//The 16th European Conference on Computer Vision. Cham: Springer, 2020: 667-684.   
[24] LI X D, LANG Y N, CHEN Y F, et al. Sharp multiple instance learning for DeepFake video detection[C]//The 28th ACM International Conference on Multimedia. New York: ACM, 2020: 1864-1872.   
[25] 李滔, 胡婷, 武丹丹. 结合金字塔结构和注意力机制的单目 深度估计[J]. 图学学报, 2024, 45(3): 454-463. LI T, HU T, WU D D. Monocular depth estimation combining pyramid structure and attention mechanism[J]. Journal of Graphics, 2024, 45(3): 454-463 (in Chinese).