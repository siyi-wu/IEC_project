# 《计算机技术与发展》网络首发论文

题目： 增强特征提取和解释的遥感图像语义分割模型  
作者： 于攀琳，吴旭，张凌云，刘子涵  
DOI： 10.20165/j.cnki.ISSN1673-629X.2025.0039  
收稿日期： 2024-10-31  
网络首发日期： 2025-03-10  
引用格式： 于攀琳，吴旭，张凌云，刘子涵．增强特征提取和解释的遥感图像语义分割模型[J/OL]．计算机技术与发展.https://doi.org/10.20165/j.cnki.ISSN1673-629X.2025.0039

网络首发：在编辑部工作流程中，稿件从录用到出版要经历录用定稿、排版定稿、整期汇编定稿等阶段。录用定稿指内容已经确定，且通过同行评议、主编终审同意刊用的稿件。排版定稿指录用定稿按照期刊特定版式（包括网络呈现版式）排版后的稿件，可暂不确定出版年、卷、期和页码。整期汇编定稿指出版年、卷、期、页码均已确定的印刷或数字出版的整期汇编稿件。录用定稿网络首发稿件内容必须符合《出版管理条例》和《期刊出版管理规定》的有关规定；学术研究成果具有创新性、科学性和先进性，符合编辑部对刊文的录用要求，不存在学术不端行为及其他侵权行为；稿件内容应基本符合国家有关书刊编辑、出版的技术标准，正确使用和统一规范语言文字、符号、数字、外文字母、法定计量单位及地图标注等。为确保录用定稿网络首发的严肃性，录用定稿一经发布，不得修改论文题目、作者、机构名称和学术内容，只可基于编辑规范进行少量文字的修改。

出版确认：纸质期刊编辑部通过与《中国学术期刊（光盘版）》电子杂志社有限公司签约，在《中国学术期刊（网络版）》出版传播平台上创办与纸质期刊内容一致的网络版，以单篇或整期出版形式，在印刷出版之前刊发论文的录用定稿、排版定稿、整期汇编定稿。因为《中国学术期刊（网络版）》是国家新闻出版广电总局批准的网络连续型出版物（ISSN 2096-4188，CN 11-6037/Z），所以签约期刊的网络版上网络首发论文视为正式出版。

# 增强特征提取和解释的遥感图像语义分割模型

于攀琳，吴 旭，张凌云，刘子涵（成都理工大学 计算机与网络安全学院，四川 成都 610059）

摘 要：DeepLab $\mathrm { V } 3 +$ 是一种具有 Encoder-Decoder 结构的语义分割模型，因其逐像素分类的特点适用于处理遥感图像的土地覆盖分类问题。然而，下采样过程导致的特征图损失，会使连续大尺度地物的内部出现不连续无标签空洞区域，并且双线性插值算法会丢失分割边缘细节。针对上述问题，本研究提出了一种基于 DeepLab $\mathrm { V } 3 +$ 改进的 V3plus-EN-TC 模型。将骨干网络替换为特征提取能力更强的 EfficientNet，引入 SE 模块和倒置残差链接，增强 Encoder 对通道信息和多尺度空间信息的感知与提取能力；融合三个层次的特征，并且采用转置卷积和双线性插值结合的上采样方法，提高 Decoder 的特征解释能力，抑制空洞区域的出现，提高边缘精度；利用 DiceFocal 联合损失函数，解决样本分布不平衡问题，并且进一步聚焦混合像元。改进模型 V3plus-EN-TC 在预处理后的遥感数据集 GID 上，较 FCN、U-Net、SegNet、PSPNet、CBAM-DeepLab $\mathrm { V } 3 +$ 、CRF-DeepLab $\mathrm { V } 3 +$ 等模型，空洞区域显著减少，模型精度提升。改进模型的平均交并比、F1 分数、平均像素精度分别达到了 $84 . 7 4 \%$ 、 $8 8 . 3 9 \%$ 、 $8 6 . 6 4 \%$ 。

关键词：遥感图像；语义分割；deeplab $\mathbf { v } 3 +$ ；efficientnet；转置卷积；损失函数  
中图分类号：TP753  
doi:10.20165/j.cnki.ISSN1673-629X.2025.0039

# A semantic segmentation model for remote sensing images with enhanced

# feature extraction and interpretation

YU Pan-lin，WU Xu，ZHANG Ling-yun，LIU Zi-han

（College of Computer Science and Cyber Security, Chengdu University of Technology, Chengdu 610059, China) Abstract: DeepLab $\mathrm { V } 3 +$ is a semantic segmentation model with an Encoder-Decoder structure. Due to its characteristic of per-pixel classification, it is suitable for dealing with the land cover classification problem of remote sensing images. However, the loss of feature maps caused by the downsampling process will lead to the appearance of discontinuous unlabeled void areas inside continuous large-scale ground objects, and the bilinear interpolation algorithm will lose the details of segmentation edges. In response to the above problems, this study proposes a V3plus-EN-TC model improved based on DeepLab ${ \mathrm { V } } 3 + { \mathrm { ~ } }$ . The backbone network is replaced with EfficientNet, which has a stronger feature extraction ability. The SE module and inverted residual connections are introduced to enhance the Encoder's ability to perceive and extract channel information and multi-scale spatial information. Features at three levels are fused, and an upsampling method combining transposed convolution and bilinear interpolation is adopted to improve the feature interpretation ability of the Decoder, suppress the appearance of void areas, and improve the edge accuracy. The DiceFocal combined loss function is utilized to solve the problem of unbalanced sample distribution and further focus on mixed pixels. On the preprocessed remote sensing dataset GID, compared with models such as FCN, U-Net, SegNet, PSPNet, CBAM-DeepLab $\mathrm { V } 3 +$ , and CRF-DeepLab $\mathrm { V } 3 +$ , the improved model V3plus-EN-TC has significantly fewer void areas and improved model accuracy. The mean intersection over union, F1 score, and mean pixel accuracy of the improved model reach $8 4 . 7 4 \%$ , $8 8 . 3 9 \%$ , and $8 6 . 6 4 \%$ respectively.

Key words:remote sensing image; semantic segmentation; deeplab $\mathbf { v } 3 +$ ; efficientnet; transposed convolution; loss function

# 0 引言

精确的土地覆盖分类结果能够为生态经济发展和城市规划提供数据支持[1][2]。语义分割通常采用像素级（端到端）分割方法来处理遥感图像的土地覆盖分类任务[3]。在深度学习出现之前，研究人员普遍采用机器学习方法来处理分割任务[4][5]，但这些方法往往忽略了遥感图像的空间复杂性以及通道相关性。例如，草地上不同植物物种可能表现出光谱差异性，而草地和森林可能表现出光谱相似性，这一现象在遥感领域被称为“同类不同谱，同谱不同类”。为了打破机器学习的局限性，研究人员提出使用深度学习技术进行遥感图像土地覆盖分类任务，如多层感知器（MLP）、卷积神经网络（CNN）、循环神经网络（RNN）和 Transformer。这些方法具有更高的鲁棒性、容错性和特征处理能力[6]。特别是基于 CNN 的 U-Net、SegNet、PSPNet、DeepLab 等模型，由于它们强大的特征提取能力，被广泛应用于遥感图像的处理[7][8]。

近年来，DeepLab $\mathrm { V } 3 +$ 成为了语义分割领域的研究热点之一。DeepLab $\mathrm { V } 3 + \mathrm { [ 9 ] }$ 整体采用 Encoder-Decoder 架构，并使用深度可分离卷积来加速训练过程，引入空洞空间金字塔池（Atrous SpatialPyramid Pooling, ASPP）模块，通过不同的空洞率的组合来增强对不同尺度特征的识别能力。图像首先经过 Encoder 的深度卷积神经网络（DCNN）和 ASPP 模块提取多尺度特征。输出再进入Decoder 与提取自 DCNN 的浅层特征 Concat 融合，使用双线性插值算法上采样恢复成原始图像大小。该模型在大多数语义分割数据集上的表现都优于其他模型[10][11]，但是其结构是针对一般图像的分割来设计的，想要在遥感图像上获得较高的精度，还需要对模型进一步优化。

Chen 等人[12]将深度可分离的 MobileNet 作为DeepLab $\mathrm { V } 3 +$ 的骨干网络，引入混合空洞卷积模块和注意力机制。Quan 等人[13]基于 DeepLab $\mathrm { V } 3 +$ 提出一种结合 U-Net 融合浅层特征的模型。Zheng 等人[14]基于注意力优化机制改进数据增强策略，并且优化 ASPP 模块空洞率的组合。上述改进考虑了模型复杂度、特征图分辨率、空间信息与通道信息的关联性、数据可靠性等影响模型精度的因素，证明增强了特征提取能力的模型能够达到更高的整体精度。然而，其分割结果中仍然存在空洞区域，以及粗糙的边缘像素划分问题。

为了实现更精确的土地覆盖分类，本研究提出一种基于 DeepLab $\mathrm { V } 3 +$ 模型改进的人工神经网络模型 V3plus-EN-TC。本研究的主要贡献包括：（1 ）提出一个增强 Encoder 特征提取能力和Decoder 解释能力的人工神经网络模型用于遥感图像语义分割任务；（2）采用更针对样本分布不平衡问题的联合损失函数来训练模型；（3）在预处理后的遥感数据集上比较其他模型与所提模型的精度衡量指标，数据结果表明所提模型在遥感图像上的表现较原模型有所提升。

# 1 方法

# 1.1 改进模型 V3plus-EN-TC

V3plus-EN-TC 整体架构如图 1 所示。本研究首先将骨干网络替换为 EfficientNet，使 Encoder具有更强的特征提取能力，并针对遥感图像的特点对模块进行优化。引入压缩奖励（Squeeze andExcitation, SE）机制，让模型着重关注携带重要信息的通道；骨干网络使用空洞卷积来增大感受野，利用倒置残差连接保持特征的维度。此外，提取出浅层和中间层特征参与后续计算。然后，在 Decoder 部分融合更多层级的特征图；采用转置卷积与双线性插值结合的方法对融合后的特征图进行上采样。使模型能够学习到更多尺度特征之间的联系，减少空洞区域的产生，细化边缘混合像元的分类，提高 Decoder 的特征解释能力。最后，选择 DiceFocal 联合损失函数作为模型训练的依据。Dice 损失减小样本中标签分布不平衡对模型训练过程的影响；Focal 损失让模型更关注不易分割的混合像元。

# 1.2 骨干网络

骨干网络的作用是从输入图像中学习和提取特征。原模型主干网络使用的是改进的 Xception，其可以被看作是带有残差连接的深度可分离卷积block 的堆叠。但是与传统的深度可分离卷积不同的是，Xception 交换了逐点卷积和逐通道卷积的顺序，先使用 $1 ^ { * } 1$ 卷积处理跨信道信息，再使用不同卷积核的组合处理空间信息。此外，骨干网络还 结 合 批 归 一 化 （ Batch Normalization, BN ） 和ReLU 激活函数，在一般图像上实现了超过 $8 5 \%$ 的平均交并比（MIoU）[15]，然而当其应用于遥感图像时，模型精度有所下降。问题主要在两个方面：本应该连续的大尺度地物内部出现了离散的小块无标签空洞区域；处于地物边缘的混合像元包含多个类别的地物，辨别这些像元的类别一直是土地覆盖分类任务中的一个挑战[16]。因此，在深度卷积层中学习不同

![](images/02f04a42cdf997a537a840dd1380f1f2cce3001f1f8113a22dbda6c1373b448f.jpg)  
图 1 V3plus-EN-TC 整体架构

![](images/94bac9e14f6d785a503d7ed5a17b5c61fab9e0f2a0e8c17fa80cf1e40691d0dc.jpg)  
图 2 MBConv block 结构图

![](images/66461551b921bf4704e985278cb6613eb2ed8867b6533d51269b9d48748adf17.jpg)  
图 3 SE 模块计算过程

尺度的特征对于增强分割性能至关重要，骨干网络提取不同尺度特征的能力是影响模型精度的关键因素之一[17]。

目前国内外研究中 DeepLab V3 常用的主干网络 有 ResNet 和 MobileNet 。 赵 玉 刚 等 人 [18] 以ResNeSt 作为主干，获得了更高的分割精度。马静等人[19]将 MobileNet 作为主干，减少训练时间和模型计算量。近年来，EfficientNet[20]因其强大的信息提取能力，也逐渐被应用于遥感图像的处理。梁 伟 [21] 将 YOLO-V4 的 主 干 网 络 替 换 为EfficientNet 来优化小目标的检测精度，刘浪等人[22]采用 EfficientNet-B0 作为主干网络来检测不同场景下的舰船，这些模型在遥感数据集上均获得

了比原模型更好的结果。

本研究为了提高原模型在遥感数据集上的表现，将原骨干网络替换为 EfficientNet。该网络结合 ResNet 的残差连接和 MobileNet 的深度可分离卷积，主要组成部分是倒置线性瓶颈（MBConv）block，其结构如图 2 所示，减少了参数量和浮点运算量。该 block 引入空洞卷积、 SE 模块，以及倒置残差连接。空洞卷积增大感受野，降低多重卷积核带来的计算开销，并且不同空洞率的组合，让 DCNN 可以提取到多尺度特征。SE 模块是一种通道注意力机制，给各个通道分配权重，使模型能够更关注重要的通道。其计算过程如图 3所示，先用卷积 $F _ { t r }$ 调整特征图的宽、高和通道数（分别由W 、 $H$ 和 $C$ 表示）；再由全局平均池化$F _ { s q }$ 将特征映射压缩成长度为 $C$ 的包含上下文信息的向量；然后由两个全连接层组成的 $F _ { e x }$ ，先将向量的 $C$ 个通道压缩为 $\frac { C } { r }$ 个通道，再使用 Sigmoid激活函数，并将通道数恢复为 $C$ 以获得每个通道的注意力权重；最后 $F _ { s c a l e }$ 将权重应用于每个通道的特征，从而获得输出。一般残差连接是先降维再升维，倒置残差连接是先升维再降维，可以在解决梯度消失和爆炸问题的同时，维持特征的维度和多样性。这些模块的加入目的是使 DCNN 具有更强的多尺度特征感知能力和特征提取能力[23]。

# 1.3 结合转置卷积与双线性插值的上采样

上采样是将图像放大的过程，常用的上采样方法包括插值法、转置卷积、反池化等。插值法计算量小于其他两种方法，是人工神经网络中最常用的上采样方法[24]。DeepLab $\mathrm { V } 3 +$ 的 Decoder 使用双线性插值将特征图还原成输入的大小，但是双线性插值会使上采样后的图像边缘变得模糊，增大模型分割混合像元的难度。转置卷积与反池化都是基于数据的方法，能够充分考虑到空间和通道信息，而转置卷积比反池化更能注意到复杂的细节特征，更适用于处理遥感图像。因此，本文选择转置卷积作为 Decoder 的第二次上采样操作，将融合多层级特征后的特征图还原至输入大小。此外，本研究增加中间层特征参与融合，提供更多层级的空间特征。原模型融合骨干网络的浅层特征图和 ASPP 输出的特征图，Decoder 无法解译出足够的信息，改进后的 Decoder 一共融合三种特征图：从 EfficientNet 提取的 $2 5 6 \times 2 5 6 \times 4 8$ 浅层特征和 $6 4 \times 6 4 \times 4 8$ 的中间层特征，以及 ASPP 输出的 $1 6 { \times } 1 6 { \times } 2 5 6$ 的特征图。先使用双线性插值全部上 采 样 到 统 一 大 小 ， 然 后 沿 通 道 拼 接 成$2 5 6 { \times } 2 5 6 { \times } 3 5 2$ 的新特征图。最后使用转置卷积处理该特征图，将尺寸扩大到同输入图像一致。转置卷积输出的计算公式如下：

$$
H _ { \mathrm { o u t } } = H _ { i n } ^ { \mathrm { \Delta * } } s t r i d e - 2 ^ { \mathrm { \ast } } p a d d i n g + \mathrm { k e r } n e l s i z e \mathrm { \quad } 
$$

$$
W _ { \mathrm { o u t } } = W _ { i n } \stackrel { * } { s } s t r i d e - 2 \stackrel { * } { * } p a d d i n g + \mathrm { k e r } n e l s i z e
$$

其中 $H _ { o u t }$ 、 $H _ { i n }$ 、 $\boldsymbol { W } _ { o u t }$ 、 ${ \cal { W } } _ { i n }$ 分别表示特征图的输出高度、输入高度、输出宽度、输入宽度。

# 1.4 DiceFocal 联合损失函数

损失函数的选择在模型训练过程中起着十分关键的作用，直接影响模型收敛的速度。首先，由于图像注释中存在背景像素，计算损失时背景像素会使损失值无效，并且遥感图像样本中普遍存在的类别分布不平衡的问题，本文选择 Dice 损失作为联合损失函数的一部分。该损失函数利用标注和预测结果的交集和并集进行计算，有效降低了背景像素的影响[25]。其次，为了让模型更好地分割容易被错分漏分的小尺度地物，本文也选择将 Focal 损失作为联合损失函数的一部分。该损失函数基于交叉熵，通过一个动态因子来降低训练过程中易分割样本的权重。DiceFocal 联合损失函数包括 Dice 损失和 Focal 损失两个部分：

$$
L _ { D i c e F o c a l } = L _ { D i c e } + L _ { F o c a l }
$$

Dice 损失计算公式如下：

$$
L _ { _ { D i c e } } = 1 - { \frac { 2 { \displaystyle \sum _ { i = 1 } ^ { N } } l _ { i } y _ { i } } { { \displaystyle \sum _ { i = 1 } ^ { N } } l _ { i } + { \displaystyle \sum _ { i = 1 } ^ { N } } y _ { i } } }
$$

Focal 损失计算公式如下：

$$
{ \cal L } _ { { } _ { F o c a l } } = - \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \left[ \alpha f _ { 1 } ( l _ { i } , y _ { i } ) + \left( 1 - \alpha \right) f _ { 2 } ( l _ { i } , y _ { i } ) \right]
$$

其中，公式（5）中的 $f _ { 1 }$ 、 $f _ { 2 }$ 计算公式分别是：

其中，的值决定两个损失函数所占的比重，本文根据经验将其设定为 0.2； $l _ { i }$ 表示标注类别，$\boldsymbol { y } _ { i }$ 表示预测类别； $\alpha$ 是 Focal 的动态因子，根据经验将其设定为 0.4。

# 2 实验设计

# 2.1 数据集介绍以及预处理

本研究所使用的数据集是由武汉大学制作的公开的高分辨率遥感数据集 $\mathrm { G I D } ^ { [ 2 6 ] }$ ，该数据集覆盖面广、分布广、空间分辨率高，专门针对土地覆盖分类任务。数据集中的图像由 GF-2 卫星拍摄，涵盖中国 60 多个城市，包含三个可见光通道（红、绿、蓝）和一个近红外通道（NIR）。GID包 含 150 幅 遥 感 图 像 ， 每 幅 图 像 的 尺 寸 为$7 3 0 0 { \times } 6 9 0 8$ 像素。还包含相对应的标注图像，标注了五个土地覆盖类型：

![](images/76655d857a40ac0fe03ecbc70c42680b3eace8d4171a0fadfdd6a43c9810f64f.jpg)  
图 4 原始图像和标注图像示例

农 田 （ Farmland ） 、 建 筑 物 （ Building ） 、 水 （Water）、森林（Forest）和草地（Meadow）。

图 4 展示了数据集中部分原始图像和标注图像。

本研究删除了原始数据中存在的空白边缘，然后随机裁剪成 $5 1 2 { \times } 5 1 2$ 的大小，一共裁剪出15000 组原图像和标注图像。为了减少 GPU 性能的浪费，从 15000 组中删除背景像素超过全图$50 \%$ 的组，最终得到 11269 组图像。其中，6000组作为训练集，3000 组作为验证集，2269 组作为测试集。由于随机裁剪图像中存在的重叠区域过多会影响模型的训练，本研究随机选择训练集中的数据进行图像增强， $2 5 \%$ 的图像水平翻转， $2 5 \%$ 的图像高斯模糊， $2 5 \%$ 的图像顺时针旋转 $9 0 ^ { \circ }$ °。

在对数据集进行深入分析时发现，建筑物样本数量相对较少，而森林和草地样本数量相对较多，农田和水的样本数量处于中间水平。不同土地覆盖类型的样本分布呈现出明显的不均匀性，会导致模型在学习过程中对样本数量较多的类别产生偏向，而对样本数量较少的类别学习不够充分，从而影响模型对各类别土地覆盖的分类准确性和泛化能力。

# 2.2 模型训练设置

本文将初始学习率设置为 0.007，训练过程中采用了等间隔余弦退火算法来降低学习率，直到达到最小值 0.00007。优化器使用带动量的 SGD，动量值设置为 0.75，减少训练时间，并且利用动量来防止模型收敛到局部最优值。

# 2.3 评价指标

为了定量地评价改进模型的分割精度，本研究使用 MIoU、F1 分数和平均像素精度（MPA）这 3 个评价指标。这些指标在混淆矩阵的基础上进行计算，需要知道预测为真的正样本TP 、预测为假的正样本 FP 、预测为真的负样本TN 和预测为假的负样本 $F \bar { N }$ 的值。

神经网络语义分割领域中，MIoU 是一个非常重要的指标。它是将预测结果和标注结果的交集除以每个类别的并集，然后将这些比率相加并计算平均值来获得 MIoU，计算公式如下：

$$
M I o U = \frac { 1 } { K } \sum _ { i = 1 } ^ { k } \frac { T P } { T P + F P + F N }
$$

其中 $\mathbf { k }$ 表示地物类别的数量，本文中 ${ \bf k } = 5$ 。

F1 分数是统计学中用于衡量二分类（或多任务二分类）模型精确度的指标，是模型的精确度和召回率的调和平均值，同时兼顾了分类模型的查准率和查全率，适用于样本不平衡的情况。本研究将像素分类的正确和错误视作二分类来计算F1 分数。F1 分数首先需要计算查准率和查全率，完整的计算公式如下：

$$
\mathrm { P r } e c i s i o n = \frac { T P } { T P + F P }
$$

$$
\mathrm { R e } c a l l = \frac { T P } { T P + F N }
$$

$$
F 1 = \frac { 2 ^ { * } \mathrm { P r } e c i s i o n ^ { * } \mathrm { R e } c a l l } { \mathrm { P r } e c i s i o n + \mathrm { R e } c a l l }
$$

像素精度（PA）是另一种基于混淆矩阵的计算，它表示的是特定类别的正确分类像素占像素总数的比例。MPA 是所有类别的像素精度的平均值，其计算公式如下：

$$
M P A = \frac { 1 } { K } \sum _ { i = 1 } ^ { k } \frac { T P + T N } { T P + T N + F P + F N }
$$

# 3 结果与分析

# 3.1 对比实验

本研究比较了八种语义分割模型在 GID 上的表现，FCN、U-Net、SegNet、PSPNet、CBAM-DeepLab V3+[27]、CRF-DeepLab $\mathrm { V } 3 { + } [ 2 8 ]$ ，以及本研究所提模型 V3plus-EN-TC。表 1 展示了每个模型的 MIoU、F1 分数和 MPA，表 2 列出了五种土地覆盖类型各自的 IoU。

表 1 不同模型分割结果评价指标  

<html><body><table><tr><td colspan="4">网络名称 MIoU (%)</td><td colspan="4">MPA（%)</td></tr><tr><td colspan="4">FCN 62.66</td><td colspan="4">69.72</td></tr><tr><td colspan="4">U-Net 67.63</td><td colspan="4">73.09</td></tr><tr><td colspan="4">SegNet 69.93</td><td colspan="4">76.96</td></tr><tr><td colspan="4">PSPNet 72.01</td><td colspan="4">78.52</td></tr><tr><td colspan="4">DeepLab V3+ 76.74 CBAM-</td><td colspan="4">82.53</td></tr><tr><td colspan="4">79.61 DeepLab V3+</td><td colspan="4">84.82</td></tr><tr><td colspan="4">CRF-DeepLab 77.12</td><td colspan="4">82.97</td></tr><tr><td colspan="4">V3plus-EN- 84.74 TC(our)</td><td colspan="4">88.39</td></tr><tr><td colspan="9">表2各个类别的IoU</td></tr><tr><td>网络名称</td><td colspan="4">Farm-Land</td><td colspan="4">Water Forest (%)</td></tr><tr><td></td><td colspan="4">(%)</td><td colspan="4">(%)</td></tr><tr><td>FCN</td><td colspan="4">60.18</td><td colspan="4">77.44 50.51</td></tr><tr><td>U-Net</td><td colspan="4">64.03</td><td colspan="4">72.78 68.23</td></tr><tr><td>SegNet</td><td colspan="4">64.78</td><td colspan="4">82.04 66.65</td></tr><tr><td>PSPNet</td><td colspan="4">71.26</td><td colspan="4">82.95 65.80</td></tr><tr><td>DeepLab V3+</td><td colspan="4">75.17</td><td colspan="4">86.12 66.24</td></tr><tr><td>CBAM- DeepLab V3+</td><td colspan="4">79.41</td><td colspan="4">90.59 68.15</td></tr><tr><td>CRF-DeepLab</td><td colspan="4">76.68</td><td colspan="4">85.26 67.05</td></tr><tr><td>V3+ V3plus-EN-</td><td>86.45</td><td></td><td colspan="4"></td><td>78.28</td></tr></table></body></html>

FCN 是较早提出的一种网络结构相对简单的方法，其性能较差。相比之下，U-Net 和 SegNet能够融合不同尺度特征，分割遥感图像可以获得更高的精确度。由于混合像元的复杂性，模型需要融合多尺度特征。PSPNet 引入空间金字塔（SPP）模块，提取更丰富的特征，改善空间分布感 知 。 DeepLab $\mathrm { V } 3 +$ 在 SPP 的启 发下 设计 了ASPP，该模块使用空洞卷积扩大感受野，并且当有充足的实验数据时，其网络结构更有助于拟合训练数据。CBAM-DeepLab $\mathrm { V } 3 +$ 在 DeepLab $\mathrm { V } 3 +$ 的主干网络引入混合注意力机制来提高 Encoder 对特征的识别和学习，CRF-DeepLab $\mathrm { V } 3 +$ 使用条件随机场 （ Conditional Random Feild ， CRF ） 辅 助Decoder 对特征的解释。二者的评价指标数据都优于原模型，证明了提高特征提取和解释的能力能够提升原模型在遥感图像上的表现。优化模型V3plus-EN-TC 既采用注意力机制来增强 Encoder的特征提取能力，又结合转置卷积获得了更强的Decoder 特征解释能力，最终分割结果优于其他模型，mIoU、F1 分数、MPA 分别达到了 $8 4 . 7 4 \%$ 、$8 8 . 3 9 \%$ 、 $8 6 . 6 4 \%$ ，其中，对建筑物和水体的分割精度最高，两种地物类型的 IoU 分别是 94.36%和$9 2 . 7 2 \%$

# 3.2 消融实验

为了验证所做改进的有效性，本研究进行了消融实验。表 3 列出的是原模型、两组消融模型、改进模型之间的对比。表 4 展示了每个类别的 IoU。其中，V3plus-EN 表示替换骨干网络为EfficientNet，V3plus-TC 表示使用结合转置卷积的上采样方法，V3plus-EN-TC 是最终改进模型。

由于 SE 模块、空洞卷积和倒置残差连接的引入增强了 Encoder 的特征提取和融合能力，新主干网络的表现优于其他骨干网络。但是经过反复实验发现由于网络层数过深，结构过于复杂，模型在训练过程中发生了过拟合，因此本研究去掉了block 中一部分 SE 模块，并且将 Drop 率设置为0.5，最终得到消融模型 V3plus-EN。表 3、表 4 可以看出该消融模型在原模型基础上有较好的提升。

此外，融合更多层次的特征图后，结合转置卷积和双线性插值两种方法对融合后的特征图进行上采样，得到的消融模型 V3plus-TC 也有比原模型更好的精度评价。

本研究改进模型 V3plus-EN-TC 的 MIoU、F1分数和 MPA，相较原模型分别增加了 $8 . 0 0 \%$ 、$5 . 8 6 \%$ 和 $6 . 5 2 \%$ ，说明提高 Encoder 的特征提取能力和 Decoder 的特征解释能力能够提升语义分割模型在遥感图像上的分割精度。

表 3 消融模型分割结果的 MIoU、F1-Score 和 MPA 比较  

<html><body><table><tr><td>网络名称</td><td>MIoU (%)</td><td>F1-Score (%)</td><td>MPA (%)</td></tr><tr><td>DeepLab V3+</td><td>76.74</td><td>82.53</td><td>80.12</td></tr><tr><td>V3plus-EN</td><td>81.68</td><td>85.38</td><td>84.01</td></tr><tr><td>V3plus-TC</td><td>78.59</td><td>84.22</td><td>81.19</td></tr><tr><td>V3plus-EN-TC(our)</td><td>84.74</td><td>88.39</td><td>86.64</td></tr></table></body></html>

表 4 消融模型的五种类别的 IoU 比较  

<html><body><table><tr><td>Farm-land 网络名称 (%)</td><td>Building Water (%) (%)</td><td>Forest (%)</td><td>Meadow (%)</td></tr><tr><td colspan="4">DeepLab</td></tr><tr><td>75.17 V3+</td><td>87.53</td><td>86.12 66.24</td><td>68.64</td></tr><tr><td>V3plus-EN 81.03</td><td>92.14 90.69 88.92</td><td>71.41</td><td>73.13</td></tr><tr><td>V3plus-TC 76.98 V3plus-EN-</td><td>87.21</td><td>65.00</td><td>73.84</td></tr><tr><td>86.45 TC(our)</td><td>94.36 92.72</td><td>71.89</td><td>78.28</td></tr></table></body></html>

![](images/a70c4140725049e484407657e5c0114097b8668a9bb9f25d4323c8a97142dcaa.jpg)  
3.3 损失曲线对比  
图 5 不同损失函数下的损失曲线

本文对比了改进模型在三种损失函数下的损失曲线，分别是 Dice、Focal，以及本文所使用的DiceFocal 联合损失函数。损失随 epoch 变化的曲线图如图 5 所示，其中红色曲线代表的是本文训练模型所使用的 DiceFocal 联合损失函数。蓝色的曲线表示使用 Dice 损失，由于它计算的是重叠区域，损失值变化很不稳定，200 个 epoch 之后值依然较大；绿色曲线表示使用 Focal 损失，收敛速度快于前者，损失值也更小；红色曲线表示使用

DiceFocal 联合损失函数，它比单独使用 Dice 或者

![](images/14050dc75f9db6d1da25351a18ad5f3449865e78ecfc3690a6055ea6060f49d1.jpg)  
图 6 分割结果对比

Focal 的效果都要更稳定，更早收敛，更趋近于理想训练结果。

# 3.4 分割结果对比

图 6 展示的是其他 DeepLab V3+优化模型与本研究所提模型的消融版本的分割结果，图中 a 列是CBAM-DeepLab ${ \mathrm { V } } 3 + { \mathrm { ~ } }$ 的结果， $\widehat { \mathbf { \phi } }$ 列 是 CRF-DeepLab${ \mathrm { V } } 3 + { \mathrm { ~ } }$ 的结果，c 列是原模型的结果，d 列是 V3plus-EN 的结果，e 列是 V3plus-TC 的结果，f 列是改进模型 V3plus-EN-TC 的结果。可以明显看出，原模型应用于遥感图像时存在较多空洞区域。a 列和 d列模型是在骨干网络中加入了能够结合通道信息和空间信息的注意力机制，相较原模型准确率有很大提升。b 列、c 列与 e 列分别是用条件随机场、双线性插值和转置卷积来解释融合后的特征，可以看出转置卷积更能还原小尺度的边缘细节。本研究所提模型的分割结果中，空洞现象显著减少，有效抑制大尺度地物中间空洞区域的出现，还展现出更优秀的边缘细节，尤其是在识别建筑物和水体方面。

法，并且使用 DiceFocal 联合损失函数进行训练。使模型具有更强的特征提取和解释能力，能够有效处理复杂的空间信息和通道信息，提高遥感图像语义分割的精度，减少大尺度地物内部空洞区域的出现。在 GID 遥感数据集上的实验结果证明了改进模型的有效性，MIoU、F1 分数和 MPA 分别达到$8 4 . 7 4 \%$ 、 $8 8 . 3 9 \%$ 和 $8 6 . 6 4 \%$ ，与原模型相比分别提高了 $8 . 0 0 \%$ 、 $5 . 8 6 \%$ 和 $6 . 5 2 \%$ 。

然而，本文提出的改进模型仍然存在进一步优化的空间。首先，模型用于其他领域数据集的泛化性能仍需评估。此外，该模型需要大量的训练时间，未来的研究可能会更加关注提高模型的训练速度。

# 参考文献：

# 4 结论

本研究基于 DeepLab ${ \mathrm { V } } 3 + { \mathrm { ~ } }$ 模型，提出了一个针对 遥 感 图 像 改 进 的 语 义 分 割 模 型 ， 用EfficientNet 替换 原本 的主 干网 络， 引入 SE 模块、空洞卷积、倒置残差连接，融合更多层次的特征，使用双线性插值和转置卷积结合的上采样方

[1] 汤泊川, 帕力旦·吐尔逊,柏洁馨, 等. 结合CNN 和 Transformer的遥感图像土地覆盖分类方法[J].微电子 学与计算机,2024,41(04):64-73.   
[2] Nalla, S., Totakura, M., Pidikiti, D., & Pranathi, K. Monitoring Urban Growth Using Land Use Land Cover Classification[J]. Lecture Notes in Networks and Systems, 2023, 615:275–283.   
[3] 林 云 浩 , 王 艳 军 , 李 少 春 , 等 . 一 种 耦 合 DeepLab 与 Transformer的农作物种植类型遥感精细分类方法[J]. 测绘学报,2024,53(02):353-366.   
[4] Moradkhani, K., & Fathi, A. Segmentation of waterbodies in remote sensing images using deep stacked ensemble model[J]. Applied Soft Computing, 2022, 124:109038.   
[5] Li, L., Zhu, Z., & Wang, C. Multiscale Entropy-Based Surface Complexity Analysis for Land Cover Image Semantic Segmentation[J]. Remote Sensing, 2023, 15(8):2192.   
[6] Cheng, X., Sun, Y., Zhang, W., Wang, Y., Cao, X., & Wang, Y. Application of Deep Learning in Multitemporal Remote Sensing Image Classification[J]. Remote Sensing, 2023, 15(15):3859.   
[7] Chen, B., Zou, X., Zhang, Y., Li, J., Li, K., Xing, J., & Tao, P. LEFORMER: A HYBRID CNNTRANSFORMER ARCHITECTURE FOR ACCURATE LAKE EXTRACTION FROM REMOTE SENSING IMAGERY. ICASSP, IEEE International Conference on Acoustics[J]. Speech and Signal Processing Proceedings, 2024, 1:5710–5714.   
[8] Liu, B., Wu, H., Bao, X., & Zhong, Z. LPCUNet:A Lightweight Pure CNN UNet for Efficient Urban Scene Remote Sensing Semantic Segmentation[C]// 2023 4th International Conference on Computer Vision. Image and Deep Learning. CVIDL:IEEE, 2023:57–61.   
[9] Chen, L.-C., Zhu, Y., Papandreou, G., Schroff, F., & Adam, H. Encoder-decoder with atrous separable convolution for semantic image segmentation[J]. Lecture Notes in Computer Science (Including Subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 2018, 11211:833–851.   
[10] Zhang, Y., Zhang, Y., & Zhang, Q. Semantic Segmentation of Traffic Scene Based on DeepLabv3+ and Attention Mechanism[C]// 2023 3rd International Conference on Neural Networks. Information and Communication Engineering. NNICE:IEEE, 2023:542– 547.   
[11] Sun, J., Zhou, J., He, Y., Jia, H., & Liang, Z. RLDeepLabv3 $+$ : A lightweight rice lodging semantic segmentation model for unmanned rice harvester[J]. Computers and Electronics in Agriculture, 2023,209:107823.   
[12] Hui C ,Yuanshou Q ,Xinyuan L , et al.An improved DeepLabv3 $^ +$ lightweight network for remote-sensing image semantic segmentation[J].Complex & Intelligent Systems,2023,10(2):2839-2849.   
[13] Quan, B., Liu, B., Fu, D., Chen, H., & Liu, X. Improved deeplabv3 for better road segmentation in remote sensing images[C]// Proceedings - 2021 International Conference on Computer Engineering and Artificial Intelligence, ICCEAI 2021,8(27):331–334.   
[14] Zheng, K., Wang, H., Qin, F., Miao, C., & Han, Z. An Improved Land Use Classification Method Based on DeepLab $\mathrm { V } 3 +$ Under GauGAN Data Enhancement[J]. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 2023, 16:5526 – 5537.   
[15] Guo, Q., & Xu, Y. Image semantic segmentation model based on CBAMUNet[C]// Proceedings of SPIE - The International Society for Optical Engineering, SPIE. 2024,13080:68-77.   
[16] Lu, K., Ma, Z., Huo, P., He, Z., Zhang, H., & Zheng, X. Mixed Pixel Saturability Based Area Estimation Model on Remote Sensing Image[C]// 2023 IEEE 6th International Conference on Pattern Recognition and Artificial Intelligence. Haikou. Hainan. China, PRAI 2023,8(18):751–757.   
[17] Hu, L., Zhou, X., Ruan, J., & Li, S. ASPP+-LANet: A Multi-Scale Context Extraction Network for Semantic Segmentation of High-Resolution Remote Sensing Images[J]. Remote Sensing, 2024, 16(6):1-13.   
[18] 赵玉刚, 刘文萍, 周焱, 等.基于注意力机制和改进 DeepLab $\cdot \mathrm { V } 3 +$ 的无人机林区图像地物分割方法[J].南京 林业大学学报(自然科学版),2024,48(04):93-103.   
[19] 马静,郭中华,马志强,等.基于轻量化的DeepLabV3 $^ +$ 遥 感图像地物分割方法[J].液晶与显示,2024,39(08):1001- 1013.   
[20] Tan, M., & Le, Q. V. EfficientNet: Rethinking model scaling for convolutional neural networks[C]// 36th International Conference on Machine Learning. Long Beach. CA. USA, ICML 2019, 7:10691–10700.   
[21] 肖振久,杨玥莹,孔祥旭.基于改进YOLOv4的遥感图像 目标检测方法[J].激光与光电子学进展,2023,60(06):407 -415.   
[22] 刘浪,刘国栋,刘佳.基于改进EfficientDet算法的可见光 遥感舰船目标检测[J].现代电子技术,2022,45(22):28- 32.   
[23] Yin, H., Yang, C., & Lu, J. Research on Remote Sensing Image Classification Algorithm Based on EfficientNet[C]// 2022 7th International Conference on Intelligent Computing and Signal Processing. Xi'an. Shaanxi. China, ICSP 2022,4(15):1757–1761.   
[24] Islam, R., Hossen, S., Ariful Islam, S. M., & Akter, S. BITLM: Bilinear Interpolation with Transfer Learning Model for Breast Cancer Classification[C]// 2023 6th International Conference on Electrical Information and Communication Technology. Khulna. Bangladesh, EICT 2023:1-5.   
[25] Ming, Q., & Xiao, X. Towards Accurate Medical Image Segmentation with Gradient-Optimized Dice Loss[J]. IEEE Signal Processing Letters, 2024, 31:191–195.   
[26] Tong, X.-Y., Xia, G.-S., Lu, Q., Shen, H., Li, S., You, S., & Zhang, L. Land-cover classification with highresolution remote sensing images using transferable deep models[J]. Remote Sensing of Environment, 2020,1:237.   
[27] Chang, H., Guo, S., Zhang, H., & Zhang, Y. Apple Planting Area Extraction B----ased on Improved DeepLab V3 $+ [ J ]$ . Nongye Jixie Xuebao/Transactions of the Chinese Society for Agricultural Machinery, 2023, 54:206–213.   
[28] Wang, Z., Fan, B., Tu, Z., Li, H., & Chen, D. Cloud and Snow Identification Based on DeepLab $\mathrm { V } 3 +$ and CRF Combined Model for GF-1 WFV Images[J]. Remote Sensing, 2022, 14(19):4880.