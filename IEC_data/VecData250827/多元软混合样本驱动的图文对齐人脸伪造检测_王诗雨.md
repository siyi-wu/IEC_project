中图法分类号：TP391. 41 文献标识码： A 文章编号：1006-8961（2025）05-1334-12

论文引用格式：Wang S Y，Feng C B，Liu C X and Jin Y S.  2025.  Multivariate and soft blending sample-driven image-text alignment for deepfakedetection.  Journal of Image and Graphics，30（5）：1334-1345（王诗雨，冯才博，刘春晓，金逸胜 .  2025.  多元软混合样本驱动的图文对齐人脸伪造检测 .  中国图象图形学报，30（5）：1334-1345）［DOI：10. 11834/jig. 240252］

# 多元软混合样本驱动的图文对齐人脸伪造检测

王诗雨，冯才博，刘春晓\*，金逸胜浙江工商大学计算机科学与技术学院，杭州 310018

摘 要： 目的　随着人脸图像合成技术的快速发展，基于深度学习的人脸伪造技术对社会信息安全的负面影响日益增长。然而，由于不同伪造方法生成的样本之间的数据分布存在较大差异，现有人脸伪造检测方法准确性不高，泛化性较差。为了解决上述问题，提出一种多元软混合样本驱动的图文对齐人脸伪造检测新方法，充分利用图像与文本的多模态信息对齐，捕捉微弱的人脸伪造痕迹。方法　考虑到传统人脸伪造检测方法仅在单一模式的伪造图像上训练，难以应对复杂伪造模式，提出了一种多元软混合的数据增广方式（multivariate and soft blending augmenta⁃tion，MSBA），促进网络同时捕捉多种伪造模式线索的能力，增强了网络模型对复杂和未知的伪造模式的检测能力。由于不同人脸伪造图像的伪造模式与伪造力度多种多样，导致网络模型真伪检测性能下降。本文基于MSBA 方式设计了多元伪造力度估计（multivariate forgery intensity estimation，MFIE）模块，有效针对不同模式和力度的人脸伪造图像进行学习，引导图像编码器提取更加具有泛化性的特征，提高了整体网络框架的检测准确性。结果　在域内实验中，与对比算法性能最好的相比，本文方法的准确率（accuracy，ACC）与AUC（area under the curve）指标分别提升$3 . 3 2 \%$ 和 $4 . 0 2 \%$ 。在跨域实验中，本文方法与6 种典型方法在5 个数据集上进行了性能测试与比较，平均AUC 指标提高 $3 . 2 7 \%$ 。消融实验结果表明本文提出的MSBA方式和MFIE模块对于人脸伪造检测性能的提升均有较好的表现。

结论　本文面向人脸伪造检测任务设计的 CLIP（contrastive language-image pre-training）网络框架大大提高了人脸伪造检测的准确性，提出的MSBA方式和MFIE模块均起到了较好的助力效果，取得了超越已有方法的性能表现。

关键词： 人脸伪造检测；图文对比预训练模型；多元软混合的数据增广方式（MSBA）；多模态交互；多元伪造力度估计（MFIE）

# Multivariate and soft blending sample-driven image-text alignment for deepfake detection

Wang　Shiyu，Feng　Caibo，Liu　Chunxiao\*，Jin　Yisheng School of Computer Science and Technology，Zhejiang Gongshang University，Hangzhou 310018，China

Abstract：Objective　With the rapid development of facial image synthesis technology，from simple image editing tech⁃收稿日期：2024-05-15；修回日期：2024-08-13；预印本日期：2024-08-20$*$ 通信作者：刘春晓 cxliu@mail.zjgsu.edu.cn

基金项目：浙江省自然科学基金项目（LY24F020004， LZ23F020004）；浙江省重点研发计划资助（2023C01039）；国家级大学生创新创业训练计 划 项 目（GJ202313003，GJ202313011，202410353033X，202410353058X）；浙 江 省 大 学 生 科 技 创 新 活 动 计 划 暨 新 苗 人 才 计 划 项 目（2024R408A037，2024R408A039）；浙江工商大学“数字 $^ +$ ”学科建设项目（SZJ2022B016）

Supported by：Natural Science Foundation of Zhejiang Province，China（LY24F020004，LZ23F020004）；Key R&D Program of Zhejiang Province， China（2023C01039）；National College Students Innovation and Entrepreneurship Training Program（GJ202313003，GJ202313011，202410353033X， 202410353058X）；Zhejiang University Students Science and Technology Innovation Activity Plan and New Talent Plan （2024R408A037， 2024R408A039）；Zhejiang Gongshang University“Digital+”Disciplinary Construction Management Project（SZJ2022B016）

niques to complex generative adversarial networks，people can easily create highly realistic fake facial images and videos， negatively impacting social information security.  The low accuracy of existing face forgery methods and their poor general⁃ ization capability can be attributed to the notable differences in data distribution among samples generated by various forg⁃ ery methods.  A method called“multivariate and soft blending sample-driven image-text alignment for Deepfake detection”， which fully utilizes the multimodal alignment of images and texts to capture subtle traces of face forgery，is proposed to address this challenge. Method　Considering that traditional face forgery detection methods are typically trained on singlemode forged images with complex forgery modes，the multivariate and soft blending augmentation（MSBA）method is intro⁃ duced.  Multivariate and soft blending images are generated by randomly mixing forged images of different forgery modes with various weights.  The network model learns to estimate the blending weights of each forgery mode and the forgery inten⁃ sity map from these images，enhancing the capability to capture multiple forgery clues simultaneously，thereby further improving the detection capabilities for complex and unknown forgery patterns.  The declining performance of the network model in distinguishing true and false detections is due to the diverse forgery modes and intensities present in the face forg⁃ ery images.  A multivariate forgery intensity estimation（MFIE）module based on the MSBA method is proposed to address this issue.  This module effectively learns from face forgery images with varying modes and intensities，guiding the image encoder to extract highly generalized features and improving the overall detection accuracy of the network framework.  The main contributions include the following：1）as the first to integrate the CLIP model into the face forgery detection task a multiple-and-soft-blending sample-driven image-text alignment network framework is proposed for face forgery detection， leveraging the multimodal information alignment of images and texts to substantially enhance detection accuracy.  2）A mul⁃ tivariate and soft blending augmentation（MSBA）method is introduced to enhance the capability of the network model to recognize various forgery patterns.  This method is utilized to synthesize multiple-and-soft-blending images encompassing complex forgery patterns.  Building upon the MSBA approach，a multivariate forgery intensity estimation（MFIE）module that guides the network model to deeply mine features related to forgery patterns and intensities within facial forgery images has been further developed.  The MSBA method and MFIE module，working in tandem，drive the backbone network to selectively extract targeted forgery cues from images that encompass a range of forgery patterns，thus enhancing the general⁃ ization and robustness of the model.  3）Experimental results demonstrate highly competitive performance in in-domain and cross-domain tests across datasets，including FaceForensics++ （FF $^ { + + }$ ），Celeb-DF，DeepFake detection challenge （DFDC），DeepFake detection challenge preview （DFDCP），Deepfake detection （DFD），and DeeperForensics-1. 0 （DFV1）.  In the experimental process，16 frames are extracted from each video for the training dataset and 32 frames for the testing dataset，with all images resized to $2 2 4 \times 2 2 4$ pixels and normalized to the range［0，1］before network input.  In the experimental setup，the network model is initialized using a pre-trained CLIP model with a $1 6 \times 1 6$ image patch size. For the training process，the AdaN optimizer is employed，setting the initial learning rate to 2E-5 and the batch size to 64. After 75 training epochs，a cosine annealing strategy is applied for 25 additional epochs，reducing the learning rate to 2E-7. The proposed method is implemented within the PyTorch framework and is trained using a single NVIDIA GeForce RTX 3090 GPU graphics card.  Based on previous work，the area under the ROC curve（AUC）and accuracy（ACC）metrics are primarily employed to evaluate the performance of the network. Result　In in-domain experiments，the approach has achieved a notable enhancement in performance metrics when compared with the most proficient existing methodologies. Specifically，a marked improvement of 3. $32 \%$ in ACC and a notable increase of 4. $02 \%$ in the AUC are observed.  In crossdomain experiments，the proposed method is tested and compared with six existing methods on the image level across five datasets，resulting in an average improvement of $3 . 2 7 \%$ in the AUC metric.  Ablation study results indicate that the pro⁃ posed MSBA method and the MFIE module both provide positive contributions to the enhancement of face forgery detection performance. Conclusion　The CLIP network framework is designed for the face forgery detection task，which substantially enhances the accuracy of detecting forged faces.  The proposed method of MSBA，coupled with the MFIE module，plays a crucial supportive role.  These contributions have led to performance gains that surpass those of existing methods.  The model parameters and computational complexity of the method are relatively high due to the use of large-scale languagevision models，which leads to certain limitations in response speed.  Future work will consider reducing the computational overhead of the model while maintaining or even further improving the accuracy and robustness of face forgery detection.

Key words：face forgery detection；contrastive language-image pre-training；multivariate and soft blending augmentation （MSBA）；multi-modal interaction；multivariate forgery intensity estimation（MFIE）

# 0　引 言

基于深度学习的人脸伪造技术不断升级，从简单的图像编辑技术到复杂的生成式对抗网络，人们可以轻松地创造出高度逼真的人脸伪造图像和视频。人脸信息是重要的生物钥匙，闸机验证、安全支付、金融服务、社交媒体领域均需要使用人脸识别技术进行身份验证。如果人脸信息被盗用，不仅对个人隐私和财产构成威胁，也可能被用于诈骗他人，为社会带来不小的安全隐患。因此，为了有效预防人脸伪造技术带来的负面影响，需要研究出一种高鲁棒性且高泛化性的人脸伪造检测方法。

为了应对上述问题，已经涌现出不少针对伪造人脸图像的检测方法（曹申豪 等，2022）。从伪造数据的角度来看，由于人脸图像伪造方法的不断更新，无法使用所有伪造方法生成的数据来训练网络模型，所以一些工作将研究重心放在人脸伪造样本制作上。Li等人（2020a）和Zhao等人（2021a）通过混合两幅不同人脸图像并对混合边界进行平滑处理（blended image，BI），从而得到更多伪造样本。Shio⁃hara 和 Yamasaki（2022）提 出 了 自 混 合 图 像（self-blended image，SBI）的方法。该方法利用微弱的颜色变换和仿射变换对真实人脸图像中的人脸区域进行抖动，再将抖动后的人脸区域与源图像混合来合成伪造样本，促使网络模型学习更加具有泛化性和鲁棒性的特征。

从网络模型的角度来看，由于不同伪造方法生成的样本之间的数据分布存在较大差异，普通的网络模型对训练数据容易产生过拟合现象，所以一些工作注重网络模型设计。Durall 等人（2020）提出了利用 DFT（discrete Fourier transform）平均对各频带的幅度进行分析，以挖掘人脸伪造的异常信息。Qian 等人（2020）利用 DCT（discrete cosine transform）提取频域信息并进行统计特征分析。Liu 等人（2021）利用DFT 中的相位谱弥补幅度谱的损失，以抑制高频特征，从而专注于局部伪造区域。Luo 等人（2021）利用 SRM（spatial rich model）操作提取图像中的高频噪声并与低频信息进行互补，从而促进网络模型进行人脸伪造检测。Liang 等人（2022）发现网络模型更倾向于关注内容信息而非伪造痕迹，设计解耦框架来去除内容信息，使网络模型关注伪造痕迹。Zhang 等人（2024）侧重于结合空间域和噪声域信息，通过特征融合和注意力机制来增强特定伪造模式的检测性能。冯才博等人（2024）结合图像块比较与残差图估计进行人脸伪造检测。Yan 等人（2023）采用多任务学习策略保证特定特征与常见伪造特征的解耦。一些方法（Chen 等，2023；Shi 等，2023）通过掩码预测学习通用或隐藏表示，增强网络模型捕获各种潜在伪造痕迹。一些方法（Huang 等，2023；Dong 等，2023）通过发现内部和外部人脸区域的身份不一致来检测伪造人脸。

总之，已有方法仍然存在检测准确率不高、网络泛化性能不足等问题。一方面，已有的人脸伪造检测方法在单一模式的伪造图像上进行训练，难以识别复杂的伪造模式，在面对未知伪造样本时泛化性能较差；另一方面，不同伪造方法的伪造模式和伪造力度多种多样，为网络模型的学习带来了巨大挑战，降低了网络模型的鲁棒性。

目前，预训练的视觉语言模型（vision-languagemodel，VLM），如 CLIP（contrastive language-imagepre-training）（Radford 等，2021）获得了大量关注，激发了目标检测、图像生成、图像伪造检测等多个领域的思考和创新（Liu 等，2023，2024）。Luo 等人（2024）的研究工作已经证明了应用预训练过的VLM 来改进具有通用视觉和文本表示的下游任务的巨大潜力，并且用于识别多模式任务的能力也十分出色，例如：Khan 和 Dang-Nguyen（2024）对现有 CLIP 模型进行适应性调整，以适用于通用的深度伪造检测。受到上述工作的启发，本文探索性地将VLM 引入人脸伪造检测任务。

本文主要贡献如下：1）首次将CLIP 模型引入人脸伪造检测任务，提出多元软混合样本驱动的图文对齐的人脸伪造检测网络框架，充分利用图像与文本的多模态信息对齐，大幅提高了人脸伪造检测的准确性。2）为了增强网络模型识别多种伪造模式的能力，提出一个多元软混合的数据增广方式（multi⁃variate and soft blending augmentation，MSBA），用 于合成包含复杂伪造模式的多元软混合图像。3）在MSBA 方式的基础上，设计了一个多元伪造力度估计（multivariate forgery intensity estimation，MFIE）模块，引导网络模型深度挖掘人脸伪造图像中与伪造模式和伪造力度相关的特征。

实验证明，本文方法提出的MSBA 方式和MFIE模块共同驱动骨干网络从包含不同伪造模式的图像中有针对性地提取对应的伪造线索，提高了网络模型的泛化性能和鲁棒性能。

# 1 本文方法

本文注意到CLIP 识别多模式样本的能力十分出色，这与人脸伪造检测算法需要识别不同模式伪造样本的目标一致。因此，本文基于CLIP 模型创新性地设计了一个多元软混合样本驱动的图文对齐人脸伪造检测网络框架，图像与文本的多模态信息对齐提升了网络模型的泛化性能与鲁棒性能。

同时，为了给网络模型提供更多不同模式的伪造样本，提高其对未知伪造模式的挖掘能力，本文提出了一种名为多元软混合的数据增广方式。该方式利用随机权重混合不同模式的伪造图像得到多元软混合样本，驱动网络模型从混杂的伪造图像中识别多种伪造模式并且挖掘细粒度的伪造线索，最终提高了网络模型的泛化性能。

# 1. 1 总体网络框架

目前，已有的人脸伪造检测方法只利用图像模态的特征来完成真伪检测任务，单一模态的特征限制了已有方法在泛化性能上的表现。基于上述考虑，本文探索性地将文本模态引入人脸伪造图像检测任务，有效利用文本编码器引导图像编码器提取更具有泛化性能的特征。图1 展示了本文多元软混合样本引导的图文对齐人脸伪造检测网络框架。

对于一幅输入图像，本文使用“The forgery typeof this fake face is $\{ { \mathrm { c l a s s } } \}$ ”语句 $\left( \boldsymbol { { \mathbf { \mathit { \Pi } } } } _ { t _ { \mathrm { c } } } \right)$ 进行描述，其中class 是 $\mathrm { F F + + }$ 数据集中4 种伪造方法和未知伪造方法 ，即 $C \in \{$ DeepFakes，NeuralTextures，FaceSwap，Face2Face，Unknow }，本文将 $t _ { \mathrm { c } }$ 输入CLIP 的文本编码器 $G _ { t } ( \cdot )$ 得到文本特征 $\pmb { F } _ { t } \in \mathbf { R } ^ { C \times d _ { t } }$ 。

![](images/8e74e7dff88355b4b4117261d2e63d558ad31207ff5e1e43321add2dc2e1bd82.jpg)  
图1　本文网络框架  
Fig. 1　Our network framework

接下来，将文本特征 $\boldsymbol { F } _ { t }$ 投影到图像特征空间中得到 $\pmb { F } _ { t } ^ { \prime } \in \mathbb { R } ^ { C \times d _ { v } }$ 。同时，输入人脸图像 $I$ 通过 CLIP 的图像块嵌入处理得到 $\pmb { F } _ { \mathrm { i m g } } \in \mathbf { R } ^ { ( N + 1 ) \times d _ { v } }$ 。之后，堆叠$F _ { \mathrm { i m g } }$ 和 $\boldsymbol { F } _ { t }$ 输入CLIP 的图像编码器 $G _ { v } \left( \cdot \right)$ ，得到对应的视觉特征 $\boldsymbol { F } _ { v }$ 。在图像编码器处理过程中，文本特征与图像特征一起输入建立多模态交互，促进网络模型深入挖掘与伪造模式相关的图像特征。

$$
\pmb { F } _ { t } ^ { \prime } = M I P \big ( \pmb { F } _ { t } \big ) , \pmb { F } _ { t } ^ { \prime } \in \mathbf { R } ^ { C \times d _ { v } }
$$

式中， $M I P ( \cdot )$ 为多模态交互投影， $\boldsymbol { F } _ { i } ^ { \prime }$ 为投影特征

另一方面，本文使用 $N$ 条语句 $t _ { \mathrm { f a k e } }$ 与输入图像的分类特征计算平均相似度，有效覆盖更广泛的语言表达方式以提高文本模态的多样性，从而提升整体框架的泛化性能与鲁棒性能。具体来说，将伪造文本 $t _ { \mathrm { f a k e } }$ 描述通过CLIP 的文本编码器 $G _ { \mathrm { t } } ( \cdot )$ 得到文本特征 $\pmb { F } _ { \mathrm { f a k e } } \in \mathbf { R } ^ { L \times d _ { t } }$ ，与图像编码器处理得到的分类特征 $\boldsymbol { F } _ { \mathrm { c l s } }$ 计算平均相似度，从而进行人脸图像真伪判别。

$$
\tilde { y } = \frac { 1 } { N } \sum _ { i = 1 } ^ { L } \frac { { F _ { \mathrm { f a k e } } \cdot F _ { \mathrm { c l s } } ^ { \mathrm { T } } } } { { \| { F _ { \mathrm { f a k e } } \| { \| { F _ { \mathrm { c l s } } } \| } }  } }
$$

$$
L _ { \mathrm { s i m } } = B C E \left( \tilde { y } , y \right)
$$

式中， $B C E ( \cdot )$ 是二值交叉熵损失函数， $y$ 和 $\tilde { y }$ 分别为输入图像的真伪标签和预测值， $L _ { \mathrm { s i m } }$ 为相似度损失。

# 1. 2　多元伪造力度估计模块

由于不同人脸伪造图像的伪造模式与伪造力度多种多样，导致网络模型真伪检测性能下降。基于上述问题，本文设计了多元伪造力度估计模块，有效针对不同模式和力度的伪造图像进行学习，引导图像编码器提取更加具有泛化性的特征，最终提高整体框架的检测准确性，如图 所示。

本文将图像块特征 $\boldsymbol { F } _ { p }$ 通过解码器处理后的得到解码特征 $\boldsymbol { F } _ { \mathrm { d e } }$ ，与文本特征 $\boldsymbol { F } _ { t }$ 之间计算余弦相似度，具体为

$$
S = { \frac { { \boldsymbol { F } } _ { t } \cdot { \boldsymbol { F } } _ { \mathrm { { d e } } } ^ { \mathrm { T } } } { \parallel { \boldsymbol { F } } _ { t } \parallel \parallel { \boldsymbol { F } } _ { \mathrm { { d e } } } \parallel } }
$$

将 相 似 度 分 数 $s$ 进 行 形 状 变 换 得 到$S ^ { \prime } \in \mathbf { R } ^ { C \times ( H / 1 6 ) \times ( W / 1 6 ) }$ ，并沿着第1 个维度使用softmax 函数得到每个通道预测的图像块级伪造力度图 $\tilde { \pmb { M } } _ { i }$ ，具体为

$$
\tilde { M } _ { i } = \frac { \mathrm { e } ^ { S _ { i , : = } ^ { \prime } } } { \displaystyle \sum _ { j = 1 } ^ { C } \mathrm { e } ^ { S _ { j , : = } ^ { \prime } } }
$$

接下来，将伪造模式特征 $\pmb { F } _ { m }$ 输入线性层得到每个伪造模式的权重 $\tilde { w } _ { i }$ ，并与每个通道相乘后求和获得最终预测的伪造力度图 $\tilde { \pmb { M } } _ { \mathrm { a l l } }$ ，具体为

$$
\tilde { \pmb { M } } _ { \mathrm { { a l l } } } = \sum _ { i = 1 } ^ { C } \tilde { \pmb { M } } _ { i } \times \tilde { \pmb { w } } _ { i }
$$

![](images/c49271dab62078a0cc44577fc3deaa04da93ebe13f4462ca5755d474794ca142.jpg)  
图2　多元伪造力度估计模块  
Fig. 2　Multivariate forgery intensity estimation module

# 1. 3　多元软混合样本增广

已有方法通常在单一模式的伪造图像上进行训练，难以面对复杂的伪造模式。考虑到上述问题，本文提出了多元软混合数据增广方式，通过随机权重混合不同伪造模式的伪造图像得到多元软混合图像，如图3所示，其中，伪造力度图乘以放大系数5进行突出显示。网络模型学习从多元软混合图像中估计每一种伪造模式的混合权重和伪造力度图，提高了同时捕捉多种伪造线索的能力，从而进一步增强人脸伪造检测的泛化性能。

首先，针对给定的真实图像 $\pmb { I } _ { \mathrm { r e a l } }$ 以及相应的伪造图像 $I _ { \mathrm { D F } }$ ， $\boldsymbol { I } _ { \mathrm { F F } }$ ， $\boldsymbol { I } _ { \mathrm { F S } }$ ， $\boldsymbol { I } _ { \mathrm { N T } }$ ，计算真实图像与每个伪造图像之间的伪造力度图，如图3（a）所示。

其次，为每个伪造力度图分配一个混合权重$w _ { 1 }$ ， $w _ { 2 }$ ， $w _ { 3 }$ ， $w _ { 4 }$ ，权重的总和为1。每种伪造模式伪造力度图与对应的权重相乘并求和得到多元软混合伪造力度图，如图 $3 ( \mathrm { b } )$ 所示。

最后，将真实图像与多元软混合伪造力度图之间进行相减，从而得到多元软混合图像，如图3（c）所示。多元软混合样本增广方式有效地整合了多个伪造图像的信息，提高了处理结果的准确性和鲁棒性。

![](images/c210ffcd4547e3306ebd57d1016714813b9503ba3f2df0c17efe9fa22483639e.jpg)  
图3　多元软混合样本增广  
Fig. 3　Multivariate and soft blending sample augmentation（ a）step 1：calculating the forgery intensity map；（b）step 2：synthesiz⁃ ing a multivariate and soft blending forgery intensity map；（c）step 3：synthesizing a multivariate and soft blending forgery images）

将多元软混合图像中不同伪造力度图的权重$w \in [ 0 , 1 ]$ 作为软标签，并在训练过程中对 $w$ 进行约束，具体为

$$
\tilde { w } = L i n e a r \big ( F _ { { \scriptscriptstyle m } } \big )
$$

$$
L _ { \mathrm { s o f t } } = \sum _ { i = 1 } ^ { C } B C E \big ( \tilde { w } _ { i } , w _ { i } \big )
$$

式中， $\boldsymbol { F } _ { \mathrm { { m } } }$ 为伪造模式特征， $\ L i n e a r ( \cdot )$ 为线形层， $\boldsymbol { w } _ { i }$ 和$\tilde { w } _ { i }$ 分别为第 $i$ 个伪造模式的伪造力度图权重和伪造模式的伪造力度图权重预测， $B C E ( \cdot )$ 是二值交叉熵损失函数， $L _ { \mathrm { s o f t } }$ 为软标签损失。

# 1. 4　多任务学习策略

# 1. 4. 1　多元伪造力度估计损失

由于伪造力度图缺乏规律性的模式，网络模型难以估计。为此，本文在图像块层面对伪造力度图进行量化，降低网络模型的学习难度。对于一幅输入的多元伪造力度图 $\pmb { M } _ { \mathrm { a l l } } ^ { \mathrm { i m g } } \in \mathbf { R } ^ { H \times \pmb { \nu } }$ ，量化的具体步骤如下：

首先，沿着通道维度进行求和，并将非零值设置为1。接下来，聚合每个图像块内所有像素信息，即对于每个图像块的像素值进行求和并除以一个图像块内的像素数量，得到最终的图像块级人脸伪造力度图 ${ \pmb M } _ { \mathrm { a l l } } ^ { \mathrm { p a t c h } } \in { \bf R } ^ { ( H / 1 6 ) \times ( W / 1 6 ) }$

多元伪造力度估计损失 $L _ { \mathrm { e s } }$ 由多元伪造力度估计模块的预测图 $\tilde { \pmb { M } } _ { \mathrm { a l l } }$ 与图像块级人脸伪造力度图${ \pmb { M } } _ { \mathrm { a l l } } ^ { \mathrm { p a t c h } }$ 通过计算得到，具体为

$$
L _ { \mathrm { e s } } = \left\| \tilde { \pmb { M } } _ { \mathrm { a l l } } - \pmb { M } _ { \mathrm { a l l } } ^ { \mathrm { p a t c h } } \right\| _ { 1 }
$$

# 1. 4. 2 总体损失函数

本文同时使用平均相似度损失 $L _ { \mathrm { s i m } } ,$ 、软标签损失$L _ { \mathrm { s o f t } }$ 和多元伪造力度估计损失 $L _ { \mathrm { e s } }$ 作为模型的损失函数。总的损失函数为

$$
L _ { \mathrm { a l l } } = \lambda _ { 1 } L _ { \mathrm { s i m } } + \lambda _ { 2 } L _ { \mathrm { s o f t } } + \lambda _ { 3 } L _ { \mathrm { e s } }
$$

式中， $\lambda _ { 1 } , \lambda _ { 2 }$ ， $\lambda _ { 3 }$ 分别为相似度损失 $L _ { \mathrm { s i m } }$ 、软标签损失$L _ { \mathrm { s o f t } }$ 和多元伪造力度估计损失 $L _ { \mathrm { e s } }$ 的权重系数。

# 2 实验与分析

# 2. 1 数据集

# 2. 1. 1 训练数据集

本文选择 ${ \mathrm { F F } } { + } { + }$ （FaceForensics $^ { + + }$ ）（Rössler 等 ，2019）作为训练数据集，它包含 1 000 个从 YouTube采集的真实视频，以及使用这1 000个真实视频制作的4 000 个伪造视频。伪造视频由4 种方法制作：1）DF（DeepFake）（Tora，2019）；（2）FS（FaceSwap）（Kowalski，2016）；（3） FF（Face2Face）（Thies 等 ，2016）；（4）NT（NerualTexture）（Thies 等，2019）。同时，该数据集提供了3 个不同质量等级的版本：未压缩（raw）、轻压缩（C23）和重压缩（C40）。本文使用C23 版本的 ${ \mathrm { F F } } { + } { + }$ 作为训练数据，并按照8∶1∶1 的比例划分训练集、验证集和测试集。

# 2. 1. 2 测试数据集

除了从 $\mathrm { F F } { + } { + }$ 中划分的测试数据集，本文的测试数据由5个公开数据集组成：CDF（Celeb-DF）（Li等，2020b）、DFDC（deepfake detection challenge）（Dol⁃hansky 等 ，2020）、DFDCP（deepfake detection chal⁃lenge preview）（Dolhansky 等 ，2019）、DFD（deepfakedetection）（Nick 和 Andrew，2019）和 DFV1（DeeperFo⁃rensics-1. 0）（Jiang 等，2020）。

# 2. 2 实验设置

对于训练数据集，每个视频提取16 帧图像，对于测试数据集，每个视频提取32 帧图像。所有图像的尺寸均为 $2 2 4 \times 2 2 4$ 像素，并在输入网络前归一化到［0，1］之间。

本文使用预训练的CLIP 来初始化网络模型，图像块大小为 $1 6 \times 1 6$ ，选择Adan（Xie 等，2024）优化器来训练网络模型，初始学习率与批处理大小分别设置为2E-5 与64，在训练75 轮之后再使用余弦退火策略训练25 轮，下调学习率至2E-7。超参数 $\lambda _ { 1 } , \lambda _ { 2 }$ 和 $\lambda _ { 3 }$ 均默认设置为1。本文方法在Pytorch框架下实现，使用一张 NVIDIA GeForce RTX 3090 GPU 显卡进行训练。参考已有工作，本文主要使用AUC（areaunder the curve）和准确率（accuracy，ACC）指标来评估网络性能。

# 2. 3 实验结果

# 2. 3. 1 域内实验结果

本文在 ${ \mathrm { F F } } { + } { + }$ 高质量和低质量数据集上进行对比实验，评估本文方法的域内性能。表1 展示了不同方法在 $\mathrm { F F + + }$ 数据集同一视频压缩质量上进行训练和测试的实验结果。

从表1 可以看出，在两种视频质量设置下，本文方法的各项评价指标均优于所有对比方法。尤其在检测难度较高的 $\mathrm { F F } { + } + \left( \mathrm { C } 4 0 \right)$ 数据集上，与目前最优方法相比，本文方法的准确率与AUC 指标分别提高了 $3 . 3 2 \%$ 和 $4 . 0 2 \%$ 。这主要得益于多元软混合数据增广和多元伪造力度估计模块相互配合，充分挖掘不同伪造模式的痕迹特征。即便在伪造痕迹严重抹除的低质量数据上，本文方法也能较好处理。

表1　不同方法在 $\mathbf { F } \mathbf { F } { + } { + }$ 数据集上的测试结果  
Table 1　Test results of different methods on $\mathbf { F F + + }$ dataset $1 \%$   

<html><body><table><tr><td rowspan="2">方法</td><td colspan="2">C40</td><td colspan="2">C23</td></tr><tr><td>ACC</td><td>AUC</td><td>ACC</td><td>AUC</td></tr><tr><td>Face X-ray(Li等,2020a)</td><td>1</td><td>61.60</td><td>1</td><td>87.35</td></tr><tr><td>Two Branch(Masi等,2020)</td><td>1</td><td>85.59</td><td>1</td><td>98.70</td></tr><tr><td>Xception(Rossler等,2019)</td><td>81.75</td><td>1</td><td>95.73</td><td>1</td></tr><tr><td>Multi-attentional(Zhao 等,2021b)</td><td>88.69</td><td>90.40</td><td>97.60</td><td>99.29</td></tr><tr><td>M2TR(Wang等,2022a)</td><td>92.89</td><td>95.31</td><td>97.93</td><td>99.51</td></tr><tr><td>LiSiam(Wang等,2022b）</td><td>87.81</td><td>91.44</td><td>96.51</td><td>99.13</td></tr><tr><td>SFDG（Wang等,2023）</td><td>92.28</td><td>95.98</td><td>98.19</td><td>99.53</td></tr><tr><td>GRnet(Guo等,2023a)</td><td>96.68</td><td>1</td><td>97.47</td><td>1</td></tr><tr><td>本文</td><td>100</td><td>100</td><td>100</td><td>100</td></tr></table></body></html>

注：加粗字体为每列最优值，“-”表示未提供该指标结果。

# 2. 3. 2　跨域实验结果

在现实场景中，检测算法通常需要面对未知伪造类型的图像。为了评估本文方法在处理未知伪造类型图像时的泛化能力，本文在 ${ \mathrm { F F } } { + } { + }$ （C23）数据集上进行训练，在其他不同的公开数据集上进行交叉测试。

在图像层面，本文方法与6 种已有方法进行比较，结果如表 2 所示。本文方法在 CDF、DFDC、

DFDCP、DFV1 和DFD 数据集上进行测试，相较于已有性能最好的方法，平均AUC指标提高了 $3 . 2 7 \%$ 。

Table 2　Test results of different methods on the cross-dataset in frame-level   

<html><body><table><tr><td rowspan="2">方法</td><td colspan="6">AUC/%</td></tr><tr><td>CDF</td><td>DFDC</td><td>DFDCP</td><td>DFV1</td><td>DFD</td><td>平均</td></tr><tr><td>CORE(Ni等,2022)</td><td>74.28</td><td>70.49</td><td>73.41</td><td>84.75</td><td>80.18</td><td>76.62</td></tr><tr><td>F3Net(Qian等,2020)</td><td>73.52</td><td>70.21</td><td>73.54</td><td>84.31</td><td>79.75</td><td>76.26</td></tr><tr><td>Xception(Rossler等,2019)</td><td>73.65</td><td>70.77</td><td>73.74</td><td>83.30</td><td>81.63</td><td>76.64</td></tr><tr><td>SPSL(Liu等,2021)</td><td>76.50</td><td>70.40</td><td>74.08</td><td>87.67</td><td>81.22</td><td>77.97</td></tr><tr><td>UCF(Yan等,2023)</td><td>75.27</td><td>71.91</td><td>75.94</td><td>82.41</td><td>80.74</td><td>77.25</td></tr><tr><td>SRM(Luo,2021)</td><td>75.52</td><td>69.95</td><td>74.08</td><td>86.38</td><td>81.20</td><td>77.42</td></tr><tr><td>本文</td><td>78.18</td><td>72.98</td><td>75.56</td><td>88.55</td><td>90.93</td><td>81.24</td></tr></table></body></html>

注：加粗字体表示各列最优结果。

在视频层面，本文方法与7种已有方法进行比较，结果如表3所示。本文在视频层面的测试方法为从每个测试视频抽取32帧，对网络输出的32 个分数取平均值作为对该视频的预测结果。相较于已有性能最好的方法，本文方法在CDF、DFDC 与DFD数据集上的AUC指标分别提高了 $1 . 1 3 \% . 5 . 4 7 \%$ 与 $1 . 7 9 \%$ 。

# 2. 3. 3 鲁棒性测试

考虑到鲁棒性也是人脸伪造检测器的一个重要

# 表3　不同方法在视频层面上的交叉数据集的测试结果

表2　不同方法在图像层面上的交叉数据集的测试结果  
Table 3　Test results of different methods on the cross-dataset in video-level   

<html><body><table><tr><td rowspan="2">方法</td><td colspan="3">AUC/%</td></tr><tr><td>CDF</td><td>DFDC DFDCP</td><td>DFD</td></tr><tr><td>Face X-ray(Li等,2020a)</td><td>80.58</td><td>1 80.92</td><td>95.40</td></tr><tr><td>ADAL(Li等,2023)</td><td>84.62</td><td>1 78.51</td><td>92.14</td></tr><tr><td>Xception(Rossler等,2019）</td><td>73.70</td><td>70.90 1</td><td></td></tr><tr><td>CORE(Ni等,2022）</td><td>75.71</td><td>1 71.41</td><td>94.09</td></tr><tr><td>GS(Guo等,2023b)</td><td>84.97</td><td>81.65</td><td></td></tr><tr><td>SFDG(Wang等,2023)</td><td>75.83</td><td>73.63</td><td>88.00</td></tr><tr><td>IID(Huang等,2023）</td><td>83.80</td><td>1 81.23</td><td>93.92</td></tr><tr><td>本文</td><td>86.10</td><td>76.37 80.38</td><td>97.19</td></tr></table></body></html>

注：加粗字体表示各列最优结果。“-”表示原文献未提供该指标结果。

特性，在 $\mathrm { F F } { + } { + }$ （C23）测试集上对本文算法进行了5 种扰动类型的评估，并与提供模型权重的现有工作 DeepFidelity（Peng 等 ，2023）、SBI（self-blendedimages）（Shiohara 和 Yamasaki，2022）、UCF（Yan 等，2023）进行了比较。这5 种扰动方法包括颜色饱和度变化、颜色对比度变化、高斯模糊、高斯噪声和JPEG压缩，每种扰动方式都分为5个级别进行扰动。如图4所示，可以看到本文方法在鲁棒性方面优于现有的最先进方法，即使是在JPEG压缩上也是如此。

# 2. 4 消融实验

# 2. 4. 1 各模块的有效性评估

本文网络框架主要包括多元软混合样本增广和多元伪造力度估计模块两个创新设计。为了测试各模块对于模型性能的影响，使用 CDF、DFDC、DFDCP、DFV1 和DFD 数据集开展消融实验以评估每个模块的有效性。表4 展示了不同参数设置下的交叉数据集测试AUC指标。使用了MSBA方式训练的网络模型在多种数据集测试，平均AUC 指标提升$2 . 2 9 \%$ ，表明该增广方式提高了网络模型同时捕捉多种伪造线索的能力，从而达到更好的检测效果。相比仅使用MSBA 方式的方案，增加了MFIE 模块的方案在多种数据集测试，平均 AUC 指标提升$3 . 1 3 \%$ ，表明该模块可以有效增强网络模型对于各种伪造模式和力度的识别，引导网络模型提取更加具有泛化性的特征，因此最终性能有所提升。

![](images/757a8b6a4901ec32cfdb4b69625c4e89e5510d038bec47bf0785c84076b913f5.jpg)  
图4　鲁棒性测试

Table 4　Evaluation of the validity of each module   

<html><body><table><tr><td rowspan="2">MSBA MFIE</td><td colspan="5">AUC/%</td></tr><tr><td>CDF</td><td>DFDC DFDCP</td><td>DFV1</td><td>DFD</td><td>平均</td></tr><tr><td>1</td><td>1</td><td>85.85 71.89</td><td>72.11</td><td>90.76</td><td>87.49 81.42</td></tr><tr><td>1</td><td>√</td><td>86.12 75.18</td><td>78.74</td><td>90.43 88.08</td><td>83.71</td></tr><tr><td>√</td><td>√</td><td>86.10 76.37</td><td>80.38</td><td>94.19</td><td>97.19 86.84</td></tr></table></body></html>

注：加粗字体表示各列最优结果。“√”和“-”表示使用和未使用。

# 2. 4. 2　语句 $t _ { \mathrm { f a k e } }$ 数量对网络性能的影响

为了验证语句 $t _ { \mathrm { f a k e } }$ 数量对实验表现的影响，本文选取不同的语句数量 $N$ 进行网络训练和性能测试，测试结果如表5 所示。综合来看，设置 $N = 1 6$ 时取得最佳性能。

# 表5　文本描述语句数量对于网络性能的影响

Fig. 4　Robustness evaluation   
Table 5　Effects of the number of text description statements on the network performance   

<html><body><table><tr><td rowspan="2">文本描述 语句数量 N</td><td colspan="5">AUC/%</td></tr><tr><td>CDF</td><td>DFDC</td><td>DFDCP</td><td>DFV1 DFD</td><td>平均</td></tr><tr><td>1</td><td>89.07</td><td>75.03</td><td>78.16</td><td>84.67</td><td>96.01</td><td>84.58</td></tr><tr><td>4</td><td>88.70</td><td>73.04</td><td>74.40</td><td>83.87</td><td>91.58</td><td>82.31</td></tr><tr><td>8</td><td>87.19</td><td>76.19</td><td>79.19</td><td>86.63</td><td>91.01</td><td>84.04</td></tr><tr><td>16</td><td>86.10</td><td>76.37</td><td>80.38</td><td>94.19</td><td>97.19</td><td>86.84</td></tr></table></body></html>

注：加粗字体表示各列最优结果。

# 2. 4. 3　损失函数权重系数对网络性能的影响

本文通过 $\lambda _ { 1 } , \lambda _ { 2 }$ 和 $\lambda _ { 3 }$ 来平衡平均相似度损失、软标签损失和多元伪造力度估计损失的重要性。实验结果如表6所示，3个损失函数的权重会对模型的

表现有不同的影响。当 $\lambda _ { \scriptscriptstyle 1 }$ ， $\lambda _ { 2 }$ ， $\lambda _ { 3 }$ 均为1 时，模型达到最佳表现，软标签损失和多元伪造力度估计损失共同引导网络模型进行训练。

表4　各模块的有效性评估  
表6　损失函数权重系数对网络性能的影响 Table 6　The effects of weights in loss function on the network performance   

<html><body><table><tr><td rowspan="2">入//λ</td><td colspan="5">AUC/%</td></tr><tr><td>CDF</td><td>DFDC</td><td>DFDCP DFV1</td><td>DFD</td><td>平均</td></tr><tr><td>1/1/1</td><td>86.10</td><td>76.37</td><td>80.38</td><td>94.19 97.19</td><td>86.84</td></tr><tr><td>1/10/1</td><td>84.02</td><td>71.09</td><td>73.96</td><td>81.43 94.37</td><td>80.97</td></tr><tr><td>1/1/10</td><td>85.76</td><td>77.35</td><td>78.93</td><td>85.87 82.35</td><td>82.05</td></tr><tr><td>1/10/10</td><td>86.51</td><td>75.72</td><td>79.20</td><td>82.22 94.80</td><td>83.69</td></tr></table></body></html>

注：加粗字体表示各列最优结果。

# 2. 4. 4　性能评估

为了对提出的模型进行性能评估，与其他3 种先进方法进行了比较，如表7 所示。评估结果表明，尽管本文模型在参数量（Params）上是最大的，但在AUC 上取得了最好效果，这凸显了模型在捕捉数据复杂性方面的优势。此外，本文模型在Flops（float⁃ing point operations per second）上排名第 2，这表明它在保持计算效率的同时，能够有效地利用其参数来提高性能。

# 2. 5　人脸伪造力度图可视化

为进一步证明本文方法增强了网络模型对于各种伪造模式和伪造力度的估计，图5（a）为输入图像，包括软混合、DF、FF、FS 和NT；图5（b）为输入图像对应的图像块级人脸伪造力度图；图5（c）为本文方法预测的图像块级人脸伪造力度图。从图5（b）

表7　性能评估Table 7 Performance evaluation  

<html><body><table><tr><td></td><td>Flops/G ↓</td><td>Params/M↓</td><td>AUC/% ↑</td></tr><tr><td>RECCE(Cao等,2022）</td><td>8.09</td><td>23.78</td><td>73.19</td></tr><tr><td>SRM(Luo等,2021)</td><td>13.81</td><td>53.24</td><td>75.52</td></tr><tr><td>UCF（Yan等,2023）</td><td>12.19</td><td>44.51</td><td>75.27</td></tr><tr><td>本文</td><td>11.55</td><td>57.65</td><td>78.18</td></tr></table></body></html>

注：加粗字体表示各列最优结果。“↑”表示值越大越好，“↓”表示值越小越好。

与图5（c）比较可以发现，本文方法受益于多元伪造力度估计模块对于伪造模式和伪造力度的挖掘能力，能够精准地挖掘到各种伪造模式的伪造痕迹。

![](images/1924b478eed93629a9a0d2c99f6f359257fe7478999b9e3cf886ebc1d7b84705.jpg)  
图5　多元伪造力度估计模块的输出结果  
Fig. 5　Output of the multivariate forgery intensity estimation module（ a）input images；（b）forgery intensity maps； （c）our method）

# 3　结 论

为了进一步提高网络模型在人脸图像伪造检测任务中的泛化能力，本文提出了多元软混合样本驱动的图文对齐人脸伪造检测方法。其中多元软混合的数据增广方式提高了网络模型同时捕捉多种伪造模式的能力，从而进一步增强人脸伪造检测的泛化性能。多元伪造力度估计模块有效针对不同模式和力度的伪造图像进行学习，引导图像编码器提取更加具有泛化性的特征，最终提高整体框架的检测准确性。实验结果表明本文方法在应对未知样本攻击时展现出比现有方法更强的鲁棒性能与泛化性能。

由于使用了大语言视觉模型，本文方法的模型参数量和计算复杂度较高，存在一定的响应速度局限性。在未来的工作中，将考虑在保持甚至进一步提高人脸伪造检测准确率和鲁棒性的情况下降低模型的计算开销。

# 参考文献（References）

Cao J Y，Ma C，Yao T P，Chen S，Ding S H and Yang X K. 2022. Endto-end reconstruction-classification learning for face forgery detec⁃ tion//Proceedings of 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition. New Orleans，USA：IEEE：4103-   
4112［DOI：10.1109/CVPR52688.2022.00408］ Cao S H，Liu X H，Mao X Q and Zou Q. 2022. A review of human face forgery and forgery-detection technologies. Journal of Image and Graphics，27（4）：1023-1038（曹申豪，刘晓辉，毛秀青，邹勤 .   
2022. 人脸伪造及检测技术综述. 中国图象图形学报， 27（4）：   
1023-1038）［DOI：10.11834/jig.200466］ Chen H，Lin Y Z，Li B and Tan S Q. 2023. Learning features of intraconsistency and inter-diversity：keys toward generalizable deepfake detection. IEEE Transactions on Circuits and Systems for Video Technology，33（3）：1468-1480 ［DOI：10.1109/TCSVT. 2022.   
3209336］ Dolhansky B，Bitton J，Pflaum B，Lu J K，Howes R，Wang M L and Ferrer C C. 2020. The deepfake detection challenge（DFDC）data⁃ set［EB/OL］.［2024-05-01］. https://arxiv.org/pdf/2006.07397.pdf Dolhansky B，Howes R，Pflaum B，Baram N and Ferrer C C. 2019. The deepfake detection challenge （DFDC） preview dataset ［EB/OL］. ［2024-05-01］. https://arxiv.org/pdf/1910.08854.pdf Dong S C，Wang J，Ji R H，Liang J J，Fan H Q and Ge Z. 2023. Implicit identity leakage：the stumbling block to improving deep⁃ fake detection generalization//Proceedings of 2023 IEEE/CVF Con⁃ ference on Computer Vision and Pattern Recognition. Vancouver， Canada： IEEE： 3994-4004 ［DOI： 10.1109/CVPR52729.2023.   
00389］ Durall R，Keuper M and Keuper J. 2020. Watch your up-convolution： CNN based generative deep neural networks are failing to repro⁃ duce spectral distributions//Proceedings of 2020 IEEE/CVF Confer⁃ ence on Computer Vision and Pattern Recognition. Seattle，USA： IEEE：7887-7896［DOI：10.1109/CVPR42600.2020.00791］   
Feng C B，Liu C X，Wang Y Y and Zhou Q D. 2024. Face forgery detec⁃ tion with image patch comparison and residual map estimation. Journal of Image and Graphics，29（2）：457-467（冯才博，刘春晓， 王昱烨， 周其当. 2024. 结合图像块比较与残差图估计的人脸伪 造检测 . 中国图象图形学报，29（2）：457-467）［DOI：10.11834/ jig.230149］   
Guo Y，Zhen C and Yan P F. 2023b. Controllable guide-space for gener⁃ alizable face forgery detection//Proceedings of 2023 IEEE/CVF International Conference on Computer Vision. Paris， France： IEEE：20761-20770［DOI：10.1109/ICCV51070.2023.01903］   
Guo Z Q，Yang G B，Chen J Y and Sun X M. 2023a. Exposing deepfake face forgeries with guided residuals. IEEE Transactions on Multime⁃ dia，25：8458-8470［DOI：10.1109/TMM.2023.3237169］   
Huang B J，Wang Z Y，Yang J F，Ai J X，Zou Q，Wang Q and Ye D P. 2023. Implicit identity driven deepfake face swapping detection// Proceedings of 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Vancouver，Canada：IEEE：4490-4499 ［DOI：10.1109/CVPR52729.2023.00436］   
Jiang L M，Li R，Wu W，Qian C and Loy C C. 2020. Deeperforensics-1.0： a large-scale dataset for real-world face forgery detection//Proceed⁃ ings of 2020 IEEE/CVF Conference on Computer Vision and Pat⁃ tern Recognition. Seattle，USA：IEEE：2886-2895 ［DOI：10. 1109/CVPR42600.2020.00296］   
Khan S A and Dang-Nguyen D T. 2024. CLIPping the deception：adapt⁃ ing vision-language models for universal deepfake detection//Pro⁃ ceedings of 2024 International Conference on Multimedia Retrieval. Phuket，Thailand：ACM：1006-1015 ［DOI：10.1145/3652583. 3658035］   
Kowalski M. 2016. FaceSwap［CP/OL］.［2024-05-01］. https://github.com/MarekKowalski/FaceSwap   
Li L Z，Bao J M，Zhang T，Yang H，Chen D，Wen F and Guo B N. 2020a. Face X-ray for more general face forgery detection//Proceed⁃ ings of 2020 IEEE/CVF Conference on Computer Vision and Pat⁃ tern Recognition. Seattle，USA：IEEE：5000-5009 ［DOI：10. 1109/CVPR42600.2020.00505］   
Li X，Ni R R，Yang P P，Fu Z Q and Zhao Y. 2023. Artifactsdisentangled adversarial learning for deepfake detection. IEEE Transactions on Circuits and Systems for Video Technology，33（4）： 1658-1670［DOI：10.1109/TCSVT.2022.3217950］   
Li Y Z，Yang X，Sun P，Qi H G and Lyu S W. 2020b. Celeb-DF：a large-scale challenging dataset for deepfake forensics//Proceedings of 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle，USA：IEEE：3204-3213 ［DOI：10.1109/ CVPR42600.2020.00327］   
Liang J H，Shi H F and Deng W H. 2022. Exploring disentangled con⁃ tent information for face forgery detection//Proceedings of the 17th European Conference on Computer Vision. Tel Aviv， Israel： Springer：128-145［DOI：10.1007/978-3-031-19781-9_8］   
Liu H，Liu X L，Tan Z C，Li X L and Zhao Y. 2024. PADVG：a simple baseline of active protection for Audio-driven video generation. ACM Transactions on Multimedia Computing， Communications and Applications，20（6）：#168［DOI：10.1145/3638556］   
Liu H，Tan Z C，Chen Q，Wei Y C，Zhao Y and Wang J D. 2023. Uni⁃ fied frequency-assisted transformer framework for detecting and grounding multi-modal manipulation［EB/OL］.［2024-05-01］. https://arxiv.org/pdf/2309.09667.pdf   
Liu H G，Li X D，Zhou W B，Chen Y F，He Y，Xue H，Zhang W M and Yu N H. 2021. Spatial-phase shallow learning：rethinking face forgery detection in frequency domain//Proceedings of 2021 IEEE/ CVF Conference on Computer Vision and Pattern Recognition. Nashville，USA：IEEE：772-781 ［DOI：10.1109/CVPR46437. 2021.00083］   
Luo Y C，Zhang Y，Yan J C and Liu W. 2021. Generalizing face forgery detection with high-frequency features//Proceedings of 2021 IEEE/ CVF Conference on Computer Vision and Pattern Recognition. Nashville， USA： IEEE： 16312-16321 ［DOI： 10.1109/ CVPR46437.2021.01605］   
Luo Z W，Gustafsson F K，Zhao Z，Sjölund J and Schön T B. 2024. Controlling vision-language models for multi-task image restoration ［EB/OL］.［2024-05-01］. https://arxiv.org/pdf/2310.01018.pdf   
Masi I，Killekar A，Mascarenhas R M，Gurudatt S P and AbdAlmageed W. 2020. Two-branch recurrent network for isolating deepfakes in videos//Proceedings of the 16th European Conference on Computer Vision. Glasgow，UK：Springer International Publishing：667-684 ［DOI：10.1007/978-3-030-58571-6_39］   
Ni Y S，Meng D P，Yu C Q，Quan C B，Ren D C and Zhao Y J. 2022. CORE：consistent representation learning for face forgery detec⁃ tion//Proceedings of 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. New Orleans，USA： IEEE：12-21［DOI：10.1109/CVPRW56347.2022.00011］   
Nick D and Andrew G. 2019. Contributing data to deepfake detection research［CP/OL］.［2024-05-01］. https://ai. googleblog. com/2019/09/contributing-data-to-deepfakedetection.html   
Peng C L，Guo H Q，Liu D C，Wang N N，Hu R M and Gao X B. 2023. DeepFidelity：perceptual forgery fidelity assessment for deepfake detection［EB/OL］.［2024-05-01］. https://arxiv.org/pdf/2312.04961.pdf   
Qian Y Y，Yin G J，Sheng L，Chen Z X and Shao J. 2020. Thinking in frequency：face forgery detection by mining frequency-aware clues// Proceedings of the 16th European Conference on Computer Vision. Glasgow，UK：Springer International Publishing：86-103 ［DOI： 10.1007/978-3-030-58610-2_6］   
Radford A，Kim JW，Hallacy C，Ramesh A，Goh G，Agarwal S，Sastry G，Askell A，Mishkin P，Clark J，Krueger G and Sutskever I. 2021. Learning transferable visual models from natural language supervision//Proceedings of the 38th International Conference on Machine Learning. Virtually：［s.n.］：8748-8763 Rössler A，Cozzolino D，Verdoliva L，Riess C，Thies J and Niessner M.   
2019. FaceForensics++ ： learning to detect manipulated facial images//Proceedings of 2019 IEEE/CVF International Conference on Computer Vision. Seoul，Korea（South）：IEEE：1-11［DOI：   
10.1109/ICCV.2019.00009］ Shi L，Zhang J and Shan S G. 2023. Real face foundation representation learning for generalized deepfake detection ［EB/OL］. ［2024-05-   
01］. https://arxiv.org/pdf/2303.08439.pdf Shiohara K and Yamasaki T. 2022. Detecting deepfakes with selfblended images//Proceedings of 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition. New Orleans，USA： IEEE：18699-18708［DOI：10.1109/CVPR52688.2022.01816］ Thies J，Zollhofer M，Stamminger M，Theobalt C and Nießner M. 2016. Face2face：real-time face capture and reenactment of RGB videos// Proceedings of 2016 IEEE Conference on Computer Vision and Pat⁃ tern Recognition. Las Vegas，USA：IEEE：2387-2395［DOI：10.   
1109/CVPR.2016.262］ Thies J，Zollhöfer M and Nießner M. 2019. Deferred neural rendering： image synthesis using neural textures. ACM Transactions on Graph⁃ ics，38（4）：#66［DOI：10.1145/3306346.3323035］ Tora M. 2019. Deepfakes［CP/OL］.［2024-05-01］. https://github.com/deepfakes/faceswap Wang J，Sun Y L and Tang J H. 2022b. LiSiam：localization invariance Siamese network for deepfake detection. IEEE Transactions on Information Forensics and Security，17：2425-2436 ［DOI：10.   
1109/TIFS.2022.3186803］ Wang J K，Wu Z X，Ouyang W H，Han X T，Chen J J，Jiang Y G and Li S N. 2022a. M2TR：multi-modal multi-scale transformers for deepfake detection//Proceedings of 2022 International Conference on Multimedia Retrieval. Newark，USA：ACM：615-623［DOI：10.   
1145/3512527.3531415］ Wang Y，Yu K，Chen C，Hu X Y and Peng S L. 2023. Dynamic graph learning with content-guided spatial-frequency relation reasoning for deepfake Detection//Proceedings of 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Vancouver，Canada： IEEE：7278-7287［DOI：10.1109/CVPR52729.2023.00703］   
Xie X Y，Zhou P，Li H，Lin Z C and Yan S C. 2024. Adan：adaptive nesterov momentum algorithm for faster optimizing deep models. IEEE Transactions on Pattern Analysis and Machine Intelligence， 46（12）：9508-9520［DOI：10.1109/TPAMI.2024.3423382］   
Yan Z Y，Zhang Y，Fan Y B and Wu B Y. 2023. UCF：uncovering com⁃ mon features for generalizable deepfake detection//Proceedings of 2023 IEEE/CVF International Conference on Computer Vision. Paris，France：IEEE：22355-22366［DOI：10.48550/arXiv.2304. 13949］   
Zhang D Y，Chen J H，Liao X，Li F，Chen J X and Yang G B. 2024. Face forgery detection via multi-feature fusion and local enhance⁃ ment. IEEE Transactions on Circuits and Systems for Video Tech⁃ nology， 34 （9） ： 8972-8977 ［DOI： 10.1109/TCSVT. 2024. 3390945］   
Zhao H Q，Wei T Y，Zhou W B，Zhang W M，Chen D D and Yu N H. 2021b. Multi-attentional deepfake detection//Proceedings of 2021 IEEE/CVF Conference on Computer Vision and Pattern Recogni⁃ tion. Nashville， USA： IEEE： 2185-2194 ［DOI： 10.1109/ CVPR46437.2021.00222］   
Zhao T C，Xu X，Xu M Z，Ding H，Xiong Y J and Xia W. 2021a. Learning self-consistency for deepfake detection//Proceedings of 2021 IEEE/CVF International Conference on Computer Vision. Montreal， Canada： IEEE： 15003-15013 ［DOI： 10.1109/ ICCV48922.2021.01475］

# 作者简介

王诗雨，女，本科生，主要研究方向为人脸伪造检测和暗水印技术。E-mail：2110080208@pop.zjgsu.edu.cn  
刘春晓，通信作者，男，副教授，主要研究方向为深度学习与计算机视觉、模式识别与智能系统。  
E-mail：cxliu@mail.zjgsu.edu.cn  
冯才博，男，硕士研究生，主要研究方向为人脸伪造检测、人脸匿名化和生成式神经网络。  
E-mail：2012190104@pop.zjgsu.edu.cn  
金逸胜，男，本科生，主要研究方向为人脸和语音的深度伪造检测。E-mail：rheinx@qq.com