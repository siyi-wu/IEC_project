# 《计算机工程》网络首发论文

题目： 音视频深度伪造与鉴伪综述  
作者： 魏方达，刘淼，孙毅，王晶，赵胜辉  
DOI： 10.19678/j.issn.1000-3428.00EC0070700  
网络首发日期： 2025-04-17  
引用格式： 魏方达，刘淼，孙毅，王晶，赵胜辉．音视频深度伪造与鉴伪综述[J/OL]．计算机工程. https://doi.org/10.19678/j.issn.1000-3428.00EC0070700

![](images/dbddfd9a21a82e8d9f8355c8ce8aca3b1d02d26ce42947c206246078cd751038.jpg)

网络首发：在编辑部工作流程中，稿件从录用到出版要经历录用定稿、排版定稿、整期汇编定稿等阶段。录用定稿指内容已经确定，且通过同行评议、主编终审同意刊用的稿件。排版定稿指录用定稿按照期刊特定版式（包括网络呈现版式）排版后的稿件，可暂不确定出版年、卷、期和页码。整期汇编定稿指出版年、卷、期、页码均已确定的印刷或数字出版的整期汇编稿件。录用定稿网络首发稿件内容必须符合《出版管理条例》和《期刊出版管理规定》的有关规定；学术研究成果具有创新性、科学性和先进性，符合编辑部对刊文的录用要求，不存在学术不端行为及其他侵权行为；稿件内容应基本符合国家有关书刊编辑、出版的技术标准，正确使用和统一规范语言文字、符号、数字、外文字母、法定计量单位及地图标注等。为确保录用定稿网络首发的严肃性，录用定稿一经发布，不得修改论文题目、作者、机构名称和学术内容，只可基于编辑规范进行少量文字的修改。

出版确认：纸质期刊编辑部通过与《中国学术期刊（光盘版）》电子杂志社有限公司签约，在《中国学术期刊（网络版）》出版传播平台上创办与纸质期刊内容一致的网络版，以单篇或整期出版形式，在印刷出版之前刊发论文的录用定稿、排版定稿、整期汇编定稿。因为《中国学术期刊（网络版）》是国家新闻出版广电总局批准的网络连续型出版物（ISSN 2096-4188，CN 11-6037/Z），所以签约期刊的网络版上网络首发论文视为正式出版。

# 音视频深度伪造与鉴伪综述

魏方达1，刘淼1，孙毅2，王晶1\*，赵胜辉1

（1.北京理工大学，信息与电子学院，北京 100081；2. 北京理工大学，网络空间安全学院，北京 100081）

摘  要：近年来，深度学习在计算机视觉以及语音信号处理等领域取得了重大成功。然而，深度学习的飞速发展也带来了负面影响，各类伪造视频，语音在网络上泛滥成灾，一些不法分子利用深度学习技术替换原始视频的人脸、编辑面部属性、合成说话人语音、克隆语音，通过制作色情视频、虚假新闻、政治谣言等造成社会动荡、混乱，威胁个人利益、国家安全。为了消除这些负面影响，众多学者从不同的角度提出解决方案。早期伪造主要集中在单模态伪造，因此，目前大多数解决方案侧重单模态的伪造识别问题，未能充分考虑音频和视频之间的内在联系，现有的单模态鉴伪方式在处理音频与视频均被伪造的情况时常常表现出次优的识别性能。近期，随着研究的深入，部分学者开始探索使用多模态模型鉴伪，取得了显著的成果。本综述回顾了视频、语音伪造及鉴伪技术，收集整理了视频、语音、音视频伪造数据集，并总结归纳了多模态鉴伪方法。最后对如今检测技术存在的问题和研究方向进行了分析并给出建议。

关键词：深度学习；伪造；鉴伪；多模态；音视频

# Review of Audio and Video Deep Fakes and Counterfeit Detection

Wei Fangda1, Liu Miao1, Sun $\mathrm { Y i } ^ { 2 }$ , Wang Jing1\*, Zhao Shenghui1 （1.Beijing Institute of Technology, School of Information and Electronics, Beijing 100081 China;

2. Beijing Institute of Technology, School of Cyberspace Science and Technology, Beijing 100081 China）

【Abstract】Deep learning has achieved significant success in fields of computer vision and speech signal processing. However, the rapid development of deep learning has also brought negative impacts. All kinds of fake videos and voices are flooding on the Internet. Some criminals use deep learning technology to replace the face of the original video, edit facial attributes, synthesize the speaker's voice, and clone speaker's voices. Criminals can cause social unrest and chaos by producing pornographic videos, fake news, political rumors, etc., threatening personal interests and national security. Many scholars have proposed solutions from different perspectives to eliminate these negative effects. Early forgery mainly focused on single-modal forgery. Therefore, most current solutions focus on single-modal forgery recognition problems and fail to consider the intrinsic relationship between audio and video fully. Existing single-modal detection methods often exhibit suboptimal recognition performance when both audio and video are forged. Recently, with the deepening of research, some scholars have begun to explore the use of multi-modal models for forgery detection and have achieved remarkable results. This survey reviews video, voice forgery, and forgery detection technologies, collects and sorts video, voice, audio and video forgery data sets, and summarizes multi-modal forgery detection methods. Finally, the existing problems and research directions of current detection technology are analyzed and suggestions are given.

【Key words】Deep learning ; Forgery ; Forgery detection $\vdots$ Multimodal ; Audio and Video DOI: 10.19678/j.issn.1000-3428.00EC0070700

# 正文内容

# 0 概述

近年来，深度伪造在网络上泛滥成灾，不法分子利用深度伪造技术合成视频和语音，对个人、社会、国家造成了严重威胁。伪造方式一般包括视频伪造及语音伪造。其中，视频伪造通过对面部操作生成合成视频，早期视频伪造多基于图形学建模，现在诸如生成对抗网络（Generative Adversarial Network，GAN）、扩散模型等深度学习技术成为主流，合成视频质量更高。语音伪造通过文本转语音或转换原始语音特征进行伪造。早期语音合成模型多为管道式，基于拼接合成法或参数法，如今端到端合成逐渐成为主流。而早期语音转换多用平行数据，如今则多用非平行数据，可实现多对多转换。

为了应对深度伪造问题，众多学者提出了解决方案，这些方案涵盖视频、语音以及音视频多模态。视频鉴伪针对视频模态。早期方法多基于取证，使用传统信号处理技术提取频域和统计特征。现在人们逐渐采用不一致性以及数据驱动方法鉴伪。语音鉴伪针对语音模态。早期多为管道式模型，采用信号处理和支持向量机等方法。如今，神经网络逐渐替代这些模块，提升了鉴伪效果。此外，为解决管道式结构易积累误差的问题，人们将前端和后端融合形成端到端结构，以提高鉴伪性能。

随着伪造技术的进步，单模态鉴伪难度升高，一些学者转向使用多模态技术采用不同的融合方式和损失函数处理音视频模态来鉴伪。此外，人们对泛化性能要求的提升，使预训练式的多模态鉴伪方法出现，其强调对真实视频的准确聚类以增强模型泛化性。

目前，针对本领域的综述多数侧重视频、语音单模态，只对早期的伪造以及鉴伪技术做了总结归纳[1]-[3]，涉及音视频多模态鉴伪的综述较少并且不够全面[4]。首先，早期的伪造以及鉴伪总结侧重视频、语音单模态。但随着技术的进步，通过扩散模型等新技术，可以合成更逼真，分辨率更高的视频、语音。其次，一些新的合成方法克服了早期伪造方法存在的缺陷。例如眨眼、头部转动、眼球转动等[1]，而这些缺陷又是大多数早期的鉴伪工作所依赖的。因此，亟需对现有工作进一步的整理以及归纳，并从音视频多模态的角度寻找解决方案。

本文在第 1 节介绍视频伪造，在第 2 节介绍语音伪造，在第 3 节介绍伪造数据集，包括视频，语音及音视频多模态伪造数据集，在第 4 节介绍视频鉴伪，在第 5 节介绍语音鉴伪，在第 6 节介绍音视频多模态鉴伪，最后，在第 7 节对现有鉴伪技术存在的问题未来的研究方向做总结归纳。

# 1 视频伪造

近年来，深度学习在计算机视觉领域取得了重大成功。然而，这也导致了合成视频的泛滥。视频伪造包含四种类型：基于面部交换的伪造，基于面部重建的伪造，基于面部属性编辑的伪造，基于面部生成的伪造。

# 1.1 基于面部交换的伪造

基于面部交换（Face Swap，FS）的伪造使用源视频的面部替换目标视频的面部，达到换脸目的。

# 1.1.1 基于图形学的面部交换

早期的面部交换方法基于图形学，通过 3D 模型对目标建模并渲染，达到换脸目的。例如，FaceSwap[5]通过 3D 模型对获取的人脸关键点进行建模和渲染，最后通过仿射变换和色彩校正实现换脸。

基于图形学的面部交换方法过于复杂，需要构建人脸的 3D 模型，时间开销大、门槛和成本都较高。此外，其对全局身份特征建模，容易忽略细节信息。

# 1.1.2 基于深度学习的面部交换

Deepfakes[6]为面部交换带来了新思路，如图 1 所示，该模型使用两个共享权重的编码器训练提取特征，使两个解码器重建人脸，达到换脸效果。FaceShifter[7]使用AEI-Net 产生初步换脸结果，HEAR-Net 对结果细化。Xu等人 提出 RAFSwap 网络，可以生成更自然的人脸。

![](images/5d5b24d85460e15393322777a47bd53ee6672690b2f4b3a33468d7340c6602d1.jpg)  
图 1  Deepfakes 面部交换架构  
Fig.1  Deepfakes face swap architecture

Liu 等人[9]通过两个给定面孔的局部形状和纹理替换实现换脸。Zhao 等人[10]提出 DiffSwap 模型，依靠扩散模型生成目标视频。Sanoojan 等人[11]提出 REFace 模型，他们将面部交换问题构建为自监督的面部修复任务。通过引入 CLIP 特征解缠，从目标图像中提取姿势，表情和照明信息。此外，使用多步去噪扩散模型加快计算速度。

基于面部交换的生成方法经历了从计算机图形学到生成对抗模型、扩散模型等深度学习算法的演变过程。随着技术的进步，面部交换算法开始关注照明、表情等局部细节信息的建模，并且针对身份保存和面部遮挡问题做出创新性的研究。

# 1.2 基于面部重建的伪造

基于面部重建（Facial Reenactment，FR）的伪造方法使用源视频面部属性重建目标视频面部属性，包括面部表情重建、嘴部重建、眼部重建、头部重建等。

# 1.2.1 基于图形学的面部重建

早期面部重建方法多基于图形学，通过 3D 模型重建和渲染来实现面部重建。Thies 等人[12]又提出 Headon模型，修改视角和姿态独立的纹理进行视频级的渲染，实现了包括表情，眼睛，头部移动的完整重建。然而，基于图形学的面部重建方法过于复杂，依赖高精度的人脸 3D 模型，难以实现高质量的面部重建。

# 1.2.2 基于深度学习的面部重建

深度学习面部重建方法大多数使用其他模态驱动生成的结果，例如语音等。Prajwal 等人[13]提出 Wav2Lip模型，如图 2 所示，将预训练的唇音同步模型作为判别器，用来监督模型生成自然，准确的唇部运动。Liang等人 提出 GC-AVT 模型，从语音内容，头部姿态和情绪表征控制合成说话人面部，以互补的形式学习说话人面部情感的表达。

面部重建从图形学方法发展到深度学习方法，经历了目标视频驱动到音频、情绪等多个模态驱动的演变，生成的视频中人物越来越逼真，并且感情丰富，对鉴伪算法提出了更高的要求。

![](images/2a95f3cd44a63d3c2d3618062792dfd00713daaf7a8422d1f9213b7029284e7a.jpg)  
图 2  Wav2Lip 架构[13]Fig.2  Wav2Lip architecture[13]

# 1.3 基于面部属性编辑的伪造

基于面部属性编辑（Facial Editing，FE）的伪造方法指添加、更改或删除目标身份属性。比如，更换目标对象的发型、胡须、年龄、体重、颜值、眼镜和种族等属性。

# 1.3.1 基于深度学习的面部属性编辑

Choi 等人[15]提出 StarGAN 模型。传统的 GAN 在多属性编辑时，对每一属性需要单独训练一个生成器，如图 3（a）所示，而该模型可以在多个属性编辑条件下使用单个生成网络生成目标，如图 3（b）所示。Gao 等人[16]提出 HifaFace 缓解生成器在合成丰富细节时的压力。此后，Xu 等人 提出 TransEditor 实现高可控性的复杂人脸属性编辑任务。

![](images/a3fd9974c4328dfe2825bbcd7aaf937875cbe4879356c970ecff47c4d5c19244.jpg)  
图 3  StarGAN 对比[15]Fig.3  StarGAN contrast[15]

早期面部属性编辑更多关注可控性问题，要求对目标属性的正确编辑，然而高可控性可能带来低可扩展性和过度约束问题。于是，研究者们针对这些问题提出新的模型。随着技术的进步，人们逐渐将注意力放在精准度上，要求在高可控性的基础上，生成的图像具有丰富细节。

# 1.4 基于面部生成的伪造

基于面部生成（Face Generation，FG）的伪造方法在没有目标身份作为基础的情况下创建伪造角色。

# 1.4.1 基于生成对抗网络的面部生成

Karras 等人[18]提出 ProGAN 网络，如图 4 所示，通过渐进式训练的方式，让生成器先生成低清图像，再逐渐升高分辨率，解决了生成高分辨率图像的难题。

![](images/fe501e69962d0a3985b96c37e7a2ac7400ca0840e75f8c4ea5ea9786081b33fe.jpg)  
图 4  ProGAN 渐进式训练[18]  
Fig.4  ProGAN Progressive Training[18]

Karras 等人[19]提出 StyleGAN 模型，其包含两个部分。如图 5 所示，其中 Mapping network 针对特征纠缠问题，利用多层全连接层解耦，Synthesis network 用于生成目标。然而，该方法生成的图像上有明显的斑点状伪影。于是，Karras 等人 提出 StyleGAN2 模型，通过调整归一化层的使用，有效避免了斑点状伪影。然而，GAN 难以控制输出特征的精确位置。于是，Karras 等人[21]提出StyleGAN3 模型，通过调整 StyleGAN2 的生成器网络结构，使其具有高质量等变性。

![](images/ed30db70264c3a55f7f9a4059c34865de9694131c6373835cf36fbe0b696d5f6.jpg)  
图 5  StyleGAN 结构[19]Fig.5  StyleGAN architecture[19]

直接使用 GAN 生成图像难以控制，但是使用文本信息生成人脸则更加可控。Xia 等人[22]提出 TediGAN 模型实现了文本控制生成人脸。SUN 等人[23]提出 AnyFace模型实现了用文本信息生成更多样化的人脸图像。

面部生成主要依靠 Gan 等生成网络生成所需面部，早期的面部生成的可控性较差，随着技术的发展，以StyleGan 等为代表的生成模型引入各类可控机制，增强了对模型的控制性。

![](images/5850a970f1fd82b55f343ce53f67a01350f88a4a2569b3707878ccd5a0d97274.jpg)  
图 6  视频伪造分类  
Fig.6  Video forge classification

# 2 语音伪造

随着深度学习语音处理技术的发展以及人们对于个性化语音合成的需求，语音伪造算法应运而生。语音伪造包括语音合成和语音转换。

# 2.1 语音合成

语音合成技术也称文本到语音（Text-to-Speech,TTS）技术，将指定文本信息转换为对应语音。

# 2.1.1 管道式语音合成

早期的语音合成技术大多基于管道式语音合成结构，将语音合成任务分为文本分析模块和声学系统模块，生成方法包括拼接合成法和参数合成法两种。

# 拼接合成法

拼接合成法将语音数据库中的语音单元按照一定的规则拼接，生成的语音有着极高的自然度。但由于单词之间的协同发音效果，字级波形的串联听起来不自然[24]。为了合成的语音更自然流畅，拼接合成法需要建立大型语音数据库来提升语音片段的数量和多样性。此外，其会采用一些拼接方法，例如使用特殊技术使音频波形在拼接处尽可能平滑[25]。

虽然拼接合成法可以生成高质量，自然的语音，但需要大量的存储空间和计算资源来管理和搜索语音数据库，并且需要复杂的算法来保证拼接的语音在声学上是连续和平滑的。此外，如果数据库中的语音数据和需要合成的文本在风格，情感和说话人特征上存在不匹配，那么合成的语音可能会听起来不自然或失真。

# 参数合成法

参数合成法通过提取和利用语音的声学参数，借助数学模型模拟人类声音。其无需直接操作语音数据库，而是通过模型学习和泛化来适应不同的语音条件。该方法在数据需求上相对较低，典型代表为基于隐马尔可夫模型（Hidden Markov Models，HMM）的参数式语音合成[26]。

参数合成法利用数学模型对数据进行紧凑表示，能够在有限的数据下通过数学模型的泛化生成新的语音，一定程度上解决了拼接合成法的过大数据库以及单词拼接不平滑问题。然而，该方法需要准确的建模和预测复杂的声学特征，难以在声码器阶段生成高质量的波形。

# 结合深度学习的管道式语音合成

传统语音合成算法虽然在某些场景下能生成可理解的语音，但受制于模型建模准确性，平滑拼接，数据库等问题，传统算法在自然度，流畅性和音质方面难以达到人类语音的水平。深度学习技术的引入极大地改善了这一状况，合成的语音在多个维度上都更加接近真实人类语言水平。

百度人工智能实验室[27]结合改进后的 WaveNet 声码器提出 Deep Voice 系统。使用神经网络代替传统的文本-语音管道式合成结构实现语音合成。此后，该实验室[28]提出基于注意力机制的全卷积语音合成系统 DeepVoice3。

深度学习在一定程度上弥补了传统语音合成方法的不足。然而，管道式语音合成系统的模块之间会产生误差积累，需要文本标注以及文本特征和声学特征的强制对齐，会限制语音合成的效果。

# 2.1.2 端到端语音合成

端到端架构将文本分析模块和声学系统模块连接起来，直接输入文本或者字符，输出对应语音。其极大程度的简化了语音合成系统，降低了对语言学知识的要求。此外，更少的模块有效避免了管道式系统的误差累积问题，在性能上实现了突破。按照选用的模型，端到端的语音合成系统可分为自回归模型与非自回归模型。

# 自回归模型

自回归模型利用过去时刻的采样点生成下一时刻的采样点。WaveNet[29]是该类模型的典型，如图 7所示，其基于前面生成的样本来生成后面的样本。Tacotron[30]采用帧水平自回归生成的方式，实现了比WaveNet 更快的生成速度。

虽然自回归生成模型生成效果不错，但其存在以下几个缺点：首先，生成速度缓慢，在对速度与实时性有较高要求的场所下难以应用。第二，其存在重复吐词或漏词的现象，难以应用于商业环境。第三，其无法细粒度控制语速、韵律及停顿等。

![](images/a38a3e485160152f1ee4f73590ce49565b24509741b63cb01622c9b862f106e1.jpg)  
图 7  WaveNet 语音生成[29] Fig.7  WaveNet speech generation[29]

# 非自回归模型

非自回归网络基于并行网络结构，可以并行生成大量语音段，极大提升语音合成速度。FastSpeech[31]基于 Transformer 架构，如图 8 所示，实现梅尔频谱图的并行生成，加快生成速度。HIFI-GAN[32]基于GAN 通过并行处理不同周期信号。Parallel Tacotron[33]使用变分自编码器（Variational Autoencoder，VAE）和迭代谱图损失提高语音合成的自然度，并且结合轻量级卷积神经网络实现自注意力机制提高生成效率。Diff-TTS[34]学习以文本为条件的梅尔谱图分布，通过扩散模型将噪声信号转换为梅尔谱图。

![](images/c8c5fbcb0e721eee60e9507d05099bb4b8fa83fc5d0352bd2fff9274d593ed84.jpg)  
图 8  FastSpeech 架构[31] Fig.8  FastSpeech architecture[31]

非自回归语音合成方法有效解决了自回归模型面对的问题，其合成速度更快，有更强的可控性，容易解决重复、漏词等问题。然而，非自回归模型构造流程复杂，一般需要借助外部工具对齐文本与声学特征，极大的增加了模型构建的复杂度与成本。

![](images/0b4ccc3985614073a3eaf8d8ee9a81ae6e6e645f0070b1847fdbc602008416eb.jpg)  
图 9  语音合成伪造分类  
Fig.9  Speech synthesis forgery classification

# 2.2 语音转换

语音转换（Voice Conversion，VC）通过转换源语音音色等语音特性生成目标语音，使目标语音听起来有目标人物的特性。

语音转换可以分为平行数据语音转换和非平行数据语音转换两种。平行数据表示源说话人和目标说话人的训练数据成对且内容相同。非平行数据表示源说话人和目标说话人的训练数据内容不同。

# 2.2.1 平行数据

传统语音转换主要使用平行训练数据。使用前一般采用动态时间规整（dynamic time warping，DTW）等对齐方法对齐语音帧。在建模时，一般采用高斯混合模型（Gaussian mixture models, GMM）[35]等参数模型法或矢量量化法[36]等非参数模型法。

基于平行数据的语音转换有着较大的局限性，首先，数据集制作成本高昂，难以扩充。其次，它们只能用于一对一或多对一的语音转换，无法做到任意说话人到任意说话人的语音转换。

# 2.2.2 非平行数据

非平行数据训练需要从非平行数据中推导出说话者之间的语音片段的映射关系。一些早期的算法有INCA 算法[37]，非负矩阵分解法[38]等

深度学习的发展使非平行数据语音转换方法得到迅速发展。 $\mathrm { { X u } }$ 等人[39]提出 Flow-VAE VC 模型，采用 VAE 提高建模能力。Choi 等人[40]提出 DDDM-VC模型，引入去噪扩散模型为生成模型中的每个属性实现有效的样式转移。Zhang 等人[41]提出 DiffGAN-VC模型实现非平行多对多语音转换。

基于非平行数据的语音转换实现难度更高，然而，深度学习技术使非平行数据得到了迅速的发展。该类语音转换解决了传统平行数据语音转换的数据限制以及说话人限制两大问题，可以实现更好的语音转换。

![](images/96aa65af672b5c628ccb597f688a975146297eb44056a72a8e1b4393294e5002.jpg)  
图 10  语音转换伪造分类  
Fig.10  Voice conversion classification

# 3 伪造数据集

现有数据集根据伪造的模态的不同，可以分为视频伪造数据集，语音伪造数据集以及音视频伪造数据集。

# 3.1 视频伪造数据集

WildDeepfake[42]：该数据集视频来源于网络，内容丰富多样，视觉效果更真实，更符合真实生活场景，换脸方法多样，更难检测。

Deeper Forensics- $\mathbf { \partial } \cdot \mathbf { 0 } ^ { [ 4 3 ] }$ ：该数据集规模较其他数据集大了一个数量级，并且生成质量好，多样性高。

Diverse Fake Face Dataset（DFFD）[44]：该数据集伪造方法涉及面部交换，表情交换，面部属性编辑，面部生成。

KoDF[45]：该数据集是一个大规模韩国 Deepfake数据集，伪造方法包括面部交换和面部重建。

CBDF[46]：该伪造方法包括面部交换和表情交换。重点针对性别分布不均衡问题，提出更公平的数据分布。

DF-Platter[47] ： 该 数 据 集 使 用 FSGAN ，FaceSwap，FaceShifter 三种方法进行面部伪造

Table 1  Summary of video forgery datasets   

<html><body><table><tr><td>名称</td><td>年份</td><td>Real</td><td>Fake</td><td>方法</td></tr><tr><td>WildDeepfake</td><td>2020</td><td>3805</td><td>3509</td><td></td></tr><tr><td>Deeper Forensics-1.0</td><td>2020</td><td>50000</td><td>10000</td><td>FS</td></tr><tr><td>Diverse Fake Face Dataset</td><td>2020</td><td>1000</td><td>3000</td><td>FS/FR/FE/FG</td></tr><tr><td>KoDF</td><td>2021</td><td>4000</td><td>4000</td><td>FS/FR</td></tr><tr><td>CBDF</td><td>2023</td><td>2000</td><td>8000</td><td>FS/FR</td></tr><tr><td>DF-Platter</td><td>2023</td><td>764</td><td>132496</td><td>FS</td></tr></table></body></html>

# 3.2 语音伪造数据集

ASVspoof 挑战赛数据集：该数据集有多个版本。其中，2021 版[48]有 LA，PA，DF 三种任务场景，其中，LA 和 DF 场景伪造涉及语音合成和语音转换。

HAD 数据集[49]：该数据集有三个类别，分别是全伪造类，部分伪造类以及全真类。构建了部分伪造数据集，伪造方法为语音合成。

ADD 挑战赛数据集：该类数据集源自音频深度合 成 检 测 挑 战 赛 ， 目 前 有 ADD2022[50] 以 及ADD2023[51]版本，两个版本伪造均涉及语音合成和语音转换，语言为中文。

LibriSeVoc 数据集[53]：该数据集是针对神经声码器的数据集，其使用六个最先进的神经声码器生成语音样本。

DFADD 数据集[54]：该数据集语言为英语，主要针对 TTS 伪造。填补了语音 deepfake 数据集针对扩散模型和流模型 TTS 合成系统的空缺。

Codecfake 数据集[55]：该数据集是针对大语言模型 deepfake 的数据集。

CVoiceFake 数据集[56]：该数据集是包括英语、中文、德语、法语和意大利语五种语言的大规模多语言数据集，利用了多种高性能且经典的语音合成技术。

表 1  视频伪造数据集汇总  
表 2 语音伪造数据集汇总  
Table 2  Summary of audio forgery datasets   

<html><body><table><tr><td>名称</td><td>年份</td><td>Real</td><td>Fake</td><td>方法</td></tr><tr><td>ASVspoof 2021 (LA)</td><td>2021</td><td>14816</td><td>133360</td><td>TTS/VC</td></tr><tr><td>ASVsp0of 2021 (DF)</td><td>2021</td><td>14869</td><td>519059</td><td>TTS/VC</td></tr><tr><td>HAD</td><td>2021</td><td>53612</td><td>53612/53612</td><td>TTS</td></tr><tr><td>ADD2022</td><td>2022</td><td>3012</td><td>24072</td><td>TTS/VC</td></tr><tr><td>ADD2023</td><td>2023</td><td>3012</td><td>24072</td><td>TTS/VC</td></tr><tr><td>LibriSeVoc</td><td>2023</td><td>13201</td><td>79206</td><td>TTS/VC</td></tr><tr><td>DFADD</td><td>2024</td><td>44455</td><td>163500</td><td>TTS</td></tr><tr><td>Codecfake</td><td>2024</td><td>132227</td><td>925939</td><td></td></tr><tr><td>CVoiceFake</td><td>2024</td><td>二</td><td>：</td><td>TTS/VC</td></tr></table></body></html>

# 3.3 音视频伪造数据集

FakeAVCeleb[57]：该数据集包括四个伪造类型：真视频真语音、真视频假语音、假视频真语音、假视频假语音。

Lav-DF[58]：该数据集伪造涉及面部重建以及语音合成。

PolyGlotFake[59]：该数据集伪造涉及视频和语音两个方面。包括七种语言，生成技术新颖。

AV-Deepfake1M[60]：该数据集伪造涉及面部重建以及语音合成。

DfakeAVMiT[61]：该数据集伪造涉及面部交换，面部重建以及语音合成。

表 3 音视频伪造数据集汇总  
Table 3  Summary of audio and video forgery datasets   

<html><body><table><tr><td>名称</td><td>年份</td><td>Real</td><td>Fake</td><td>方法</td></tr><tr><td>FakeAVCeleb</td><td>2021</td><td>500</td><td>19500</td><td>FS/FR/TTS</td></tr><tr><td>Lav-DF</td><td>2022</td><td>36431</td><td>99873</td><td>FR/TTS</td></tr><tr><td>PolyGlotFake</td><td>2023</td><td>766</td><td>14472</td><td>FR/TTS</td></tr><tr><td>AV-Deepfake1M</td><td>2023</td><td>286721</td><td>860039</td><td>FR/TTS</td></tr><tr><td>DfakeAVMiT</td><td>2023</td><td>540</td><td>6480</td><td>FS/FR/TTS</td></tr></table></body></html>

# 4 视频伪造鉴别

传统视频鉴伪主要基于图像取证方法，通过传统信号处理方法提取图像特征，利用图像频域和统计特征鉴伪。例如局部噪声估计[62]等。然而，这些方法多数依赖特定的伪造特征。视频伪造技术的发展使伪造视频可以克服这些缺陷，导致检测效果不佳。

合成视频通常存在一些与真实世界不一致的信息，可以基于生理信息一致性、图像篡改痕迹和基于时域一致性鉴伪。

# 4.1 基于生理信息一致性的鉴伪

合成视频往往会忽视人的真实生理信息，因此，可以基于生理信息鉴伪。Ciftci 等人[63]利用面部的三个区域提取光电容积脉搏信号鉴伪。Haliassos 等人[64]根据嘴部运动检测假视频。Wang 等人[65]分别设计分析整个面部和嘴部区域动态信息的两个分支鉴伪。Nguyen 等人[66]通过眉毛匹配进行鉴伪。

基于生理信息一致性的鉴伪方法有明确的检测对象，如脉搏，心率等，可以使用一些预训练的提取脉搏、心率等生理信息的模型来提取特征，简化前端训练过程。然而，其严重依赖检测对象，随着深度生成技术的进步，伪造技术逐渐克服了各种生理信息缺陷，使基于生理信息一致性的鉴伪模型失去检测对

象。

# 4.2 基于图像篡改痕迹的鉴伪

合成视频通常会存在一些伪造痕迹，可根据伪造痕迹鉴伪。Nirkin 等人[67]检测人脸与头发、耳朵、脖子等差异鉴伪。Khormail 等人[68]利用 Transformer 模型从局部图像特征和不同伪造尺度下像素的全局关系中学习隐藏的扰动痕迹。

基于图像伪造痕迹的鉴伪方法通过提取合成过程中不可避免且共通的伪造痕迹鉴伪，具有更高适应能力。然而，其严重依赖明显的图像伪造痕迹，随着伪造技术的提升，通过扩散模型等新型生成技术已经可以合成极其逼真的图像，伪造痕迹的隐蔽性大幅提升，只关注图像的伪造痕迹显然已经不够了。

# 4.3 基于时域一致性的鉴伪

伪造视频是通过逐帧操作生成的，帧与帧之间存在不自然的图像过度或时间不一致性，如图 11 所示。可以利用视频的时域一致性进行鉴伪。Gu 等人[69]使用空间、时间模块提取特征并整合信息，捕捉视频中的时空不一致性来鉴伪。Zhang 等人[70]检测视频的局部面部分布的时空不一致来鉴伪。Xu 等人[71]提取视频的连续四个帧鉴伪。

基于时域一致性的鉴伪方法比前两种方法更加稳健，充分利用了视频的单模态信息。但其对帧与帧之间的变化很敏感，并且没有解决单帧图像的鉴伪问题，而是换到时域角度解决鉴伪问题。

![](images/05c540c1a9d6256595eb1031e6d6a5c5dce1f2dd0862c67cb40eed1bfb50b8d2.jpg)  
图 11  时域差异性[69]  
Fig.11  Differences in time domains[69]

# 4.4 数据驱动的鉴伪

除了基于一致性的鉴伪方法，还有不少研究者关注基于数据驱动的方式来鉴伪，这类鉴伪方式使用大量的数据训练模型来鉴伪。Zhao 等人[72]提出多注意力深度伪造检测网络来鉴伪。Li 等人[73]对伪像与不相关信息解耦来鉴伪。Hua 等人[74]使用补丁通道将面部潜在特征转换为多通道可解释特征，促进可解释的人脸伪造检测。Zhang 等人[75]提取视频图像局部和全局特征，利用稀疏交叉注意力融合鉴伪。Qiao 等人[76]设计了完全无监督的 Deepfake 探测器，使用伪标签生成器来标记训练样本，并通过对比学习完善判别特征。Lu 等人[77]提出使用远距离注意力分别建模时间和空间来进行细粒度鉴伪。Zhou 等人[78]提出一种新的 FAS 测试时域泛化框架，该框架利用测试数据来提高模型的泛化性。

基于数据驱动的鉴伪方法依靠大量数据训练模型来鉴伪，通过数据驱动模型寻找伪造特征。该类方法没有特别关注视频的某一类特征，如生理信息，伪造痕迹等特定特征，因此可学习的信息更多，准确度可能会更高。然而，该类方法对数据分布依赖性较高，要求测试对象与训练数据具有相近的数据分布，这也导致其存在泛化性的问题。

![](images/a6f796fa2e23f4de2f91210c8fb2386d76afc5489c9be5ff67893d6292c9b18a.jpg)  
图 12  视频鉴伪分类

# 5 语音伪造鉴别

语音鉴伪按照功能结构可分为前端和后端。前端分析语音信号并提取具有区分性的特征，后端根据提取的特征鉴伪。检测器可以将前端与后端结合成管道式鉴别器来鉴伪，也可以使用端到端鉴伪系统以取得更好效果。

# 5.1 管道式系统

# 5.1.1 前端系统

传统前端系统主要使用信号处理提取特征。其中，梅尔频率倒谱特征[79]与倒谱特征线性频率倒谱系数[80]经常作为语音鉴伪的特征。

深度神经网络可以代替传统特征更好的提取伪造痕迹。Martin-Donas 等人[81]使用预训练的 Wav2vec2作为特征提取器来鉴伪。Guo 等人[82]使用 WavLM 获得帧级特征鉴伪。Xie 等人[83]基于 W2V2 前端在掩码特征编码器上求解对比任务来训练，可以表示语音情境化信息。

# 5.1.2 后端系统

后端系统利用前端系统提取的特征进行分析并分类。传统的分类器大多基于机器学习方法，其中最广 泛 的 是 高 斯 混 合 模 型 （ Gaussian Mixture Model,GMM）分类器[84]和支持向量机[85]

近年来，伪造语音检测系统的后端大多基于神经网络分类器，这类分类器在性能方面超越了传统分类器。例如 SENet[86]通过挤压-激励操作显示建模通道之间的相互依赖性，自适应地重新校准通道特征响应。Transformer [87]捕捉输入序列的全局和局部关系，在伪造音频定位任务中表现出色。

管道式系统将鉴伪系统分为前端和后端两个模块，增强了可解释性，降低了训练压力。模型可以针对不同任务替换相应模块，减轻了替换成本，增加了灵活性。然而，管道式系统的模块之间存在误差累积，需要合理的设计两个模块和它们的关系来降低模块间的误差传递。因此，管道式系统难以实现多模块且高复杂度的模型。

# 5.2 端到端系统

端到端系统将特征提取器和分类器结合起来联合优化，十分有竞争力。Ge 等人[88]提出 RawPC，自动学习语音深度伪造和欺骗检测解决方案，同时优化其他网络组件和参数。Tak 等人[89]引入频谱和时间子图以及图池化策略训练光谱-时间图注意力网络。

端到端系统采用整体训练的方式，可以在训练过程中自动优化模块间关系，一定程度降低了设计难度。因此，可以实现更复杂的系统，在鉴伪性能上更强。然而，端到端系统的整体训练加大了训练压力。此外，该系统的可解释性差，其更像一个黑盒，设计者难以透过模型看到内部数据的处理过程。

![](images/d20131fe5a7ccfef165425fb63fcb2fcf0c1e0a7a9ff6e18ca52db423913ce42.jpg)  
Fig.12  Video counterfeit classification   
图 13  语音鉴伪分类  
Fig.13  Audio counterfeit classification

# 6 音视频多模态鉴伪

单模态鉴伪方法受到输入模态的限制，难以应对多模态的灵活伪造。而多模态鉴别伪造方法将音视频特征信息融合鉴伪，可以获得更好的鉴伪性能。按照其训练方式可以分为端到端训练方式和预训练方式。

# 6.1 端到端训练方式

端到端训练的检测方法较为常见，通过融合视听特征，对比视听特征不一致等方式得到鉴伪结果。

# 6.1.1 集成学习

集成学习使用两个模态的信息在决策层上投票来鉴伪。Zhou 等人[90]提出一种基线模型，如图 14 所示，对视频流和音频流独立训练并进行预测，最后将预测的概率结合进行鉴伪。AVFakeNet[91]使用 DenseSwin Transformer 网络组成音频和视频分支，通过比较两个分支的预测标签得到最终标签。

集成学习利用音频和视频分支的预测结果投票来得到最终预测结果。这种方法虽然很直观，但并没有利用音视频的内在联系，多模态鉴伪模型的性能将极大的取决于单个分支的性能。

![](images/706358b6c9f8df2946ba29c6f3da82c0579465fa7634f8969a55b3fbaa73731a.jpg)  
图 14  集成学习架构[90]

6.1.2 联合学习

基于联合学习的鉴伪方法利用模态信息融合鉴伪，可以更充分的利用音视频的内在联系。Zhou 等人[90]提出应用交叉注意力聚合音频和视觉特征。Raza等人[92]提出 Multimodaltrace 模型，其结构如图 15 所示，融合了音视频的特征进行鉴伪。此外，其使用多标签多类的最终分类，充分考虑了音频和视频的分类。Wang 等人[93]提出 AVT2-DWF 模型，通过带有 n帧标记化策略编码器和 Transformer 结构来放大模态内伪造线索，使用基于跨模态注意力的 DWF 模块融合音视频模态信息放大跨模态伪造线索。

基于联合学习的鉴伪方法更好的融合了语音和视频模态的信息。然而，其忽略了模态间的相关性等关系，仅在最后分类部分使用损失函数对整体进行优化，这样的结构对每个模态分支的优化欠佳。

![](images/bb7f02814bce683abb83ecb2cd97d5ea64b16c0fdb23efaef06ebd5bd2664b3d.jpg)  
图 15  Multimodaltrace 架构[92] Fig.15  Multimodaltrace architecture[92]

# 6.1.3 视听一致性

基于视听一致性的鉴伪方法关注音视频之间的各类不一致问题，包括时间不同步，情感不一致等问题。Mittal 等人[94]研究了音视频的情绪不匹配问题，并以此作为多模态鉴伪线索。然而，该方法严重依赖情绪识别模型，如果情绪识别模型性能不佳，则最终的鉴别性能也会受到影响。Chugh 等人[95]使用 MDS来衡量音视频差异。Cheng 等人[96]通过面部和音频的内在相关性，从语音-人脸匹配视角提出 VFD 模型。

上述多模态鉴伪方法仅考虑了视频的伪造，将真实的音频当作辅助监控信号，没有考虑音频模态的伪造问题。Yang 等人 提出 Avoid-DF 模型，使用双向交叉注意力模块对多模态交互建模。Katamneni 等人[97]提出 AVoiDD 模型，其包含两个解码器，一个提取音视频同质特征，另一个提取音视频异质特征，缓解了模态间分布差异。Liu 等人 提出 MCL 模型，利用跨模态对比学习探索模态内和跨模态伪造线索，减少模态差距并探索多模态伪造痕迹。Liu 等人[99]针对伪造时间定位问题，通过具有自注意力的嵌入级融合机制进行伪造定位，并且利用包括视听不一致和时间不一致的多维对比损失来优化模型，如图 16 所示。

![](images/ad8d739a43bcba4c5d4ae4a9124b1741ca5d59abd8b690ad8ffcb38fe7c670c4.jpg)  
Fig.14  Ensemble learning architecture[90]   
图 16  基于嵌入级融合和多维对比损失的多模态鉴伪[99] Fig.16  Multimodal forgery Using Embedding-Level Fusion and Multi-Dimensional Contrastive Loss[99]

近年来，人们发现，由于伪造视频的独立模态伪造导致的学习表示的不一致和不确定性，仅用最终的分类结果来指导每个模态的优化容易导致混乱。针对此问题，Zou 等人[100]提出模态内正则化和跨模态正则化方法指导多模态模型优化，其结构如图 17 所示，拉近真视频的音视频特征距离，拉远合成视频的特征距离。此外，通过拉近或拉远不同视频的单模态特征来细化单模态表示。

基于视听一致性的鉴伪方法不仅融合了模态信息，还充分利用了模态间的关系。该方法不仅会在最终分类加入损失函数，一般还会在模态融合区域使用对比学习构建更全面的损失函数来优化整个网络结构。这种方法对每个模态分支的优化都有了更精细的把控，可以获得更好的性能。

![](images/87075ab350d803442332620de692e735e0a828a416a140cb51b36a30eefd35e9.jpg)  
图 17  正则化的多模态架构[100]

# 6.2 预训练方式

预训练方式先在真实视频上训练，学习真实视频的特点，再在真伪数据集上进行微调训练来提升泛化性能，通过对真实视频的准确建模来鉴伪。

Oorloff 等人[101]等人提出一种两阶段跨模态学习方法 AVFF，如图 18 所示，第一阶段使用掩码和特征融合等手段捕捉真实的视听对应关系，第二阶段对学习到的表示微调。Yu 等人[102]提出 PVASS-MDD 架构，在 PVASS 辅助阶段将两个增强的视觉视图与相应的音频线索相关联来探索真实音视频对应关系。在MDD 阶段，使用冻结的 PVASS 网络来鉴伪。Li 等人[103]在真实数据上预训练音频识别模型和视觉识别模型提取特征，再计算两个序列之间的距离来鉴伪。Feng 等人[104]使用自回归 Transformer 模型为视听同步特征分配概率来鉴伪。

预训练方式虽然泛化能力更好，但其第一阶段仅在真实视频上学习，对数据集要求高，否则容易错过关键伪造特征而产生误判。此外，其模型的精度要求高，否则难以对任意类型的伪造产生响应。

![](images/9ff18784ec867bff51a836ee60e77d26f86ba621732f9dd08b61ea8cfa1a1d13.jpg)  
图 18  AVFF 结构[101]  
Fig.18  AVFF structures[101]

# 表 4 音视频多模态鉴伪汇总

Table 4  Audio and video multimodal counterfeit detection summary   

<html><body><table><tr><td>年份</td><td>文献</td><td>类别</td></tr><tr><td>2021</td><td>Zhou 等人[90]</td><td>集成学习</td></tr><tr><td>2023</td><td>Ilyas等人[91]</td><td>集成学习</td></tr><tr><td>2021</td><td>Zhou 等人[90]</td><td>联合学习</td></tr><tr><td>2023</td><td>Raza等人[92]</td><td>联合学习</td></tr><tr><td>2024</td><td>Wang等人[93]</td><td>联合学习</td></tr><tr><td>2020</td><td>Mittal 等人[94]</td><td>视听一致性学习</td></tr><tr><td>2020</td><td>Chugh 等人[95]</td><td>视听一致性学习</td></tr><tr><td>2023</td><td>Cheng等人[96]</td><td>视听一致性学习</td></tr><tr><td>2023</td><td>Katamneni等人[97]</td><td>视听一致性学习</td></tr><tr><td>2023</td><td>Liu等人[98]</td><td>视听一致性学习</td></tr><tr><td>2023</td><td>Yang等人[61]</td><td>视听一致性学习</td></tr><tr><td>2024</td><td>Liu 等人[99]</td><td>视听一致性学习</td></tr><tr><td>2024</td><td>Zou 等人[100]</td><td>视听一致性学习</td></tr><tr><td>2023</td><td>Yu 等人[102]</td><td>预训练方式</td></tr><tr><td>2023</td><td>Feng等人[104]</td><td>预训练方式</td></tr><tr><td>2024</td><td>Oorloff 等人[101]</td><td>预训练方式</td></tr><tr><td>2024</td><td>Li等人[103]</td><td>预训练方式</td></tr></table></body></html>

![](images/534733a8da47a7038ea9ef536f7897d989ab65d8d12238d4aeb3eb70ed45fdd0.jpg)  
Fig.17  Regularized multimodal architecture[100]   
图 19  多模态鉴伪分类  
Fig.19  Multimodal forgery classification

# 7 结束语

# 7.1 鉴伪技术问题

（1）泛化性能问题：目前的各式鉴伪算法对已知伪造方法取得了不错的效果。然而，不同的生成算法侧重点不同，特征不同，导致泛化性问题。现有的多数应对方法是将新的生成算法加入到伪造数据集中做增强训练[105]。这种方法时效性差，在面对新的生成手段时仍有可能乏力。另一些学者提出了其他增强泛化性能的方法，但这些方法可能会造成其他方面的问题。例如音视频多模态鉴伪时使用预训练方式[101]-[104]，虽然可以一定程度增强泛化性，但其对模型和数据要求严格。因此，鉴伪泛化性问题仍旧是目前的一大难点，并且亟待解决。

（2）抗干扰问题：目前多数鉴伪算法在实验环境中训练以及测试，但现实世界远比实验环境复杂。例如，物理空间中环境光照、噪声的干扰，数字空间中各种压缩算法或音视频后处理的干扰，人为加入的对抗样本等干扰均会影响模型性能。导致实验室环境中模型很高的识别率到实际应用环境中下降明显。

（3）实时性问题：目前的鉴伪算法大多侧重检测的准确率、召回率等指标。然而，现实环境中存在视频会议、网络直播等场景，如何在有限算力下处理实时传输的视频仍是一个问题。

（4）数据集问题：一方面，目前部分公开数据集分辨率较低，随着网络带宽的增加，高分辨率视频逐渐占据主流，低分辨率下训练的模型可能难以适应现实环境中的高分辨率视频，导致模型实用性不高。另一方面，数据集的标注信息不够全面，如未标注伪造方法、伪造区域、其他音视频编辑处理方法，部分数据集来源不可靠，特别是其中所谓真实视频来自互联网，不能确保其未经过其他音视频处理软件加工。

（5）可解释性问题：目前的大多数鉴伪方法注重分类问题，只求将视频、语音正确分类，却没有过多关注判别原因，解释性差。一方面，鉴伪模型难以做到完全准确的分类，需要用户做出一定程度的判断，因此，在鉴伪模型投入使用时，如果无法向用户合理指出伪造区域或解释判伪原因，可能难以博得用户的信任。另一方面，鉴伪模型常常类似黑盒，如果可以加强其可解释性，有助于推动鉴伪模型的进步。

# 7.2 未来研究方向

（1）研究泛化性强的算法：目前的算法在泛化性能上表现不佳，难以对训练集数据中未出现的伪造方法准确分类。为了提升模型的泛化性，可从以下两个方面研究：第一种研究方案以数据为中心，首先，可以探索更多的伪造类型，通过更多样化的数据来增强泛化性。其次，可以对数据预处理，例如平移、旋转、加噪等方式。第三，可以引用零样本学习等训练方式。第二种研究方案以模型为中心：首先，可以通过调整模型结构，增强模型的适应能力。其次，可以调整目标函数或采用一些正则化方法。第三，可以通过调整模型的超参数，寻找最优配置。

（2）研究适应复杂环境的鉴伪算法：目前鉴伪算法在面对复杂环境。例如噪声环境或数据缺失等异常情况时，表现不佳，鲁棒性差。可以在训练时通过加噪、缩放、压缩等数据增强手段提升模型的鲁棒性。

（3）主动防御算法：目前鉴伪算法都是在被动防御。可以通过主动向网络视频中注入一些标志来辅助鉴伪，也可以加入一些噪声扰动来弱化生成模型生成视频的能力[106]。

（4）提升检测算法的实时性：在实际应用中，鉴伪算法经常面对实时传输的视频，目前这方面研究还不是很深入。因此，需要研究更好的实时检测算法。

（5）制作更好的数据集：数据集作为模型训练与学习的基础，其重要性不言而喻。然而，如何处理好音视频分辨率和数据可信性仍是问题。因此，需要研究制作高分辨率、细粒度标注、来源可信的数据集。

（6）研究可解释性强的鉴伪模型：鉴伪模型的可解释性在分类任务中至关重要。然而，目前的大多数工作并没有重视这方面的研究。一些可行的研究方向如下：首先，伪造视频、语音并不一定是全部伪造的，有时仅有部分区域是伪造的。因此，对其伪造区域的定位问题是值得深入研究的一个方向[99]。第二，可解释性语言生成，鉴伪模型虽然是分类模型，但也需要面对用户。可以为鉴伪模型加入一些生成类算法，通过合成解释性文字等方式向用户解释[107]。

# 7.3 总结与展望

近年来深度学习的飞速发展带动了合成技术的发展，出现了一系列深度伪造音视频技术，可以模仿受害者的面部、语音或操纵受害者的面部、语音特征等。其滥用将严重侵犯公民的合法权益，损害企业商业信誉，威胁国家安全和公共安全。本文归纳了单模态伪造方法，单模态、多模态伪造鉴别方法，并进行了科学的分类与分析。最后，指出了目前鉴伪方法存在的问题，并探讨了未来的研究方向，希望推动深度伪造鉴别的进一步发展。

# 参考文献

[1] 李旭嵘,纪守领,吴春明,等.深度伪造与检测技术综述[J]. 软件学报, 2021. Li Xurong, Ji Shouling, Wu Chunming, et al. A Survey on Deepfakes and Detection Techniques[J]. Journal of Softmax, 2021.   
[2] 任延珍,刘晨雨,刘武洋,等.语音伪造及检测技术研究综 述[J].信号处理, 2021, 37(12): 28. Ren Yanzhen, Liu Chenyu, Liu Wuyang, et al. A Survey on Forgery and Detection[J]. Journal of Signal Processing, 2021, 37(12): 28.   
[3] 梁瑞刚,吕培卓,赵月,等.视听觉深度伪造检测技术研究 综述[J].信息安全学报, 2020, 5(2): 1-17. Liang Ruigang, Lv Peizhuo, Zhao yue, et al. A Survey of Audiovisual Deepfake Detection Techniques[J].Journal of Cyber Security, 2020, 5(2): 1-17   
[4] 李泽宇,张旭鸿,蒲誉文,等.多模态深度伪造及检测技术 综述[J].计算机研究与发展,2023,60(6): 1396-1416. Li Zeyu, Zhang Xuhong, Pu Yuwen, et al. A Survey on Multimodal Deepfake and Detection Techniques[J]. Journal of Computer Research and Development, 2023, 60(6): 1396-1416.   
[5] FaceSwap. FaceSwap github[EB/OL]. [2022-09-14]. https://github.com/MarekKowalski/FaceSwap.   
[6] Deepfakes. Deepfakes github[EB/OL]. [2022-09-14]. https://github.com/Deepfakes/faceswap.   
[7] Li L, Bao J, Yang H, et al. Faceshifter: Towards high fidelity and occlusion aware face swapping[C]// Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020: 5074-5083.   
[8] Xu C, Zhang J, Hua M, et al. Region-Aware Face Swapping[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. New Orleans, LA, USA: IEEE, 2022: 7632-7641.   
[9] Liu Zhian, Li Maomao, Zhang Yong, et al. Fine-Grained Face Swapping Via Regional GAN Inversion[C]// IEEE/CVF Conference on Computer Vision and Pattern Recognition: IEEE, 2023: 8578-8587.   
[10] Zhao W, Rao Y, Shi W, et al. Diffswap: High-Fidelity and Controllable Face Swapping via 3D-Aware Masked Diffusion[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Vancouver, BC, Canada: IEEE, 2023: 8568-8577.   
[11] Baliah S, Lin Q, Liao S, et al. Realistic and Efficient Face Swapping: A Unified Approach with Diffusion Models[J]. arXiv, 2024, arXiv preprint arXiv:2409.07269.   
[12] Thies J, Zollhöfer M, Theobalt C, et al. Headon: Real-time Reenactment of Human Portrait Videos[J]. ACM Trans. Graph, 2018, 37(4): 1-13.   
[13] Prajwal K R, Mukhopadhyay R, Namboodiri V P, et al. A Lip Sync Expert Is All You Need for Speech to Lip Generation In The Wild[C]//Proceedings of the 28th ACM international conference on multimedia. New York, NY, USA: Association for Computing Machinery, 2020: 484-492.   
[14] Liang B, Pan Y, Guo Z, et al. Expressive Talking Head Generation with Granular Audio-Visual Control[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. New Orleans, LA, USA: IEEE, 2022: 3387-3396.   
[15] Choi Y, Choi M, Kim M, et al. Stargan: Unified Generative Adversarial Networks for Multi-Domain Image-to-Image Translation[C]//Proceedings of the IEEE conference on computer vision and pattern recognition. Salt Lake City, UT, USA: IEEE, 2018: 8789-8797.   
[16] Gao Y, Wei F, Bao J, et al. High-fidelity and Arbitrary Face Editing[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Nashville, TN, USA: IEEE, 2021: 16115-16124.   
[17] Xu Y, Yin Y, Jiang L, et al. Transeditor: Transformer-Based Dual-Space GAN for Highly Controllable Facial Editing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. New Orleans, LA, USA: IEEE, 2022: 7683-7692.   
[18] Karras T, Aila T, Laine S, et al. Progressive Growing of GANs for Improved Quality, Stability, and Variation[C]// Proceedings of International Conference on Learning Representations. Vancouver, Canada: OpenReview, 2018.   
[19] Karras T, Laine S, Aila T. A Style-Based Generator Architecture for Generative Adversarial Networks[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Long Beach, CA, USA: IEEE, 2019: 4401-4410.   
[20] Karras T, Laine S, Aittala M, et al. Analyzing and Improving the Image Quality of StyleGAN[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Seattle, WA, USA: IEEE, 2020: 8110-8119.   
[21] Karras T, Aittala M, Laine S, et al. Alias-Free Generative Adversarial Networks[C]//Proceedings of the 35th International Conference on Neural Information Processing Systems. Red Hook, NY, USA: Curran Associates Inc, 2021, 66: 1-12.   
[22] Xia W, Yang Y, Xue J H, et al. Tedigan: Text-Guided Diverse Face Image Generation and Manipulation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Nashville, TN, USA: IEEE, 2021: 2256-2265.   
[23] Sun J, Deng Q, Li Q, et al. AnyFace: Free-style Text-to-Face Synthesis and Manipulation[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. New Orleans, LA, USA: IEEE, 2022: 18687-18696.   
[24] Rabiner L R, Schafer R W. Introduction to Digital Speech Processing[J]. Foundations and Trends in Signal Processing, 2007, 1(1–2): 1-194.   
[25] Balestri M, Pacchiotti A, Quazza S. Choose the best to modify the least: A new generation concatenative synthesis system[C]//European Conference on Speech Communication and Technology DBLP. Budapest, Hungary: ISCA, 1999: 2291-2294.   
[26] Nishigaki Y, Takamichi S, Toda T, et al. HMM-Based Speech Synthesis System with Prosody Modification Based on Speech Input[J]. IEICE Technical Report; IEICE Tech. Rep, 2014, 114(365): 81-86.   
[27] Arık S Ö, Chrzanowski M, Coates A, et al. Deep Voice: Real-time Neural Text-to-Speech[C]//International Conference on Machine Learning. Sydney, Australia: PMLR, 2017: 195-204.   
[28] Ping W, Peng K, Gibiansky A, et al. Deep Voice3: 2000-Speaker Neural Text-to-Speech[C]//International Conference on Learning Representations. Vancouver, BC, Canada: OpenReview, 2018, 79: 1094-1099.   
[29] Van Den Oord A, Dieleman S, Zen H, et al. Wavenet: A Generative Model for Raw Audio[C]//ISCA Workshop on Speech Synthesis Workshop, Sunnyvale, USA, 2016: 13-15.   
[30] Wang Y, Skerry-Ryan R J, Stanton D, et al. Tacotron: A Fully End-to-End Text-to-Speech Synthesis Model[J]. arXiv, 2017, arXiv preprint arXiv:1703.10135.   
[31] Ren Y, Ruan Y, Tan X, et al. Fastspeech: Fast, Robust and Controllable Text to Speech[J]. Advances in neural information processing systems, 2019, 32.   
[32] Kong J, Kim J, Bae J. Hifi-GAN: Generative Adversarial Networks for Efficient and High Fidelity Speech Synthesis[J]. Advances in neural information processing systems, 2020, 33: 17022-17033.   
[33] Elias I, Zen H, Shen J, et al. Parallel Tacotron: Non-Autoregressive and Controllable TTS[C]//ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing. Toronto, ON, Canada: IEEE, 2021: 5709-5713.   
[34] Jeong M, Kim H, Cheon S J, et al. Diff-TTS: A Denoising Diffusion Model for Text-to-Speech[C]//Interspeech. Brno, Czechia: ISCA, 2021: 3605-3609.   
[35] Kawanami H, Iwami Y, Toda T, et al. GMM-based Voice Conversion Applied to Emotional Speech Synthesis[J]. 2003.   
[36] Wu D Y, Lee H. One-Shot Voice Conversion by Vector Quantization[C]//ICASSP 2020-2020 IEEE International Conference on Acoustics, Speech and Signal Processing. Barcelona, Spain: IEEE, 2020: 7734-7738.   
[37] Suda H, Kotani $\mathbf { G }$ Saito D. Nonparallel Training of Exemplar-Based Voice Conversion System Using INCA-Based Alignment Technique[C]//Interspeech. Shanghai, China: ISCA, 2020: 4681-4685.   
[38] Suda H, Kotani G, Saito D. INmfCA Algorithm for Training of Nonparallel Voice Conversion Systems Based on Non-Negative Matrix Factorization[J]. IEICE Transactions on Information and Systems, 2022, 105(6): 1196-1210.   
[39] Xu L, Zhong R, Liu Y, et al. Flow-VAE VC: End-to-End Flow Framework with Contrastive Loss for Zero-shot Voice Conversion[C]//Interspeech. Dublin, Ireland: ISCA, 2023: 2293-2297.   
[40] Choi H Y, Lee S H, Lee S W. DDDM-VC: Decoupled Denoising Diffusion Models with Disentangled Representation and Prior Mixup for Verified Robust Voice Conversion[C]//Proceedings of the AAAI Conference on Artificial Intelligence. VANCOUVER, CANADA: AAAI, 2024, 38(16): 17862-17870.   
[41] Zhang X, Wang J, Cheng N, et al. Voice Conversion with Denoising Diffusion Probabilistic GAN Models[C]//International Conference on Advanced Data Mining and Applications. Cham. Berlin, Heidelberg: Springer-Verlag, 2023: 154-167.   
[42] Zi B, Chang M, Chen J, et al. Wilddeepfake: A challenging real-world dataset for deepfake detection[C]//Proceedings of the 28th ACM international conference on multimedia. New York, NY, USA: Association for Computing Machinery, 2020: 2382-2390.   
[43] Jiang L, Li R, Wu W, et al. Deeperforensics-1.0: A large-scale dataset for real-world face forgery detection[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Seattle, WA, USA: IEEE, 2020: 2889-2898.   
[44] Dang H, Liu F, Stehouwer J, et al. On the detection of digital face manipulation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern recognition. Seattle, WA, USA: IEEE, 2020: 5781-5790.   
[45] Kwon P, You J, Nam G, et al. Kodf: A large-scale korean deepfake detection dataset[C]//Proceedings of the IEEE/CVF international conference on computer vision. Montreal, QC, Canada: IEEE, 2021: 10744-10753.   
[46] Nadimpalli A V, Rattani A. GBDF: gender balanced deepfake dataset towards fair deepfake detection[C]//International Conference on Pattern Recognition. Berlin, Heidelberg: Springer-Verlag, 2022: 320-337.   
[47] Narayan K, Agarwal H, Thakral K, et al. Df-platter: Multi-face heterogeneous deepfake dataset[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Vancouver, BC, Canada: IEEE, 2023: 9739-9748.   
[48] Liu X, Wang X, Sahidullah M, et al. Asvspoof 2021: Towards spoofed and deepfake speech detection in the wild[J]. IEEE/ACM Trans. Audio, Speech and Lang. Proc, 2023, 31: 2507-2522.   
[49] Yi J, Bai Y, Tao J, et al. Half-truth: A partially fake audio detection dataset[C]//Interspeech. Brno, Czechia: ISCA, 2021: 1654-1658.   
[50] Yi J, Fu R, Tao J, et al. Add 2022: the first audio deep synthesis detection challenge[C]//ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing. Singapore, Singapore: IEEE, 2022: 9216-9220.   
[51] Yi J, Tao J, Fu R, et al. Add 2023: the second audio deepfake detection challenge[C]//IJCAI 2023 Workshopon Deepfake Audio Detection and Analysis. Macao, S.A.R, 2023.   
[52] Sanderson C. The vidtimit database[J]. 2002.   
[53] Sun Chengzhe, Jia Shan, Hou Shuwei, et al. AI-Synthesized Voice Detection Using Neural Vocoder Artifacts[C]// Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. Vancouver, BC, Canada: IEEE, 2023: 904-912.   
[54] Du J, Lin I, Chiu I, et al. DFADD: The Diffusion and Flow-Matching Based Audio Deepfake Dataset[J]. arXiv, 2024, arXiv preprint arXiv:2409.08731.   
[55] Lu Y, Xie Y, Fu R, et al. Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio[C]//Interspeech. Kos Island, Greece: ISCA, 2024: 1390-1394.   
[56] Li Xinfeng, Li Kai, Zheng Yifan, et al. SafeEar: Content Privacy-Preserving Audio Deepfake Detection[C]// Proceedings of the 2024 ACM SIGSAC Conference on Computer and Communications Security (CCS). New York, NY, USA: Association for Computing Machinery, 2024: 3585-3599.   
[57] Khalid H, Tariq S, Kim M, et al. FakeAVCeleb: A novel audio-video multimodal deepfake dataset[C]//Conference on Neural Information Processing Systems Track on Datasets and Benchmarks. Seattle, WA, USA: MIT Press, 2021.   
[58] Cai Z, Stefanov K, Dhall A, et al. Do you really mean that? content driven audio-visual deepfake dataset and multimodal method for temporal forgery localization[C]//International Conference on Digital Image Computing: Techniques and Applications. Sydney, Australia: IEEE, 2022: 1-10.   
[59] Hou Y, Fu H, Chen C, et al. PolyGlotFake: A Novel Multilingual and Multimodal DeepFake Dataset[J]. arxiv, 2024, arxiv preprint arxiv:2405.08838.   
[60] Cai Z, Ghosh S, Adatia A P, et al. AV-Deepfake1M: A large-scale LLM-driven audio-visual deepfake dataset[C]//Proceedings of the 32nd ACM International Conference on Multimedia. New York, NY, USA: Association for Computing Machinery, 2024: 7414-7423.   
[61] Yang W, Zhou X, Chen Z, et al. Avoid-df: Audio-visual joint learning for detecting deepfake[J]. IEEE Transactions on Information Forensics and Security, 2023, 18: 2015-2029.   
[62] Pan X, Zhang X, Lyu S. Exposing image splicing with inconsistent local noise variances[C]//IEEE International conference on computational photography. Seattle, WA, USA: IEEE, 2012: 1-10.   
[63] Ciftci U A, Demir I, Yin L. Fakecatcher: Detection of synthetic portrait videos using biological signals[J]. IEEE transactions on pattern analysis and machine intelligence, 2020: 1-1.   
[64] Haliassos A, Vougioukas K, Petridis S, et al. Lips don't lie: A generalisable and robust approach to face forgery detection[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Nashville, TN, USA: IEEE, 2021: 5039-5049.   
[65] Wang H, Liu Z, Wang S. Exploiting complementary dynamic incoherence for deepfake video detection[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2023, 33(8): 4027-4040.   
[66] Nguyen H M, Derakhshani R. Eyebrow recognition for identifying deepfake videos[C]//international conference of the biometrics special interest group. Darmstadt, Germany: IEEE, 2020: 1-5.   
[67] Nirkin Y, Wolf L, Keller Y, et al. Deepfake detection based on discrepancies between faces and their context[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021, 44(10): 6111-6121.   
[68] Khormali A, Yuan J S. DFDT: an end-to-end deepfake detection framework using vision transformer[J]. Applied Sciences, 2022, 12(6): 2953.   
[69] Gu Z, Chen Y, Yao T, et al. Spatiotemporal inconsistency learning for deepfake video detection[C]//Proceedings of the 29th ACM international conference on multimedia. New York, NY, USA: Association for Computing Machinery, 2021: 3473-3481.   
[70] Zhang D, Lin F, Hua Y, et al. Deepfake video detection with spatiotemporal dropout transformer[C]//Proceedings of the 30th ACM international conference on multimedia. New York, NY, USA: Association for Computing Machinery, 2022: 5833-5841.   
[71] Xu Y, Liang J, Jia G, et al. Tall: Thumbnail layout for deepfake video detection[C]//Proceedings of the IEEE/CVF international conference on computer vision. Paris, France: IEEE, 2023: 22658-22668.   
[72] Zhao H, Zhou W, Chen D, et al. Multi-attentional deepfake detection[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. Nashville, TN, USA: IEEE, 2021: 2185-2194.   
[73] X. Li, R. Ni, P. Yang, et al. Artifacts-Disentangled Adversarial Learning for Deepfake Detection[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2023, 33(4): 1658-1670.   
[74] Y. Hua, R. Shi, P. Wang, et al. Learning Patch-Channel Correspondence for Interpretable Face Forgery Detection[J]. IEEE Transactions on Image Processing, 2023, 32: 1668-1680.   
[75] Yiwen ZHANG, Manchun CAI, Yonghao CHEN, Yi ZHU, Lifeng YAO. Multi-Scale Deepfake Detection Method with Fusion of Spatial Features[J]. Computer Engineering, 2024, 50(7): 240-250.   
[76] T Qiao, S Xie, Y Chen, F. Fully Unsupervised Deepfake Video Detection Via Enhanced Contrastive Learning[J]. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024, 46(7): 4654-4668.   
[77] W Lu, L Liu, B Zhang, et al. Detection of Deepfake Videos Using Long-Distance Attention[J]. IEEE Transactions on Neural Networks and Learning Systems, 2024, 35(7): 9366-9379.   
[78] Q Zhou, K -Y Zhang, T Yao, et al. Test-Time Domain Generalization for Face Anti-Spoofing[C]//IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USA: IEEE, 2024: 175-187.   
[79] Wu Z, Chng E S, Li H. Detecting converted speech and natural speech for anti-spoofing attack in speaker recognition[C]//Interspeech. Portland, OR, USA: ISCA, 2012: 1700-1703.   
[80] Chitale M, Dhawale A, Dubey M, et al. A Hybrid CNN-LSTM Approach for Deepfake Audio Detection[C]// International Conference on Artificial Intelligence For Internet of Things. Vellore, India: IEEE, 2024: 1-6.   
[81] Martín-Doñas J M, Álvarez A. The vicomtech audio deepfake detection system based on wav2vec2 for the 2022 add challenge[C]//ICASSP 2022-2022 IEEE International Conference on Acoustics, Speech and Signal Processing. IEEE, 2022: 9241-9245.   
[82] Guo Y, Huang H, Chen X, et al. Audio Deepfake Detection With Self-Supervised Wavlm And Multi-Fusion Attentive Classifier[C]//ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing. Seoul, Korea, Republic of: IEEE, 2024: 12702-12706.   
[83] Y Xie, H Cheng, Y Wang, et al. Learning A Self-Supervised Domain-Invariant Feature Representation for Generalized Audio Deepfake Detection[C]//Interspeech. Dublin, Ireland: ISCA, 2023: 2808-2812.   
[84] Lei Z, Yang Y, Liu C, et al. Siamese Convolutional Neural Network Using Gaussian Probability Feature for Spoofing Speech Detection[C]//Interspeech. Shanghai, China: ISCA, 2020: 1116-1120.   
[85] Hamza A, Javed A R R, Iqbal F, et al. Deepfake audio detection via MFCC features using machine learning[J]. IEEE Access, 2022, 10: 134018-134028.   
[86] Li X, Li N, Weng C, et al. Replay and synthetic speech detection with res2net architecture[C]//ICASSP 2021-2021 IEEE international conference on acoustics, speech and signal processing. Toronto, ON, Canada: IEEE, 2021: 6354-6358.   
[87] Liu X, Liu M, Wang L, et al. Leveraging positional-related local-global dependency for synthetic speech detection[C]//ICASSP 2023-2023 IEEE International Conference on Acoustics, Speech and Signal Processing. Rhodes Island, Greece: IEEE, 2023: 1-5.   
[88] Ge W, Patino J, Todisco M, et al. Raw differentiable architecture search for speech deepfake and spoofing detection[C]//Edition of the Automatic Speaker Verification and Spoofing Countermeasures Challenge. ISCA, 2021: 22-28. graph attention networks for speaker verification anti-spoofing and speech deepfake detection[C]//Edition of the Automatic Speaker Verification and Spoofing Countermeasures Challenge. ISCA, 2021: 1-8.   
[90] Zhou Y, Lim S N. Joint audio-visual deepfake detection[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. Montreal, QC, Canada: IEEE, 2021: 14800-14809.   
[91] Ilyas H, Javed A, Malik K M. AVFakeNet: A unified end-to-end Dense Swin Transformer deep learning model for audio–visual deepfakes detection[J]. Applied Soft Computing, 2023, 136: 110124.   
[92] Raza M A, Malik K M. Multimodaltrace: Deepfake detection using audiovisual representation learning[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Vancouver, BC, Canada: IEEE, 2023: 993-1000.   
[93] Wang R, Ye D, Tang L, et al. AVT2-DWF: Improving Deepfake Detection with Audio-Visual Fusion and Dynamic Weighting Strategies[J]. arXiv, 2024, arXiv preprint arXiv:2403.14974.   
[94] Mittal T, Bhattacharya U, Chandra R, et al. Emotions don't lie: An audio-visual deepfake detection method using affective cues[C]//Proceedings of the 28th ACM international conference on multimedia. New York, NY, USA: Association for Computing Machinery, 2020: 2823-2832.   
[95] Chugh K, Gupta P, Dhall A, et al. Not made for each other-audio-visual dissonance-based deepfake detection and localization[C]//Proceedings of the 28th ACM international conference on multimedia. New York, NY, USA: Association for Computing Machinery, 2020: 439-447.   
[96] Cheng H, Guo Y, Wang T, et al. Voice-face homogeneity tells deepfake[J]. ACM Transactions on Multimedia Computing, Communications and Applications, 2023, 20(3): 1-22.   
[97] Katamneni V S, Rattani A. MIS-AVoiDD: Modality invariant and specific representation for audio-visual deepfake detection[C]//International Conference on Machine Learning and Applications. Jacksonville, FL, USA: IEEE, 2023: 1371-1378.   
[98] Liu X, Yu Y, Li X, et al. Mcl: multimodal contrastive learning for deepfake detection[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2024, 34(4): 2803-2813.   
[99] M. Liu, J. Wang, X. Qian, et al. Audio-Visual Temporal Forgery Detection Using Embedding-Level Fusion and Multi-Dimensional Contrastive Loss[J], IEEE Transactions on Circuits and Systems for Video Technology, 2024, 34(8): 6937-6948.   
[100] Zou H, Shen M, Hu Y, et al. Cross-Modality and Within-Modality Regularization for Audio-Visual Deepfake Detection[C]//ICASSP 2024-2024 IEEE International Conference on Acoustics, Speech and Signal Processing. Seoul, Korea, Republic of: IEEE, 2024: 4900-4904.   
[101] Oorloff T, Koppisetti S, Bonettini N, et al. AVFF: Audio-Visual Feature Fusion for Video Deepfake Detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Seattle, WA, USA: IEEE, 2024: 27102-27112.   
[102] Yu Y, Liu X, Ni R, et al. Pvass-mdd: predictive visual-audio alignment self-supervision for multimodal deepfake detection[J]. IEEE Transactions on Circuits and Systems for Video Technology, 2023.   
[103] Li X, Liu Z, Chen C, et al. Zero-Shot Fake Video Detection by Audio-Visual Consistency[C]//Interspeech. Kos, Greece: ISCA, 2024: 2935-2939.   
[104] Feng C, Chen Z, Owens A. Self-supervised video forensics by audio-visual anomaly detection[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. Vancouver, BC, Canada: IEEE, 2023: 10491-10503.   
[105] Li X, Yu K, Ji S, et al. Fighting against deepfake: Patch&pair convolutional neural networks (PPCNN)[C]//Companion Proceedings of the Web Conference. New York, NY, USA: Association for Computing Machinery, 2020: 88-89.   
[106] DAI Lei, CAO Lin, GUO Yanan, ZHANG Fan, DU Kangning. Deepfake Cross-Model Defense Method Based on Generative Adversarial Network[J]. Computer Engineering, 2024, 50(10): 100-109.   
[107] Haq Ijaz Ul, Malik Khalid Mahmood, Muhammad Khan[J]. ACM Trans. Multimedia Comput. Commun. Appl, 2024, 20(11): 341.